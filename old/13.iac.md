
## **IaC & CI/CD要件**

**IaC & CI/CD 統合要件 - 包括的セキュリティ分析パイプライン**

**目次**

1.  [概要](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#%E6%A6%82%E8%A6%81)

2.  [パイプライン全体構成](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#1-%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E5%85%A8%E4%BD%93%E6%A7%8B%E6%88%90)

3.  [静的解析の詳細実装](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#2-%E9%9D%99%E7%9A%84%E8%A7%A3%E6%9E%90%E3%81%AE%E8%A9%B3%E7%B4%B0%E5%AE%9F%E8%A3%85)

4.  [IAM
    アクセス分析の詳細実装](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#3-iam-%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9%E5%88%86%E6%9E%90%E3%81%AE%E8%A9%B3%E7%B4%B0%E5%AE%9F%E8%A3%85)

5.  [動的解析の詳細実装](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#4-%E5%8B%95%E7%9A%84%E8%A7%A3%E6%9E%90%E3%81%AE%E8%A9%B3%E7%B4%B0%E5%AE%9F%E8%A3%85)

6.  [エラーハンドリング Lambda
    関数](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#5-%E3%82%A8%E3%83%A9%E3%83%BC%E3%83%8F%E3%83%B3%E3%83%89%E3%83%AA%E3%83%B3%E3%82%B0-lambda-%E9%96%A2%E6%95%B0%E8%A9%B3%E7%B4%B0%E5%AE%9F%E8%A3%85)

7.  [監視とレポーティングシステム](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#6-%E7%9B%A3%E8%A6%96%E3%81%A8%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0)

8.  [実装まとめ](https://claude.ai/chat/16c88446-c69f-4115-9456-55d2d27864d0#7-%E5%AE%9F%E8%A3%85%E3%81%BE%E3%81%A8%E3%82%81)

**概要**

**プロジェクト背景**

TechNova社は現在、従来のモノリシックアーキテクチャから120アカウント構成のマイクロサービス環境への移行を進めています。このプロセスにおいて、Infrastructure
as Code (IaC)
とCI/CDパイプラインの統合により、セキュリティ品質を担保しながら開発効率を最大化する包括的なセキュリティ分析システムが必要となりました。

**要件の目的**

本要件書は、静的解析、IAM権限分析、動的脆弱性検証を統合した多層防御型のDevSecOpsパイプラインを定義します。これにより、デプロイ前の段階で潜在的なセキュリティリスクを確実に検出・修正し、120アカウント環境全体のセキュリティガバナンスを実現します。

**パイプライン実行順序**

ソース取得 → 静的解析 → IAM分析 → 動的解析 → 承認 → デプロイ

↓ ↓ ↓ ↓ ↓ ↓

Git取得 構文チェック 権限分析 脆弱性検証 人間判断 実行

↓ ↓ ↓ ↓ ↓ ↓

1-2分 10-20分 5-10分 45-60分 可変 10-20分

**実装対象サービス**

- **AWS CodePipeline**: メインパイプライン制御・ワークフロー管理

- **AWS CodeBuild**: ステージ別実行環境・分析ツール実行

- **AWS CodeCommit**: ソースコード管理・バージョン管理

- **AWS S3**: アーティファクト保存・ログ管理・レポート保存

- **AWS SNS**: エラー通知・承認要求・アラート配信

- **AWS Lambda**: エラーハンドリング・自動化処理・レポート生成

- **AWS IAM Access Analyzer**: 権限分析・外部アクセス検出

- **AWS DynamoDB**: 結果保存・メトリクス追跡・エラー記録

- **Amazon CloudWatch**: 監視・メトリクス・ダッシュボード

- **AWS Systems Manager**: 設定管理・パラメータストア

- **Amazon EventBridge**: イベント処理・スケジューリング

**セキュリティ分析の3層構造**

**第1層: 静的解析 (10-20分)**

- **Terraform Validate**: 構文検証・参照整合性

- **TFLint**: コード品質・ベストプラクティス

- **Checkov**: セキュリティ設定・コンプライアンス

**第2層: IAM分析 (5-10分)**

- **組織レベル分析**: 120アカウント横断の権限関係

- **外部アクセス検出**: 予期しない外部からのアクセス

- **未使用権限特定**: 過剰権限・不要権限の洗い出し

**第3層: 動的解析 (45-60分・並列実行)**

- **インフラ脆弱性スキャン**: ネットワーク・OS・ミドルウェア

- **アプリ脆弱性スキャン**: OWASP Top 10・Webアプリケーション

- **ペネトレーションテスト**: 実悪用可能性・侵入経路検証

**1. パイプライン全体構成**

**1.1 基本アーキテクチャ**

resource "aws_codepipeline" "security_integrated_pipeline" {

name = "security-integrated-deployment-pipeline"

role_arn = aws_iam_role.codepipeline_role.arn

artifact_store {

location = aws_s3_bucket.pipeline_artifacts.bucket

type = "S3"

encryption_key {

id = aws_kms_key.pipeline_key.arn

type = "KMS"

}

}

\# ========================================

\# ソース取得

\# ========================================

stage {

name = "Source"

action {

name = "SourceAction"

category = "Source"

owner = "AWS"

provider = "CodeCommit"

version = "1"

output_artifacts = \["source_output"\]

configuration = {

RepositoryName = aws_codecommit_repository.iac_repo.repository_name

BranchName = "main"

}

}

}

\# ========================================

\# 静的解析（3つのステージに分割）

\# ========================================

stage {

name = "TerraformValidate"

action {

name = "ValidateAction"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.terraform_validate.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "ERROR_HANDLING_MODE"

value = var.error_handling_mode

type = "PLAINTEXT"

}

\])

}

on_failure {

action_type_id {

category = "Invoke"

owner = "AWS"

provider = "Lambda"

version = "1"

}

configuration = {

FunctionName = aws_lambda_function.pipeline_error_handler.function_name

UserParameters = jsonencode({

stage_name = "TerraformValidate"

error_type = "VALIDATION_ERROR"

pipeline_name = "security-integrated-deployment-pipeline"

})

}

}

}

}

\# 手動承認ステージ（Validateエラー時）

stage {

name = "ValidateErrorApproval"

action {

name = "ManualApproval"

category = "Approval"

owner = "AWS"

provider = "Manual"

version = "1"

configuration = {

NotificationArn = aws_sns_topic.pipeline_approvals.arn

CustomData = "Terraform Validate failed. Review errors and approve to
continue or reject to stop pipeline."

}

}

}

stage {

name = "TFLint"

action {

name = "LintAction"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.tflint.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "ERROR_HANDLING_MODE"

value = var.error_handling_mode

type = "PLAINTEXT"

}

\])

}

on_failure {

action_type_id {

category = "Invoke"

owner = "AWS"

provider = "Lambda"

version = "1"

}

configuration = {

FunctionName = aws_lambda_function.pipeline_error_handler.function_name

UserParameters = jsonencode({

stage_name = "TFLint"

error_type = "LINTING_ERROR"

pipeline_name = "security-integrated-deployment-pipeline"

})

}

}

}

}

\# 手動承認ステージ（TFLintエラー時）

stage {

name = "TFLintErrorApproval"

action {

name = "ManualApproval"

category = "Approval"

owner = "AWS"

provider = "Manual"

version = "1"

configuration = {

NotificationArn = aws_sns_topic.pipeline_approvals.arn

CustomData = "TFLint analysis failed. Review linting errors and approve
to continue."

}

}

}

stage {

name = "Checkov"

action {

name = "CheckovAction"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.checkov.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "ERROR_HANDLING_MODE"

value = var.error_handling_mode

type = "PLAINTEXT"

}

\])

}

on_failure {

action_type_id {

category = "Invoke"

owner = "AWS"

provider = "Lambda"

version = "1"

}

configuration = {

FunctionName = aws_lambda_function.pipeline_error_handler.function_name

UserParameters = jsonencode({

stage_name = "Checkov"

error_type = "SECURITY_VIOLATION"

pipeline_name = "security-integrated-deployment-pipeline"

})

}

}

}

}

\# セキュリティ承認ステージ（静的解析完了後）

stage {

name = "StaticAnalysisApproval"

action {

name = "StaticAnalysisApproval"

category = "Approval"

owner = "AWS"

provider = "Manual"

version = "1"

configuration = {

NotificationArn = aws_sns_topic.security_approvals.arn

CustomData = "Static security analysis complete. Review all findings
before proceeding to IAM analysis."

}

}

}

\# ========================================

\# IAM アクセス分析

\# ========================================

stage {

name = "IAMAccessAnalysis"

action {

name = "AnalyzeIAMAccess"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.iam_access_analysis.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "TARGET_ENVIRONMENT"

value = "staging"

type = "PLAINTEXT"

},

{

name = "ANALYZER_ARN"

value = aws_accessanalyzer_analyzer.organization_analyzer.arn

type = "PLAINTEXT"

},

{

name = "ERROR_HANDLING_MODE"

value = var.error_handling_mode

type = "PLAINTEXT"

}

\])

}

on_failure {

action_type_id {

category = "Invoke"

owner = "AWS"

provider = "Lambda"

version = "1"

}

configuration = {

FunctionName = aws_lambda_function.pipeline_error_handler.function_name

UserParameters = jsonencode({

stage_name = "IAMAccessAnalysis"

error_type = "IAM_SECURITY_VIOLATION"

pipeline_name = "security-integrated-deployment-pipeline"

})

}

}

}

}

\# IAM分析結果承認ステージ

stage {

name = "IAMAnalysisApproval"

action {

name = "IAMAnalysisApproval"

category = "Approval"

owner = "AWS"

provider = "Manual"

version = "1"

configuration = {

NotificationArn = aws_sns_topic.iam_analysis_approvals.arn

CustomData = "IAM Access Analysis complete. Review external access
findings and unused permissions before proceeding to dynamic analysis."

}

}

}

\# ========================================

\# 動的解析（並列実行）

\# ========================================

stage {

name = "DynamicSecurityAnalysis"

action {

name = "InfraVulnScan"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.dynamic_vuln_scan.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "TARGET_ENVIRONMENT"

value = "staging"

type = "PLAINTEXT"

},

{

name = "ERROR_HANDLING_MODE"

value = var.error_handling_mode

type = "PLAINTEXT"

}

\])

}

run_order = 1

}

action {

name = "AppVulnScan"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.dynamic_app_scan.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "TARGET_ENVIRONMENT"

value = "staging"

type = "PLAINTEXT"

}

\])

}

run_order = 1

}

action {

name = "PenTest"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.penetration_test.name

EnvironmentVariables = jsonencode(\[

{

name = "PIPELINE_EXECUTION_ID"

value = "#{codepipeline.PipelineExecutionId}"

type = "PLAINTEXT"

},

{

name = "TARGET_ENVIRONMENT"

value = "staging"

type = "PLAINTEXT"

}

\])

}

run_order = 1

}

}

\# 動的解析結果承認ステージ

stage {

name = "DynamicAnalysisApproval"

action {

name = "DynamicAnalysisApproval"

category = "Approval"

owner = "AWS"

provider = "Manual"

version = "1"

configuration = {

NotificationArn = aws_sns_topic.security_approvals.arn

CustomData = "Dynamic security analysis complete. Review vulnerability
scan results and penetration test findings."

}

}

}

\# ========================================

\# デプロイステージ（開発環境）

\# ========================================

stage {

name = "DeployDev"

action {

name = "DeployToDev"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.terraform_deploy_dev.name

EnvironmentVariables = jsonencode(\[

{

name = "TARGET_ENVIRONMENT"

value = "development"

type = "PLAINTEXT"

},

{

name = "TERRAFORM_WORKSPACE"

value = "dev"

type = "PLAINTEXT"

}

\])

}

}

}

\# 本番承認ステージ

stage {

name = "ProductionApproval"

action {

name = "ProductionDeploymentApproval"

category = "Approval"

owner = "AWS"

provider = "Manual"

version = "1"

configuration = {

NotificationArn = aws_sns_topic.production_approvals.arn

CustomData = "Development deployment successful. Approve for production
deployment."

}

}

}

\# デプロイステージ（本番環境）

stage {

name = "DeployProd"

action {

name = "DeployToProd"

category = "Build"

owner = "AWS"

provider = "CodeBuild"

version = "1"

input_artifacts = \["source_output"\]

configuration = {

ProjectName = aws_codebuild_project.terraform_deploy_prod.name

EnvironmentVariables = jsonencode(\[

{

name = "TARGET_ENVIRONMENT"

value = "production"

type = "PLAINTEXT"

},

{

name = "TERRAFORM_WORKSPACE"

value = "prod"

type = "PLAINTEXT"

}

\])

}

}

}

tags = {

Purpose = "Integrated Security Pipeline"

Environment = "Multi-Account"

Owner = "SecurityTeam"

Project = "TechNova-DevSecOps"

}

}

**1.2 必要なIAMロール**

\# CodePipeline サービスロール

resource "aws_iam_role" "codepipeline_role" {

name = "codepipeline-security-pipeline-role"

assume_role_policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Action = "sts:AssumeRole"

Effect = "Allow"

Principal = {

Service = "codepipeline.amazonaws.com"

}

}

\]

})

tags = {

Purpose = "CodePipeline Service Role"

Environment = "Multi-Account"

}

}

resource "aws_iam_role_policy" "codepipeline_policy" {

name = "codepipeline-security-pipeline-policy"

role = aws_iam_role.codepipeline_role.id

policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Effect = "Allow"

Action = \[

"s3:GetBucketVersioning",

"s3:PutObject",

"s3:GetObject",

"s3:GetObjectVersion"

\]

Resource = \[

aws_s3_bucket.pipeline_artifacts.arn,

"\${aws_s3_bucket.pipeline_artifacts.arn}/\*"

\]

},

{

Effect = "Allow"

Action = \[

"codebuild:BatchGetBuilds",

"codebuild:StartBuild"

\]

Resource = "\*"

},

{

Effect = "Allow"

Action = \[

"lambda:InvokeFunction"

\]

Resource = \[

aws_lambda_function.pipeline_error_handler.arn

\]

},

{

Effect = "Allow"

Action = \[

"sns:Publish"

\]

Resource = \[

aws_sns_topic.pipeline_approvals.arn,

aws_sns_topic.security_approvals.arn,

aws_sns_topic.production_approvals.arn

\]

}

\]

})

}

\# CodeBuild サービスロール

resource "aws_iam_role" "codebuild_role" {

name = "codebuild-security-analysis-role"

assume_role_policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Action = "sts:AssumeRole"

Effect = "Allow"

Principal = {

Service = "codebuild.amazonaws.com"

}

}

\]

})

tags = {

Purpose = "CodeBuild Service Role"

Environment = "Multi-Account"

}

}

resource "aws_iam_role_policy" "codebuild_policy" {

name = "codebuild-security-analysis-policy"

role = aws_iam_role.codebuild_role.id

policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Effect = "Allow"

Action = \[

"logs:CreateLogGroup",

"logs:CreateLogStream",

"logs:PutLogEvents"

\]

Resource = "arn:aws:logs:\*:\*:\*"

},

{

Effect = "Allow"

Action = \[

"s3:GetObject",

"s3:GetObjectVersion",

"s3:PutObject"

\]

Resource = \[

"\${aws_s3_bucket.pipeline_artifacts.arn}/\*",

"\${aws_s3_bucket.build_logs.arn}/\*"

\]

},

{

Effect = "Allow"

Action = \[

"sns:Publish"

\]

Resource = \[

aws_sns_topic.pipeline_errors.arn,

aws_sns_topic.critical_security_alerts.arn

\]

},

{

Effect = "Allow"

Action = \[

"ec2:DescribeInstances",

"ec2:DescribeSecurityGroups",

"ec2:DescribeVpcs",

"ec2:DescribeSubnets",

"ec2:DescribeNetworkAcls"

\]

Resource = "\*"

},

{

Effect = "Allow"

Action = \[

"cloudwatch:PutMetricData"

\]

Resource = "\*"

Condition = {

StringEquals = {

"cloudwatch:namespace" = \[

"TechNova/SecurityAnalysis",

"TechNova/IAMSecurity",

"TechNova/PenetrationTesting"

\]

}

}

}

\]

})

}

**2. 静的解析の詳細実装**

**2.1 Terraform Validate**

resource "aws_codebuild_project" "terraform_validate" {

name = "terraform-validate-with-error-handling"

description = "Terraform validation with enhanced error handling"

service_role = aws_iam_role.codebuild_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_MEDIUM"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials_type = "CODEBUILD"

environment_variable {

name = "S3_LOG_BUCKET"

value = aws_s3_bucket.build_logs.id

}

environment_variable {

name = "ERROR_HANDLER_FUNCTION"

value = aws_lambda_function.build_error_handler.function_name

}

}

source {

type = "CODEPIPELINE"

buildspec = "buildspecs/validate.yml"

}

tags = {

Stage = "StaticAnalysis"

Tool = "TerraformValidate"

ErrorHandling = "Enabled"

}

}

**2.2 validate.yml（詳細エラーハンドリング）**

version: 0.2

env:

variables:

LOG_FILE: "terraform-validate-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

TERRAFORM_VERSION: "1.5.0"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up environment..." \| tee logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# Terraform インストール

wget
https://releases.hashicorp.com/terraform/\${TERRAFORM_VERSION}/terraform\_\${TERRAFORM_VERSION}\_linux_amd64.zip

unzip terraform\_\${TERRAFORM_VERSION}\_linux_amd64.zip

sudo mv terraform /usr/local/bin/

terraform version \| tee -a logs/\$LOG_FILE

pre_build:

commands:

\- echo "\[PRE_BUILD\] Initializing Terraform..." \| tee -a
logs/\$LOG_FILE

\- \|

\# Backend設定なしでinit（validation用）

terraform init -backend=false \>\> logs/\$LOG_FILE 2\>&1

INIT_EXIT_CODE=\$?

if \[ \$INIT_EXIT_CODE -ne 0 \]; then

echo "\[ERROR\] Terraform init failed with exit code \$INIT_EXIT_CODE"
\| tee -a logs/\$LOG_FILE

export BUILD_ERROR="TERRAFORM_INIT_FAILED"

export BUILD_EXIT_CODE=\$INIT_EXIT_CODE

fi

build:

commands:

\- echo "\[BUILD\] Running terraform validate..." \| tee -a
logs/\$LOG_FILE

\- \|

\# Terraform validate実行

terraform validate \>\> logs/\$LOG_FILE 2\>&1

VALIDATE_EXIT_CODE=\$?

if \[ \$VALIDATE_EXIT_CODE -ne 0 \]; then

echo "\[ERROR\] Terraform validate failed with exit code
\$VALIDATE_EXIT_CODE" \| tee -a logs/\$LOG_FILE

\# エラー詳細の解析

if grep -q "Invalid reference" logs/\$LOG_FILE; then

export SPECIFIC_ERROR="INVALID_REFERENCE"

elif grep -q "Missing required argument" logs/\$LOG_FILE; then

export SPECIFIC_ERROR="MISSING_ARGUMENT"

elif grep -q "Duplicate resource" logs/\$LOG_FILE; then

export SPECIFIC_ERROR="DUPLICATE_RESOURCE"

elif grep -q "Invalid provider configuration" logs/\$LOG_FILE; then

export SPECIFIC_ERROR="INVALID_PROVIDER_CONFIG"

elif grep -q "Module not found" logs/\$LOG_FILE; then

export SPECIFIC_ERROR="MODULE_NOT_FOUND"

else

export SPECIFIC_ERROR="GENERIC_VALIDATION_ERROR"

fi

export BUILD_ERROR="TERRAFORM_VALIDATE_FAILED"

export BUILD_EXIT_CODE=\$VALIDATE_EXIT_CODE

\# エラーハンドリング Lambda を呼び出し

aws lambda invoke \\

--function-name \$ERROR_HANDLER_FUNCTION \\

--payload '{

"stage": "TerraformValidate",

"error_type": "'\$SPECIFIC_ERROR'",

"exit_code": '\$VALIDATE_EXIT_CODE',

"pipeline_execution_id": "'\$PIPELINE_EXECUTION_ID'",

"log_file": "'\$LOG_FILE'",

"error_handling_mode": "'\$ERROR_HANDLING_MODE'"

}' \\

/tmp/error_response.json

else

echo "\[SUCCESS\] Terraform validate completed successfully" \| tee -a
logs/\$LOG_FILE

fi

post_build:

commands:

\- echo "\[POST_BUILD\] Uploading logs and handling errors..." \| tee -a
logs/\$LOG_FILE

\- \|

\# ログをS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "S3 upload failed"

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Error handling mode is STOP. Failing build." \| tee
-a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Error handling mode is CONTINUE. Build continues
despite errors." \| tee -a logs/\$LOG_FILE

exit 0

elif \[ "\$ERROR_HANDLING_MODE" = "MANUAL_APPROVAL" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Error handling mode is MANUAL_APPROVAL. Human
intervention required." \| tee -a logs/\$LOG_FILE

\# 手動承認待ちのマーカーファイル作成

echo "\$BUILD_ERROR" \> /tmp/manual_approval_required.txt

aws s3 cp /tmp/manual_approval_required.txt
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/manual_approval_required.txt

exit 0

else

echo "\[POST_BUILD\] No errors or successful completion" \| tee -a
logs/\$LOG_FILE exit 0 fi

artifacts: files: - logs/\* - /tmp/manual_approval_required.txt name:
terraform-validate-results

\### 2.3 TFLint解析

\`\`\`hcl

resource "aws_codebuild_project" "tflint" {

name = "tflint-security-analysis"

description = "TFLint code quality and best practices analysis"

service_role = aws_iam_role.codebuild_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_MEDIUM"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials_type = "CODEBUILD"

environment_variable {

name = "TFLINT_CONFIG_BUCKET"

value = aws_s3_bucket.security_configs.id

}

environment_variable {

name = "TFLINT_RULES_SEVERITY"

value = "HIGH"

}

}

source {

type = "CODEPIPELINE"

buildspec = "buildspecs/tflint.yml"

}

tags = {

Stage = "StaticAnalysis"

Tool = "TFLint"

Severity = "High"

}

}

2.4 tflint.yml（包括的品質分析）

version: 0.2

env:

variables:

LOG_FILE: "tflint-analysis-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

TFLINT_VERSION: "0.47.0"

RESULTS_FILE: "tflint-results.json"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up TFLint environment..." \| tee
logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# TFLint インストール

curl -s
https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh
\| bash

tflint --version \| tee -a logs/\$LOG_FILE

\# AWS ruleset プラグインのセットアップ

mkdir -p ~/.tflint.d/plugins

tflint --init \| tee -a logs/\$LOG_FILE

pre_build:

commands:

\- echo "\[PRE_BUILD\] Downloading TFLint configuration..." \| tee -a
logs/\$LOG_FILE

\- \|

\# カスタム設定をS3からダウンロード

aws s3 cp s3://\$TFLINT_CONFIG_BUCKET/tflint-config/.tflint.hcl
.tflint.hcl \|\| echo "Using default config"

\# ルールセット設定確認

if \[ -f .tflint.hcl \]; then

echo "\[PRE_BUILD\] Using custom TFLint configuration:" \| tee -a
logs/\$LOG_FILE

cat .tflint.hcl \| tee -a logs/\$LOG_FILE

else

echo "\[PRE_BUILD\] Creating default TFLint configuration" \| tee -a
logs/\$LOG_FILE

cat \> .tflint.hcl \<\< 'EOF'

plugin "aws" {

enabled = true

version = "0.24.1"

source = "github.com/terraform-linters/tflint-ruleset-aws"

}

rule "terraform_deprecated_interpolation" {

enabled = true

}

rule "terraform_unused_declarations" {

enabled = true

}

rule "terraform_comment_syntax" {

enabled = true

}

rule "terraform_documented_outputs" {

enabled = true

}

rule "terraform_documented_variables" {

enabled = true

}

rule "terraform_typed_variables" {

enabled = true

}

rule "terraform_module_pinned_source" {

enabled = true

style = "semver"

}

rule "terraform_naming_convention" {

enabled = true

format = "snake_case"

}

rule "terraform_standard_module_structure" {

enabled = true

}

rule "aws_instance_invalid_type" {

enabled = true

}

rule "aws_security_group_rule_invalid_protocol" {

enabled = true

}

rule "aws_db_instance_invalid_type" {

enabled = true

}

rule "aws_elasticache_cluster_invalid_type" {

enabled = true

}

rule "aws_alb_invalid_security_group" {

enabled = true

}

rule "aws_elb_invalid_security_group" {

enabled = true

}

rule "aws_instance_invalid_ami" {

enabled = true

}

rule "aws_launch_configuration_invalid_image_id" {

enabled = true

}

rule "aws_route_invalid_route_table" {

enabled = true

}

EOF

fi

build:

commands:

\- echo "\[BUILD\] Running TFLint analysis..." \| tee -a logs/\$LOG_FILE

\- \|

\# TFLint実行（JSON形式で結果出力）

tflint --format=json --force \> logs/\$RESULTS_FILE 2\>&1

TFLINT_EXIT_CODE=\$?

\# 結果をログにも出力（人間が読みやすい形式）

echo "\[BUILD\] TFLint Results Summary:" \| tee -a logs/\$LOG_FILE

tflint --format=compact \| tee -a logs/\$LOG_FILE

\# JSONから重要度別の問題数を集計

ERROR_COUNT=\$(jq '\[.issues\[\] \| select(.rule.severity == "error")\]
\| length' logs/\$RESULTS_FILE 2\>/dev/null \|\| echo "0")

WARNING_COUNT=\$(jq '\[.issues\[\] \| select(.rule.severity ==
"warning")\] \| length' logs/\$RESULTS_FILE 2\>/dev/null \|\| echo "0")

NOTICE_COUNT=\$(jq '\[.issues\[\] \| select(.rule.severity ==
"notice")\] \| length' logs/\$RESULTS_FILE 2\>/dev/null \|\| echo "0")

echo "\[BUILD\] Issues Summary:" \| tee -a logs/\$LOG_FILE

echo " - Errors: \$ERROR_COUNT" \| tee -a logs/\$LOG_FILE

echo " - Warnings: \$WARNING_COUNT" \| tee -a logs/\$LOG_FILE

echo " - Notices: \$NOTICE_COUNT" \| tee -a logs/\$LOG_FILE

\# 重要度に基づく判定

if \[ "\$TFLINT_RULES_SEVERITY" = "HIGH" \] && \[ "\$ERROR_COUNT" -gt 0
\]; then

echo "\[ERROR\] High severity mode: Found \$ERROR_COUNT error(s)" \| tee
-a logs/\$LOG_FILE

export BUILD_ERROR="TFLINT_HIGH_SEVERITY_VIOLATIONS"

export BUILD_EXIT_CODE=1

elif \[ "\$TFLINT_RULES_SEVERITY" = "MEDIUM" \] && \[ \$((ERROR_COUNT +
WARNING_COUNT)) -gt 0 \]; then

echo "\[ERROR\] Medium severity mode: Found \$((ERROR_COUNT +
WARNING_COUNT)) error(s)/warning(s)" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="TFLINT_MEDIUM_SEVERITY_VIOLATIONS"

export BUILD_EXIT_CODE=1

elif \[ "\$TFLINT_RULES_SEVERITY" = "LOW" \] && \[ \$((ERROR_COUNT +
WARNING_COUNT + NOTICE_COUNT)) -gt 0 \]; then

echo "\[ERROR\] Low severity mode: Found \$((ERROR_COUNT +
WARNING_COUNT + NOTICE_COUNT)) issue(s)" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="TFLINT_LOW_SEVERITY_VIOLATIONS"

export BUILD_EXIT_CODE=1

else

echo "\[SUCCESS\] TFLint analysis passed for severity level:
\$TFLINT_RULES_SEVERITY" \| tee -a logs/\$LOG_FILE

fi

\# 詳細な違反分析

if \[ ! -z "\$BUILD_ERROR" \]; then

echo "\[BUILD\] Detailed violation analysis:" \| tee -a logs/\$LOG_FILE

jq -r '.issues\[\] \| "Rule: \\.rule.name) \| Severity:
\\.rule.severity) \| File: \\.range.filename):\\.range.start.line) \|
Message: \\.message)"' logs/\$RESULTS_FILE \| tee -a logs/\$LOG_FILE

fi

post_build:

commands:

\- echo "\[POST_BUILD\] Processing results and uploading artifacts..."
\| tee -a logs/\$LOG_FILE

\- \|

\# CloudWatchメトリクス送信

aws cloudwatch put-metric-data \\

--namespace "TechNova/SecurityAnalysis" \\

--metric-data \\

MetricName=TFLintErrors,Value=\$ERROR_COUNT,Unit=Count \\

MetricName=TFLintWarnings,Value=\$WARNING_COUNT,Unit=Count \\

MetricName=TFLintNotices,Value=\$NOTICE_COUNT,Unit=Count \\

--region \$AWS_DEFAULT_REGION \|\| echo "CloudWatch metrics failed"

\# 結果をS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "Log upload failed"

aws s3 cp logs/\$RESULTS_FILE
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$RESULTS_FILE \|\| echo "Results
upload failed"

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Stopping pipeline due to TFLint violations" \| tee
-a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Continuing pipeline despite TFLint violations" \|
tee -a logs/\$LOG_FILE

exit 0

else

exit 0

fi

artifacts:

files:

\- logs/\*

name: tflint-analysis-results

2.5 Checkov セキュリティ解析

resource "aws_codebuild_project" "checkov" {

name = "checkov-security-compliance"

description = "Checkov security and compliance analysis"

service_role = aws_iam_role.codebuild_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_LARGE"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials_type = "CODEBUILD"

environment_variable {

name = "CHECKOV_CONFIG_BUCKET"

value = aws_s3_bucket.security_configs.id

}

environment_variable {

name = "SECURITY_FRAMEWORK"

value = "CIS,NIST,SOC2"

}

environment_variable {

name = "COMPLIANCE_THRESHOLD"

value = "80"

}

}

source {

type = "CODEPIPELINE"

buildspec = "buildspecs/checkov.yml"

}

tags = {

Stage = "StaticAnalysis"

Tool = "Checkov"

Compliance = "CIS-NIST-SOC2"

}

}

2.6 checkov.yml（包括的セキュリティ検証）

version: 0.2

env:

variables:

LOG_FILE: "checkov-analysis-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

RESULTS_FILE: "checkov-results.json"

SUMMARY_FILE: "checkov-summary.json"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up Checkov environment..." \| tee
logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# Python環境とCheckovのインストール

python3 -m pip install --upgrade pip

pip3 install checkov==2.3.228

checkov --version \| tee -a logs/\$LOG_FILE

\# 必要なPythonライブラリ

pip3 install jq boto3 requests

pre_build:

commands:

\- echo "\[PRE_BUILD\] Configuring Checkov security policies..." \| tee
-a logs/\$LOG_FILE

\- \|

\# カスタム設定をS3からダウンロード

aws s3 cp s3://\$CHECKOV_CONFIG_BUCKET/checkov-config/checkov.yaml
checkov.yaml \|\| echo "Using default config"

\# カスタムポリシーファイルをダウンロード

mkdir -p custom-policies

aws s3 sync s3://\$CHECKOV_CONFIG_BUCKET/custom-policies/
custom-policies/ \|\| echo "No custom policies found"

\# デフォルト設定の作成

if \[ ! -f checkov.yaml \]; then

echo "\[PRE_BUILD\] Creating default Checkov configuration" \| tee -a
logs/\$LOG_FILE

cat \> checkov.yaml \<\< 'EOF'

branch: main

check:

\- CKV_AWS_1 \# Root access key check

\- CKV_AWS_2 \# ALB listener security

\- CKV_AWS_3 \# EBS encryption

\- CKV_AWS_7 \# KMS key rotation

\- CKV_AWS_8 \# Launch config public IP

\- CKV_AWS_9 \# ElastiCache Redis AUTH

\- CKV_AWS_10 \# RDS backup retention

\- CKV_AWS_14 \# Ensure all data stored in the Launch configuration or
Auto Scaling Group EBS is securely encrypted at rest

\- CKV_AWS_17 \# RDS encryption

\- CKV_AWS_18 \# S3 server side encryption

\- CKV_AWS_19 \# S3 encryption with customer key

\- CKV_AWS_20 \# S3 public access block

\- CKV_AWS_21 \# S3 versioning

\- CKV_AWS_23 \# ECR image scanning

\- CKV_AWS_24 \# Security group description

\- CKV_AWS_25 \# Security group rule description

\- CKV_AWS_27 \# SQS encryption

\- CKV_AWS_28 \# DynamoDB point in time recovery

\- CKV_AWS_33 \# ECR image tag immutability

\- CKV_AWS_34 \# CloudFront WAF

\- CKV_AWS_35 \# CloudTrail encryption

\- CKV_AWS_36 \# ElastiCache encryption transit

\- CKV_AWS_37 \# ElastiCache encryption rest

\- CKV_AWS_38 \# RDS public access

\- CKV_AWS_39 \# Lambda environment encryption

\- CKV_AWS_40 \# IAM policy attached to users

\- CKV_AWS_41 \# IAM policy attached to groups

\- CKV_AWS_42 \# IAM policy attached to roles

\- CKV_AWS_43 \# Lambda function X-Ray tracing

\- CKV_AWS_45 \# Lambda environment credentials

\- CKV_AWS_46 \# API Gateway X-Ray tracing

\- CKV_AWS_47 \# API Gateway request validation

\- CKV_AWS_48 \# API Gateway SSL certificate

\- CKV_AWS_49 \# API Gateway execution logging

\- CKV_AWS_50 \# Lambda function dead letter queue

\- CKV_AWS_51 \# ECR repository policy

\- CKV_AWS_54 \# S3 bucket public read

\- CKV_AWS_55 \# S3 bucket public write

\- CKV_AWS_56 \# S3 bucket public read ACP

\- CKV_AWS_57 \# S3 bucket public write ACP

\- CKV_AWS_58 \# S3 bucket logging

\- CKV_AWS_59 \# API Gateway cache encryption

\- CKV_AWS_60 \# IAM policy allows root access

\- CKV_AWS_61 \# IAM policy allows full admin access

\- CKV_AWS_62 \# IAM policy allows trust

\- CKV_AWS_63 \# IAM policy allows manage

\- CKV_AWS_64 \# Redshift encryption

\- CKV_AWS_65 \# Redshift SSL

\- CKV_AWS_66 \# CloudTrail log validation

\- CKV_AWS_67 \# CloudTrail encryption

\- CKV_AWS_68 \# CloudFront default root object

\- CKV_AWS_69 \# S3 bucket encryption

\- CKV_AWS_70 \# RDS deletion protection

\- CKV_AWS_71 \# Redshift public access

\- CKV_AWS_72 \# RDS encryption key

\- CKV_AWS_73 \# API Gateway X-Ray tracing

\- CKV_AWS_74 \# Lambda function code signing

\- CKV_AWS_75 \# DocDB encryption

\- CKV_AWS_76 \# API Gateway access logging

\- CKV_AWS_77 \# Neptune encryption

\- CKV_AWS_78 \# CodeBuild image pull credentials

\- CKV_AWS_79 \# Lambda function reserved concurrency

\- CKV_AWS_80 \# MSK cluster logging

\- CKV_AWS_81 \# ElastiCache replication group encryption

\- CKV_AWS_82 \# ElastiCache replication group auth token

\- CKV_AWS_83 \# ElastiCache replication group backup

\- CKV_AWS_84 \# ElastiCache replication group multi AZ

\- CKV_AWS_85 \# DocDB backup retention

\- CKV_AWS_86 \# CloudFront default root object

\- CKV_AWS_87 \# Redshift enhanced VPC routing

\- CKV_AWS_88 \# EC2 public IP

\- CKV_AWS_89 \# DMS replication instance public access

\- CKV_AWS_90 \# DocDB logging

\- CKV_AWS_91 \# EKS endpoint private access

\- CKV_AWS_92 \# EKS endpoint public access

skip_check:

\# 環境固有でスキップするチェック

framework:

\- cis

\- nist_cybersecurity_framework_1.1

\- soc2

output: json

quiet: false

external_checks_dir: ./custom-policies

EOF

fi

build:

commands:

\- echo "\[BUILD\] Running Checkov security analysis..." \| tee -a
logs/\$LOG_FILE

\- \|

\# Checkov実行（複数フレームワークでの検証）

echo "\[BUILD\] Starting comprehensive security analysis with multiple
frameworks" \| tee -a logs/\$LOG_FILE

\# 基本的なCheckov実行

checkov -d . \\

--config-file checkov.yaml \\

--output json \\

--output-file logs/\$RESULTS_FILE \\

--framework terraform \\

--download-external-modules True \\

--evaluate-variables True \>\> logs/\$LOG_FILE 2\>&1

CHECKOV_EXIT_CODE=\$?

\# 結果の詳細分析

if \[ -f logs/\$RESULTS_FILE \]; then

echo "\[BUILD\] Analyzing Checkov results..." \| tee -a logs/\$LOG_FILE

\# 統計情報の抽出

TOTAL_CHECKS=\$(jq '.summary.parsing_errors + .summary.passed_checks +
.summary.failed_checks + .summary.skipped_checks' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

PASSED_CHECKS=\$(jq '.summary.passed_checks' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

FAILED_CHECKS=\$(jq '.summary.failed_checks' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

SKIPPED_CHECKS=\$(jq '.summary.skipped_checks' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

PARSING_ERRORS=\$(jq '.summary.parsing_errors' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

\# コンプライアンススコア計算

if \[ \$TOTAL_CHECKS -gt 0 \]; then

COMPLIANCE_SCORE=\$(echo "scale=2; \$PASSED_CHECKS \* 100 /
(\$PASSED_CHECKS + \$FAILED_CHECKS)" \| bc -l 2\>/dev/null \|\| echo
"0")

else

COMPLIANCE_SCORE="0"

fi

echo "\[BUILD\] Security Analysis Summary:" \| tee -a logs/\$LOG_FILE

echo " - Total Checks: \$TOTAL_CHECKS" \| tee -a logs/\$LOG_FILE

echo " - Passed: \$PASSED_CHECKS" \| tee -a logs/\$LOG_FILE

echo " - Failed: \$FAILED_CHECKS" \| tee -a logs/\$LOG_FILE

echo " - Skipped: \$SKIPPED_CHECKS" \| tee -a logs/\$LOG_FILE

echo " - Parsing Errors: \$PARSING_ERRORS" \| tee -a logs/\$LOG_FILE

echo " - Compliance Score: \${COMPLIANCE_SCORE}%" \| tee -a
logs/\$LOG_FILE

\# 重要度別の失敗分析

HIGH_SEVERITY_FAILS=\$(jq '\[.results.failed_checks\[\] \|
select(.severity == "HIGH")\] \| length' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

MEDIUM_SEVERITY_FAILS=\$(jq '\[.results.failed_checks\[\] \|
select(.severity == "MEDIUM")\] \| length' logs/\$RESULTS_FILE
2\>/dev/null \|\| echo "0")

LOW_SEVERITY_FAILS=\$(jq '\[.results.failed_checks\[\] \|
select(.severity == "LOW")\] \| length' logs/\$RESULTS_FILE 2\>/dev/null
\|\| echo "0")

echo " - High Severity Failures: \$HIGH_SEVERITY_FAILS" \| tee -a
logs/\$LOG_FILE

echo " - Medium Severity Failures: \$MEDIUM_SEVERITY_FAILS" \| tee -a
logs/\$LOG_FILE

echo " - Low Severity Failures: \$LOW_SEVERITY_FAILS" \| tee -a
logs/\$LOG_FILE

\# サマリーJSONファイル作成

cat \> logs/\$SUMMARY_FILE \<\< EOF

{

"pipeline_execution_id": "\$PIPELINE_EXECUTION_ID",

"timestamp": "\$(date -u +%Y-%m-%dT%H:%M:%SZ)",

"total_checks": \$TOTAL_CHECKS,

"passed_checks": \$PASSED_CHECKS,

"failed_checks": \$FAILED_CHECKS,

"skipped_checks": \$SKIPPED_CHECKS,

"parsing_errors": \$PARSING_ERRORS,

"compliance_score": \$COMPLIANCE_SCORE,

"severity_breakdown": {

"high": \$HIGH_SEVERITY_FAILS,

"medium": \$MEDIUM_SEVERITY_FAILS,

"low": \$LOW_SEVERITY_FAILS

},

"compliance_threshold": \$COMPLIANCE_THRESHOLD,

"threshold_met": \$(echo "\$COMPLIANCE_SCORE \>= \$COMPLIANCE_THRESHOLD"
\| bc -l)

}

EOF

\# コンプライアンス閾値の判定

THRESHOLD_MET=\$(echo "\$COMPLIANCE_SCORE \>= \$COMPLIANCE_THRESHOLD" \|
bc -l)

if \[ "\$THRESHOLD_MET" = "0" \]; then

echo "\[ERROR\] Compliance score \${COMPLIANCE_SCORE}% is below
threshold \${COMPLIANCE_THRESHOLD}%" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="COMPLIANCE_THRESHOLD_NOT_MET"

export BUILD_EXIT_CODE=1

elif \[ \$HIGH_SEVERITY_FAILS -gt 0 \]; then

echo "\[ERROR\] Found \$HIGH_SEVERITY_FAILS high severity security
violations" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="HIGH_SEVERITY_SECURITY_VIOLATIONS"

export BUILD_EXIT_CODE=1

elif \[ \$PARSING_ERRORS -gt 0 \]; then

echo "\[ERROR\] Found \$PARSING_ERRORS parsing errors" \| tee -a
logs/\$LOG_FILE

export BUILD_ERROR="PARSING_ERRORS"

export BUILD_EXIT_CODE=1

else

echo "\[SUCCESS\] Security analysis passed all requirements" \| tee -a
logs/\$LOG_FILE

fi

\# 失敗したチェックの詳細表示

if \[ \$FAILED_CHECKS -gt 0 \]; then

echo "\[BUILD\] Failed Security Checks Details:" \| tee -a
logs/\$LOG_FILE

jq -r '.results.failed_checks\[\] \| "Check: \\.check_id) \| File:
\\.file_path):\\.file_line_range\[0\]) \| Message: \\.check_name)"'
logs/\$RESULTS_FILE \| head -20 \| tee -a logs/\$LOG_FILE

if \[ \$FAILED_CHECKS -gt 20 \]; then

echo "... and \$((FAILED_CHECKS - 20)) more failures. See full report
for details." \| tee -a logs/\$LOG_FILE

fi

fi

else

echo "\[ERROR\] Checkov results file not found" \| tee -a
logs/\$LOG_FILE

export BUILD_ERROR="CHECKOV_EXECUTION_FAILED"

export BUILD_EXIT_CODE=1

fi

post_build:

commands:

\- echo "\[POST_BUILD\] Processing results and generating reports..." \|
tee -a logs/\$LOG_FILE

\- \|

\# CloudWatchメトリクス送信

if \[ -f logs/\$SUMMARY_FILE \]; then

aws cloudwatch put-metric-data \\

--namespace "TechNova/SecurityAnalysis" \\

--metric-data \\

MetricName=CheckovTotalChecks,Value=\$TOTAL_CHECKS,Unit=Count \\

MetricName=CheckovPassedChecks,Value=\$PASSED_CHECKS,Unit=Count \\

MetricName=CheckovFailedChecks,Value=\$FAILED_CHECKS,Unit=Count \\

MetricName=CheckovComplianceScore,Value=\$COMPLIANCE_SCORE,Unit=Percent
\\

MetricName=CheckovHighSeverityFails,Value=\$HIGH_SEVERITY_FAILS,Unit=Count
\\

--region \$AWS_DEFAULT_REGION \|\| echo "CloudWatch metrics failed"

fi

\# 重要な場合はSNS通知送信

if \[ ! -z "\$BUILD_ERROR" \] && \[ "\$BUILD_ERROR" =
"HIGH_SEVERITY_SECURITY_VIOLATIONS" \]; then

aws sns publish \\

--topic-arn \$CRITICAL_SECURITY_ALERTS_TOPIC \\

--subject "CRITICAL: High Severity Security Violations Detected" \\

--message "Pipeline: \$PIPELINE_EXECUTION_ID\nHigh Severity Failures:
\$HIGH_SEVERITY_FAILS\nCompliance Score: \${COMPLIANCE_SCORE}%\nReview
required immediately." \\

--region \$AWS_DEFAULT_REGION \|\| echo "SNS notification failed"

fi

\# 結果をS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "Log upload failed"

aws s3 cp logs/\$RESULTS_FILE
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$RESULTS_FILE \|\| echo "Results
upload failed"

aws s3 cp logs/\$SUMMARY_FILE
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$SUMMARY_FILE \|\| echo "Summary
upload failed"

\# HTMLレポート生成

if command -v checkov \>/dev/null 2\>&1; then

checkov -d . \\

--config-file checkov.yaml \\

--output cli \\

--framework terraform \> logs/checkov-report.txt 2\>&1 \|\| echo "HTML
report generation failed"

aws s3 cp logs/checkov-report.txt
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/checkov-report.txt \|\| echo "Report
upload failed"

fi

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Stopping pipeline due to security violations" \|
tee -a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Continuing pipeline despite security violations" \|
tee -a logs/\$LOG_FILE

exit 0

else

exit 0

fi

artifacts:

files:

\- logs/\*

name: checkov-security-analysis-results

**3. IAM アクセス分析の詳細実装**

**3.1 IAM Access Analyzer設定**

\# 組織レベルのAccess Analyzer

resource "aws_accessanalyzer_analyzer" "organization_analyzer" {

analyzer_name = "technova-organization-analyzer"

type = "ORGANIZATION"

tags = {

Purpose = "Organization-wide IAM Access Analysis"

Environment = "Multi-Account"

Project = "TechNova-DevSecOps"

}

}

\# アカウントレベルのAccess Analyzer（各アカウント用）

resource "aws_accessanalyzer_analyzer" "account_analyzer" {

analyzer_name = "technova-account-analyzer"

type = "ACCOUNT"

tags = {

Purpose = "Account-level IAM Access Analysis"

Environment = "Multi-Account"

Project = "TechNova-DevSecOps"

}

}

3.2 IAM分析CodeBuildプロジェクト

resource "aws_codebuild_project" "iam_access_analysis" {

name = "iam-access-comprehensive-analysis"

description = "Comprehensive IAM access rights and security analysis"

service_role = aws_iam_role.codebuild_iam_analysis_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_LARGE"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials_type = "CODEBUILD"

environment_variable {

name = "ORGANIZATION_ID"

value = data.aws_organizations_organization.current.id

}

environment_variable {

name = "ACCOUNT_LIST_BUCKET"

value = aws_s3_bucket.security_configs.id

}

environment_variable {

name = "IAM_ANALYSIS_DEPTH"

value = "COMPREHENSIVE"

}

}

source { type = "CODEPIPELINE" buildspec = "buildspecs/iam_analysis.yml"
}

tags = { Stage = "IAMAnalysis" Tool = "AccessAnalyzer" Scope =
"Organization" } }

**IAM分析用の拡張ロール**

resource "aws_iam_role" "codebuild_iam_analysis_role" { name =
"codebuild-iam-analysis-role"

assume_role_policy = jsonencode({ Version = "2012-10-17" Statement = \[
{ Action = "sts:AssumeRole" Effect = "Allow" Principal = { Service =
"codebuild.amazonaws.com" } } \] })

tags = { Purpose = "IAM Analysis CodeBuild Role" Environment =
"Multi-Account" } }

resource "aws_iam_role_policy" "codebuild_iam_analysis_policy" { name =
"codebuild-iam-analysis-policy" role =
aws_iam_role.codebuild_iam_analysis_role.id

policy = jsonencode({ Version = "2012-10-17" Statement = \[ { Effect =
"Allow" Action = \[ "logs:CreateLogGroup", "logs:CreateLogStream",
"logs:PutLogEvents" \] Resource = "arn:aws:logs:*:*:*" }, { Effect =
"Allow" Action = \[ "s3:GetObject", "s3:GetObjectVersion",
"s3:PutObject" \] Resource = \[
"\${aws_s3_bucket.pipeline_artifacts.arn}/*",
"\${aws_s3_bucket.build_logs.arn}/*",
"\${aws_s3_bucket.security_configs.arn}/*" \] }, { Effect = "Allow"
Action = \[ "access-analyzer:ListAnalyzers",
"access-analyzer:ListFindings", "access-analyzer:GetFinding",
"access-analyzer:GetAnalyzer", "access-analyzer:ListArchiveRules" \]
Resource = "*" }, { Effect = "Allow" Action = \[ "iam:ListRoles",
"iam:ListUsers", "iam:ListGroups", "iam:ListPolicies", "iam:GetRole",
"iam:GetUser", "iam:GetGroup", "iam:GetPolicy", "iam:GetPolicyVersion",
"iam:ListRolePolicies", "iam:ListUserPolicies", "iam:ListGroupPolicies",
"iam:ListAttachedRolePolicies", "iam:ListAttachedUserPolicies",
"iam:ListAttachedGroupPolicies", "iam:GetRolePolicy",
"iam:GetUserPolicy", "iam:GetGroupPolicy",
"iam:SimulatePrincipalPolicy", "iam:GetAccountSummary" \] Resource = "*"
}, { Effect = "Allow" Action = \[ "organizations:ListAccounts",
"organizations:DescribeOrganization", "organizations:ListRoots",
"organizations:ListOrganizationalUnitsForParent",
"organizations:ListAccountsForParent" \] Resource = "*" }, { Effect =
"Allow" Action = \[ "sts:AssumeRole" \] Resource =
"arn:aws:iam::*:role/TechNova-CrossAccount-SecurityAnalysis-Role" }, {
Effect = "Allow" Action = \[ "cloudwatch:PutMetricData" \] Resource =
"\*" Condition = { StringEquals = { "cloudwatch:namespace" =
"TechNova/IAMSecurity" } } } \] }) }

\### 3.3 iam_analysis.yml（包括的IAM分析）

\`\`\`yaml

version: 0.2

env:

variables:

LOG_FILE: "iam-analysis-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

FINDINGS_FILE: "iam-findings.json"

CROSS_ACCOUNT_FINDINGS_FILE: "cross-account-findings.json"

UNUSED_PERMISSIONS_FILE: "unused-permissions.json"

EXTERNAL_ACCESS_FILE: "external-access-findings.json"

SUMMARY_FILE: "iam-analysis-summary.json"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up IAM analysis environment..." \| tee
logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# 必要なツールのインストール

yum update -y

yum install -y jq bc python3-pip

pip3 install boto3 botocore awscli

\# カスタムIAM分析スクリプトの準備

aws s3 cp s3://\$ACCOUNT_LIST_BUCKET/scripts/iam-analyzer.py
iam-analyzer.py \|\| echo "Using default analysis script"

if \[ ! -f iam-analyzer.py \]; then

echo "\[INSTALL\] Creating default IAM analysis script" \| tee -a
logs/\$LOG_FILE

cat \> iam-analyzer.py \<\< 'EOF'

\#!/usr/bin/env python3

import boto3

import json

import sys

from datetime import datetime, timedelta

from botocore.exceptions import ClientError

class IAMSecurityAnalyzer:

def \_\_init\_\_(self):

self.session = boto3.Session()

self.iam = self.session.client('iam')

self.access_analyzer = self.session.client('accessanalyzer')

self.organizations = self.session.client('organizations')

self.sts = self.session.client('sts')

def analyze_organization_accounts(self):

"""120アカウント全体のIAM分析"""

try:

response = self.organizations.list_accounts()

accounts = response\['Accounts'\]

print(f"Found {len(accounts)} accounts in organization")

return accounts

except ClientError as e:

print(f"Error listing accounts: {e}")

return \[\]

def analyze_external_access(self, analyzer_arn):

"""外部アクセス分析"""

findings = \[\]

try:

paginator = self.access_analyzer.get_paginator('list_findings')

for page in paginator.paginate(analyzerArn=analyzer_arn):

findings.extend(page\['findings'\])

external_findings = \[\]

for finding in findings:

if finding\['status'\] == 'ACTIVE':

external_findings.append({

'id': finding\['id'\],

'resource_type': finding\['resourceType'\],

'resource': finding\['resource'\],

'principal': finding.get('principal', {}),

'action': finding.get('action', \[\]),

'condition': finding.get('condition', {}),

'created_at': finding\['createdAt'\].isoformat(),

'updated_at': finding\['updatedAt'\].isoformat(),

'status': finding\['status'\]

})

return external_findings

except ClientError as e:

print(f"Error analyzing external access: {e}")

return \[\]

def analyze_unused_permissions(self):

"""未使用権限の分析"""

unused_permissions = \[\]

try:

\# ロール一覧取得

paginator = self.iam.get_paginator('list_roles')

for page in paginator.paginate():

for role in page\['Roles'\]:

role_name = role\['RoleName'\]

last_used = role.get('RoleLastUsed', {})

\# 90日以上未使用のロールを特定

if 'LastUsedDate' in last_used:

last_used_date = last_used\['LastUsedDate'\]

if datetime.now(last_used_date.tzinfo) - last_used_date \>
timedelta(days=90):

unused_permissions.append({

'type': 'role',

'name': role_name,

'arn': role\['Arn'\],

'last_used': last_used_date.isoformat(),

'days_unused': (datetime.now(last_used_date.tzinfo) -
last_used_date).days,

'created_date': role\['CreateDate'\].isoformat()

})

else:

\# 未使用（使用履歴なし）

created_date = role\['CreateDate'\]

days_since_creation = (datetime.now(created_date.tzinfo) -
created_date).days

if days_since_creation \> 30: \# 作成から30日以上経過

unused_permissions.append({

'type': 'role',

'name': role_name,

'arn': role\['Arn'\],

'last_used': 'never',

'days_unused': days_since_creation,

'created_date': created_date.isoformat()

})

\# ユーザー分析

paginator = self.iam.get_paginator('list_users')

for page in paginator.paginate():

for user in page\['Users'\]:

user_name = user\['UserName'\]

\# アクセスキーの最終使用日確認

try:

access_keys = self.iam.list_access_keys(UserName=user_name)

for key in access_keys\['AccessKeyMetadata'\]:

key_id = key\['AccessKeyId'\]

last_used_response =
self.iam.get_access_key_last_used(AccessKeyId=key_id)

last_used_info = last_used_response\['AccessKeyLastUsed'\]

if 'LastUsedDate' in last_used_info:

last_used_date = last_used_info\['LastUsedDate'\]

if datetime.now(last_used_date.tzinfo) - last_used_date \>
timedelta(days=90):

unused_permissions.append({

'type': 'user_access_key',

'name': user_name,

'access_key_id': key_id,

'last_used': last_used_date.isoformat(),

'days_unused': (datetime.now(last_used_date.tzinfo) -
last_used_date).days,

'created_date': key\['CreateDate'\].isoformat()

})

else:

created_date = key\['CreateDate'\]

days_since_creation = (datetime.now(created_date.tzinfo) -
created_date).days

if days_since_creation \> 30:

unused_permissions.append({

'type': 'user_access_key',

'name': user_name,

'access_key_id': key_id,

'last_used': 'never',

'days_unused': days_since_creation,

'created_date': created_date.isoformat()

})

except ClientError as e:

print(f"Error analyzing user {user_name}: {e}")

continue

return unused_permissions

except ClientError as e:

print(f"Error analyzing unused permissions: {e}")

return \[\]

def analyze_cross_account_access(self):

"""クロスアカウントアクセス分析"""

cross_account_findings = \[\]

try:

\# 現在のアカウントID取得

current_account = self.sts.get_caller_identity()\['Account'\]

\# ロール一覧取得してクロスアカウント信頼関係を分析

paginator = self.iam.get_paginator('list_roles')

for page in paginator.paginate():

for role in page\['Roles'\]:

assume_role_policy = role\['AssumeRolePolicyDocument'\]

\# URLエンコードされたポリシーをデコード

import urllib.parse

policy_doc = json.loads(urllib.parse.unquote(assume_role_policy))

for statement in policy_doc.get('Statement', \[\]):

principal = statement.get('Principal', {})

\# AWS principalの分析

if isinstance(principal, dict) and 'AWS' in principal:

aws_principals = principal\['AWS'\]

if isinstance(aws_principals, str):

aws_principals = \[aws_principals\]

for aws_principal in aws_principals:

if ':' in aws_principal and aws_principal.split(':')\[4\] !=
current_account:

cross_account_findings.append({

'role_name': role\['RoleName'\],

'role_arn': role\['Arn'\],

'trusted_principal': aws_principal,

'trusted_account': aws_principal.split(':')\[4\] if ':' in aws_principal
else 'unknown',

'statement_effect': statement.get('Effect', 'Allow'),

'conditions': statement.get('Condition', {}),

'created_date': role\['CreateDate'\].isoformat()

})

return cross_account_findings

except ClientError as e:

print(f"Error analyzing cross-account access: {e}")

return \[\]

if \_\_name\_\_ == "\_\_main\_\_":

analyzer = IAMSecurityAnalyzer()

\# 分析実行

print("Starting comprehensive IAM security analysis...")

accounts = analyzer.analyze_organization_accounts()

print(f"Organization analysis: {len(accounts)} accounts")

analyzer_arn = sys.argv\[1\] if len(sys.argv) \> 1 else None

if analyzer_arn:

external_access = analyzer.analyze_external_access(analyzer_arn)

print(f"External access analysis: {len(external_access)} findings")

with open('logs/external-access-findings.json', 'w') as f:

json.dump(external_access, f, indent=2, default=str)

unused_permissions = analyzer.analyze_unused_permissions()

print(f"Unused permissions analysis: {len(unused_permissions)}
findings")

with open('logs/unused-permissions.json', 'w') as f:

json.dump(unused_permissions, f, indent=2, default=str)

cross_account_access = analyzer.analyze_cross_account_access()

print(f"Cross-account access analysis: {len(cross_account_access)}
findings")

with open('logs/cross-account-findings.json', 'w') as f:

json.dump(cross_account_access, f, indent=2, default=str)

\# サマリー作成

summary = {

'timestamp': datetime.now().isoformat(),

'organization_accounts': len(accounts),

'external_access_findings': len(external_access) if analyzer_arn else 0,

'unused_permissions': len(unused_permissions),

'cross_account_trusts': len(cross_account_access),

'total_security_findings': len(external_access) +
len(unused_permissions) + len(cross_account_access) if analyzer_arn else
len(unused_permissions) + len(cross_account_access)

}

with open('logs/iam-analysis-summary.json', 'w') as f:

json.dump(summary, f, indent=2)

print("IAM security analysis completed")

EOF

chmod +x iam-analyzer.py

fi

pre_build:

commands:

\- echo "\[PRE_BUILD\] Preparing IAM analysis configuration..." \| tee
-a logs/\$LOG_FILE

\- \|

\# Access Analyzerの状態確認

if \[ ! -z "\$ANALYZER_ARN" \]; then

aws accessanalyzer get-analyzer --analyzer-arn \$ANALYZER_ARN \>\>
logs/\$LOG_FILE 2\>&1

ANALYZER_STATUS=\$?

if \[ \$ANALYZER_STATUS -ne 0 \]; then

echo "\[PRE_BUILD\] Warning: Access Analyzer not accessible, skipping
external access analysis" \| tee -a logs/\$LOG_FILE

export ANALYZER_ARN=""

fi

fi

\# 組織アカウント一覧の取得

aws organizations list-accounts \> logs/organization-accounts.json
2\>/dev/null \|\| echo "\[\]" \> logs/organization-accounts.json

ACCOUNT_COUNT=\$(jq length logs/organization-accounts.json)

echo "\[PRE_BUILD\] Found \$ACCOUNT_COUNT accounts in organization" \|
tee -a logs/\$LOG_FILE

build:

commands:

\- echo "\[BUILD\] Starting comprehensive IAM security analysis..." \|
tee -a logs/\$LOG_FILE

\- \|

\# Python分析スクリプト実行

python3 iam-analyzer.py "\$ANALYZER_ARN" \>\> logs/\$LOG_FILE 2\>&1

ANALYSIS_EXIT_CODE=\$?

if \[ \$ANALYSIS_EXIT_CODE -ne 0 \]; then

echo "\[ERROR\] IAM analysis script failed with exit code
\$ANALYSIS_EXIT_CODE" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="IAM_ANALYSIS_SCRIPT_FAILED"

export BUILD_EXIT_CODE=\$ANALYSIS_EXIT_CODE

fi

\# 分析結果の検証と統計

if \[ -f logs/\$SUMMARY_FILE \]; then

echo "\[BUILD\] Processing analysis results..." \| tee -a
logs/\$LOG_FILE

\# サマリー情報の抽出

TOTAL_ACCOUNTS=\$(jq '.organization_accounts' logs/\$SUMMARY_FILE)

EXTERNAL_FINDINGS=\$(jq '.external_access_findings' logs/\$SUMMARY_FILE)

UNUSED_PERMISSIONS=\$(jq '.unused_permissions' logs/\$SUMMARY_FILE)

CROSS_ACCOUNT_TRUSTS=\$(jq '.cross_account_trusts' logs/\$SUMMARY_FILE)

TOTAL_FINDINGS=\$(jq '.total_security_findings' logs/\$SUMMARY_FILE)

echo "\[BUILD\] IAM Security Analysis Results:" \| tee -a
logs/\$LOG_FILE

echo " - Organization Accounts: \$TOTAL_ACCOUNTS" \| tee -a
logs/\$LOG_FILE

echo " - External Access Findings: \$EXTERNAL_FINDINGS" \| tee -a
logs/\$LOG_FILE

echo " - Unused Permissions: \$UNUSED_PERMISSIONS" \| tee -a
logs/\$LOG_FILE

echo " - Cross-Account Trusts: \$CROSS_ACCOUNT_TRUSTS" \| tee -a
logs/\$LOG_FILE

echo " - Total Security Findings: \$TOTAL_FINDINGS" \| tee -a
logs/\$LOG_FILE

\# リスクレベルの判定

HIGH_RISK_THRESHOLD=50

MEDIUM_RISK_THRESHOLD=20

if \[ \$TOTAL_FINDINGS -gt \$HIGH_RISK_THRESHOLD \]; then

echo "\[ERROR\] High risk: \$TOTAL_FINDINGS security findings exceed
threshold (\$HIGH_RISK_THRESHOLD)" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="HIGH_RISK_IAM_FINDINGS"

export BUILD_EXIT_CODE=1

export RISK_LEVEL="HIGH"

elif \[ \$TOTAL_FINDINGS -gt \$MEDIUM_RISK_THRESHOLD \]; then

echo "\[WARNING\] Medium risk: \$TOTAL_FINDINGS security findings exceed
threshold (\$MEDIUM_RISK_THRESHOLD)" \| tee -a logs/\$LOG_FILE

export BUILD_ERROR="MEDIUM_RISK_IAM_FINDINGS"

export BUILD_EXIT_CODE=1

export RISK_LEVEL="MEDIUM"

else

echo "\[SUCCESS\] Low risk: \$TOTAL_FINDINGS security findings within
acceptable limits" \| tee -a logs/\$LOG_FILE

export RISK_LEVEL="LOW"

fi

\# 重要な外部アクセス検出の詳細分析

if \[ -f logs/\$EXTERNAL_ACCESS_FILE \] && \[ \$EXTERNAL_FINDINGS -gt 0
\]; then

echo "\[BUILD\] Analyzing external access details..." \| tee -a
logs/\$LOG_FILE

\# 重要度の高い外部アクセスを特定

CRITICAL_EXTERNAL=\$(jq '\[.\[\] \| select(.resource_type ==
"AWS::S3::Bucket" or .resource_type == "AWS::IAM::Role")\] \| length'
logs/\$EXTERNAL_ACCESS_FILE)

if \[ \$CRITICAL_EXTERNAL -gt 0 \]; then

echo "\[ERROR\] Found \$CRITICAL_EXTERNAL critical external access
findings" \| tee -a logs/\$LOG_FILE

echo "\[BUILD\] Critical External Access Details:" \| tee -a
logs/\$LOG_FILE

jq -r '.\[\] \| select(.resource_type == "AWS::S3::Bucket" or
.resource_type == "AWS::IAM::Role") \| "Resource: \\.resource) \| Type:
\\.resource_type) \| Principal: \\.principal)"'
logs/\$EXTERNAL_ACCESS_FILE \| head -10 \| tee -a logs/\$LOG_FILE

if \[ -z "\$BUILD_ERROR" \]; then

export BUILD_ERROR="CRITICAL_EXTERNAL_ACCESS_DETECTED"

export BUILD_EXIT_CODE=1

fi

fi

fi

\# 未使用権限の詳細分析

if \[ -f logs/\$UNUSED_PERMISSIONS_FILE \] && \[ \$UNUSED_PERMISSIONS
-gt 0 \]; then

echo "\[BUILD\] Analyzing unused permissions details..." \| tee -a
logs/\$LOG_FILE

\# 長期間未使用の権限を特定

LONG_UNUSED=\$(jq '\[.\[\] \| select(.days_unused \> 180)\] \| length'
logs/\$UNUSED_PERMISSIONS_FILE)

NEVER_USED=\$(jq '\[.\[\] \| select(.last_used == "never")\] \| length'
logs/\$UNUSED_PERMISSIONS_FILE)

echo " - Long-term unused (\>180 days): \$LONG_UNUSED" \| tee -a
logs/\$LOG_FILE

echo " - Never used: \$NEVER_USED" \| tee -a logs/\$LOG_FILE

if \[ \$LONG_UNUSED -gt 10 \]; then

echo "\[WARNING\] Excessive long-term unused permissions detected" \|
tee -a logs/\$LOG_FILE

fi

fi

\# クロスアカウントアクセスの分析

if \[ -f logs/\$CROSS_ACCOUNT_FINDINGS_FILE \] && \[
\$CROSS_ACCOUNT_TRUSTS -gt 0 \]; then

echo "\[BUILD\] Analyzing cross-account access details..." \| tee -a
logs/\$LOG_FILE

\# 外部アカウントからの信頼関係を特定

EXTERNAL_TRUSTS=\$(jq '\[.\[\] \| select(.trusted_account !=
"'\$AWS_ACCOUNT_ID'")\] \| length' logs/\$CROSS_ACCOUNT_FINDINGS_FILE
2\>/dev/null \|\| echo "0")

if \[ \$EXTERNAL_TRUSTS -gt 0 \]; then

echo "\[WARNING\] Found \$EXTERNAL_TRUSTS external account trust
relationships" \| tee -a logs/\$LOG_FILE

echo "\[BUILD\] External Trust Details:" \| tee -a logs/\$LOG_FILE

jq -r '.\[\] \| select(.trusted_account != "'\$AWS_ACCOUNT_ID'") \|
"Role: \\.role_name) \| Trusted Account: \\.trusted_account) \|
Principal: \\.trusted_principal)"' logs/\$CROSS_ACCOUNT_FINDINGS_FILE \|
head -5 \| tee -a logs/\$LOG_FILE

fi

fi

else

echo "\[ERROR\] IAM analysis summary not found" \| tee -a
logs/\$LOG_FILE

export BUILD_ERROR="IAM_ANALYSIS_INCOMPLETE"

export BUILD_EXIT_CODE=1

fi

post_build:

commands:

\- echo "\[POST_BUILD\] Processing IAM analysis results and generating
reports..." \| tee -a logs/\$LOG_FILE

\- \|

\# CloudWatchメトリクス送信

if \[ -f logs/\$SUMMARY_FILE \]; then

aws cloudwatch put-metric-data \\

--namespace "TechNova/IAMSecurity" \\

--metric-data \\

MetricName=OrganizationAccounts,Value=\$TOTAL_ACCOUNTS,Unit=Count \\

MetricName=ExternalAccessFindings,Value=\$EXTERNAL_FINDINGS,Unit=Count
\\

MetricName=UnusedPermissions,Value=\$UNUSED_PERMISSIONS,Unit=Count \\

MetricName=CrossAccountTrusts,Value=\$CROSS_ACCOUNT_TRUSTS,Unit=Count \\

MetricName=TotalSecurityFindings,Value=\$TOTAL_FINDINGS,Unit=Count \\

--region \$AWS_DEFAULT_REGION \|\| echo "CloudWatch metrics failed"

fi

\# 高リスクの場合はSNS通知

if \[ "\$RISK_LEVEL" = "HIGH" \] \|\| \[ ! -z "\$BUILD_ERROR" \] && \[\[
"\$BUILD_ERROR" == \*"CRITICAL"\* \]\]; then

aws sns publish \\

--topic-arn \$CRITICAL_SECURITY_ALERTS_TOPIC \\

--subject "CRITICAL: High-Risk IAM Security Findings Detected" \\

--message "Pipeline: \$PIPELINE_EXECUTION_ID\nRisk Level:
\$RISK_LEVEL\nTotal Findings: \$TOTAL_FINDINGS\nExternal Access:
\$EXTERNAL_FINDINGS\nUnused Permissions:
\$UNUSED_PERMISSIONS\nCross-Account Trusts:
\$CROSS_ACCOUNT_TRUSTS\nImmediate review required." \\

--region \$AWS_DEFAULT_REGION \|\| echo "SNS notification failed"

fi

\# 結果ファイルをS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "Log upload failed"

for file in \$SUMMARY_FILE \$EXTERNAL_ACCESS_FILE
\$UNUSED_PERMISSIONS_FILE \$CROSS_ACCOUNT_FINDINGS_FILE; do

if \[ -f logs/\$file \]; then

aws s3 cp logs/\$file s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$file \|\| echo
"\$file upload failed"

fi

done

\# 組織アカウント情報もアップロード

if \[ -f logs/organization-accounts.json \]; then

aws s3 cp logs/organization-accounts.json
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/organization-accounts.json \|\| echo
"Organization accounts upload failed"

fi

\# HTMLレポート生成

if \[ -f logs/\$SUMMARY_FILE \]; then

cat \> logs/iam-security-report.html \<\< EOF

\<!DOCTYPE html\>

\<html\>

\<head\>

\<title\>IAM Security Analysis Report - Pipeline
\$PIPELINE_EXECUTION_ID\</title\>

\<style\>

body { font-family: Arial, sans-serif; margin: 40px; }

.header { background: \#f0f0f0; padding: 20px; border-radius: 5px; }

.section { margin: 20px 0; padding: 15px; border-left: 4px solid
\#007cba; }

.high-risk { border-left-color: \#d32f2f; background: \#ffebee; }

.medium-risk { border-left-color: \#f57c00; background: \#fff3e0; }

.low-risk { border-left-color: \#388e3c; background: \#e8f5e8; }

.metric { display: inline-block; margin: 10px; padding: 10px;
background: \#f5f5f5; border-radius: 3px; }

\</style\>

\</head\>

\<body\>

\<div class="header"\>

\<h1\>IAM Security Analysis Report\</h1\>

\<p\>Pipeline Execution: \$PIPELINE_EXECUTION_ID\</p\>

\<p\>Analysis Date: \$(date)\</p\>

\<p\>Risk Level: \<strong\>\$RISK_LEVEL\</strong\>\</p\>

\</div\>

\<div class="section \${RISK_LEVEL,,}-risk"\>

\<h2\>Summary Metrics\</h2\>

\<div class="metric"\>Organization Accounts:
\<strong\>\$TOTAL_ACCOUNTS\</strong\>\</div\>

\<div class="metric"\>External Access Findings:
\<strong\>\$EXTERNAL_FINDINGS\</strong\>\</div\>

\<div class="metric"\>Unused Permissions:
\<strong\>\$UNUSED_PERMISSIONS\</strong\>\</div\>

\<div class="metric"\>Cross-Account Trusts:
\<strong\>\$CROSS_ACCOUNT_TRUSTS\</strong\>\</div\>

\<div class="metric"\>Total Security Findings:
\<strong\>\$TOTAL_FINDINGS\</strong\>\</div\>

\</div\>

\<div class="section"\>

\<h2\>Detailed Analysis\</h2\>

\<p\>Complete analysis results are available in the pipeline artifacts
and S3 logs.\</p\>

\<ul\>

\<li\>External Access Details: external-access-findings.json\</li\>

\<li\>Unused Permissions: unused-permissions.json\</li\>

\<li\>Cross-Account Findings: cross-account-findings.json\</li\>

\<li\>Full Log: \$LOG_FILE\</li\>

\</ul\>

\</div\>

\</body\>

\</html\>

EOF

aws s3 cp logs/iam-security-report.html
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/iam-security-report.html \|\| echo
"HTML report upload failed"

fi

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Stopping pipeline due to IAM security violations"
\| tee -a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Continuing pipeline despite IAM security
violations" \| tee -a logs/\$LOG_FILE

exit 0

else

exit 0

fi

artifacts:

files:

\- logs/\*

name: iam-security-analysis-results

**4. 動的解析の詳細実装**

**4.1 動的脆弱性スキャン（インフラ）**

resource "aws_codebuild_project" "dynamic_vuln_scan" {

name = "dynamic-infrastructure-vulnerability-scan"

description = "Dynamic infrastructure vulnerability scanning using
multiple tools"

service_role = aws_iam_role.codebuild_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_LARGE"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials\_

image_pull_credentials_type = "CODEBUILD"

privileged_mode = true

environment_variable {

name = "SCAN_TARGET_ENVIRONMENT"

value = "staging"

}

environment_variable {

name = "VULNERABILITY_TOOLS"

value = "nmap,nuclei,testssl"

}

environment_variable {

name = "SCAN_INTENSITY"

value = "COMPREHENSIVE"

}

environment_variable {

name = "MAX_SCAN_DURATION"

value = "3600" \# 1時間

}

}

source {

type = "CODEPIPELINE"

buildspec = "buildspecs/dynamic_vuln_scan.yml"

}

timeout_in_minutes = 90

tags = {

Stage = "DynamicAnalysis"

Tool = "VulnerabilityScanner"

Target = "Infrastructure"

}

}

4.2 dynamic_vuln_scan.yml（包括的インフラ脆弱性スキャン）

version: 0.2

env:

variables:

LOG_FILE: "dynamic-vuln-scan-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

NMAP_RESULTS: "nmap-scan-results.xml"

NUCLEI_RESULTS: "nuclei-scan-results.json"

TESTSSL_RESULTS: "testssl-scan-results.json"

CONSOLIDATED_RESULTS: "vulnerability-scan-results.json"

SUMMARY_FILE: "vulnerability-scan-summary.json"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up vulnerability scanning environment..."
\| tee logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# システム更新

yum update -y

yum install -y docker jq bc nmap git golang

\# Docker起動

service docker start

usermod -a -G docker codebuild

\# Nucleiインストール

GO111MODULE=on go install -v
github.com/projectdiscovery/nuclei/v2/cmd/nuclei@latest

export PATH=\$PATH:/root/go/bin

nuclei -version \| tee -a logs/\$LOG_FILE

\# Nucleiテンプレート更新

nuclei -update-templates

\# testssl.shインストール

git clone --depth 1 https://github.com/drwetter/testssl.sh.git

chmod +x testssl.sh/testssl.sh

\# カスタムスキャン設定の取得

aws s3 cp
s3://\$SECURITY_CONFIGS_BUCKET/vuln-scan-config/scan-targets.json
scan-targets.json \|\| echo '{"targets": \[\]}' \> scan-targets.json

aws s3 cp s3://\$SECURITY_CONFIGS_BUCKET/vuln-scan-config/exclusions.txt
exclusions.txt \|\| touch exclusions.txt

pre_build:

commands:

\- echo "\[PRE_BUILD\] Preparing vulnerability scan targets..." \| tee
-a logs/\$LOG_FILE

\- \|

\# ターゲット環境のリソース検出

echo "\[PRE_BUILD\] Discovering scan targets in \$TARGET_ENVIRONMENT
environment" \| tee -a logs/\$LOG_FILE

\# ALB/ELBエンドポイント取得

aws elbv2 describe-load-balancers --query
'LoadBalancers\[?State.Code==\`active\`\].\[DNSName,Type\]' --output
text \| while read dns_name type; do

echo "ALB/NLB: \$dns_name (\$type)" \| tee -a logs/\$LOG_FILE

echo "\$dns_name" \>\> scan-targets-discovered.txt

done

\# CloudFrontディストリビューション取得

aws cloudfront list-distributions --query
'DistributionList.Items\[?Status==\`Deployed\`\].DomainName' --output
text \| tr '\t' '\n' \| while read domain; do

if \[ ! -z "\$domain" \]; then

echo "CloudFront: \$domain" \| tee -a logs/\$LOG_FILE

echo "\$domain" \>\> scan-targets-discovered.txt

fi

done

\# API Gatewayエンドポイント取得

aws apigateway get-rest-apis --query
'items\[?endpointConfiguration.types\[0\]==\`REGIONAL\`\].\[id,name\]'
--output text \| while read api_id name; do

api_endpoint="\${api_id}.execute-api.\${AWS_DEFAULT_REGION}.amazonaws.com"

echo "API Gateway: \$api_endpoint (\$name)" \| tee -a logs/\$LOG_FILE

echo "\$api_endpoint" \>\> scan-targets-discovered.txt

done

\# EC2インスタンスのパブリックIP取得（注意深く）

aws ec2 describe-instances \\

--filters "Name=instance-state-name,Values=running"
"Name=tag:Environment,Values=\$TARGET_ENVIRONMENT" \\

--query
'Reservations\[\].Instances\[?PublicIpAddress\].\[PublicIpAddress,Tags\[?Key==\`Name\`\].Value\|\[0\]\]'
\\

--output text \| while read ip name; do

if \[ ! -z "\$ip" \] && \[ "\$ip" != "None" \]; then

echo "EC2 Instance: \$ip (\$name)" \| tee -a logs/\$LOG_FILE

echo "\$ip" \>\> scan-targets-discovered.txt

fi

done

\# 重複除去とソート

if \[ -f scan-targets-discovered.txt \]; then

sort -u scan-targets-discovered.txt \> scan-targets-final.txt

TARGET_COUNT=\$(wc -l \< scan-targets-final.txt)

echo "\[PRE_BUILD\] Found \$TARGET_COUNT scan targets" \| tee -a
logs/\$LOG_FILE

else

echo "\[PRE_BUILD\] No scan targets discovered" \| tee -a
logs/\$LOG_FILE

touch scan-targets-final.txt

fi

\# カスタム設定からのターゲット追加

if \[ -f scan-targets.json \]; then

jq -r '.targets\[\]' scan-targets.json \>\> scan-targets-final.txt

sort -u scan-targets-final.txt \> scan-targets-temp.txt

mv scan-targets-temp.txt scan-targets-final.txt

fi

build:

commands:

\- echo "\[BUILD\] Starting comprehensive vulnerability scanning..." \|
tee -a logs/\$LOG_FILE

\- \|

SCAN_START_TIME=\$(date +%s)

MAX_DURATION=\${MAX_SCAN_DURATION:-3600}

\# 結果ファイル初期化

echo '{"nmap_results": \[\], "nuclei_results": \[\], "testssl_results":
\[\], "summary": {}}' \> logs/\$CONSOLIDATED_RESULTS

\# スキャン対象がない場合の処理

if \[ ! -s scan-targets-final.txt \]; then

echo "\[BUILD\] No targets to scan, creating empty results" \| tee -a
logs/\$LOG_FILE

echo '{"targets_scanned": 0, "vulnerabilities_found": 0,
"scan_duration": 0}' \> logs/\$SUMMARY_FILE

else

echo "\[BUILD\] Starting scans on \$(wc -l \< scan-targets-final.txt)
targets" \| tee -a logs/\$LOG_FILE

\# 並列スキャン実行

while IFS= read -r target; do

CURRENT_TIME=\$(date +%s)

ELAPSED_TIME=\$((CURRENT_TIME - SCAN_START_TIME))

if \[ \$ELAPSED_TIME -gt \$MAX_DURATION \]; then

echo "\[BUILD\] Maximum scan duration reached, stopping scans" \| tee -a
logs/\$LOG_FILE

break

fi

if \[ -z "\$target" \]; then

continue

fi

echo "\[BUILD\] Scanning target: \$target" \| tee -a logs/\$LOG_FILE

\# Nmapスキャン（ポートスキャン・サービス検出）

if \[\[ "\$VULNERABILITY_TOOLS" == \*"nmap"\* \]\]; then

echo "\[BUILD\] Running Nmap scan on \$target" \| tee -a logs/\$LOG_FILE

nmap -sS -sV -O -A --script=vuln,safe,discovery \\

-oX logs/nmap-\${target//\[^a-zA-Z0-9\]/\_}.xml \\

-oN logs/nmap-\${target//\[^a-zA-Z0-9\]/\_}.txt \\

--max-retries 2 --host-timeout 10m \\

\$target \>\> logs/\$LOG_FILE 2\>&1 &

NMAP_PID=\$!

echo "Nmap PID: \$NMAP_PID for target \$target" \| tee -a
logs/\$LOG_FILE

fi

\# Nucleiスキャン（脆弱性テンプレート）

if \[\[ "\$VULNERABILITY_TOOLS" == \*"nuclei"\* \]\]; then

echo "\[BUILD\] Running Nuclei scan on \$target" \| tee -a
logs/\$LOG_FILE

nuclei -u "http://\$target" -u "https://\$target" \\

-severity critical,high,medium \\

-json -o logs/nuclei-\${target//\[^a-zA-Z0-9\]/\_}.json \\

-timeout 30 \\

-retries 2 \\

-rate-limit 10 \>\> logs/\$LOG_FILE 2\>&1 &

NUCLEI_PID=\$!

echo "Nuclei PID: \$NUCLEI_PID for target \$target" \| tee -a
logs/\$LOG_FILE

fi

\# testssl.shスキャン（SSL/TLS設定）

if \[\[ "\$VULNERABILITY_TOOLS" == \*"testssl"\* \]\]; then

echo "\[BUILD\] Running testssl.sh scan on \$target" \| tee -a
logs/\$LOG_FILE

./testssl.sh/testssl.sh --jsonfile
logs/testssl-\${target//\[^a-zA-Z0-9\]/\_}.json \\

--logfile logs/testssl-\${target//\[^a-zA-Z0-9\]/\_}.log \\

--severity HIGH \\

--fast \\

\$target \>\> logs/\$LOG_FILE 2\>&1 &

TESTSSL_PID=\$!

echo "testssl PID: \$TESTSSL_PID for target \$target" \| tee -a
logs/\$LOG_FILE

fi

\# 同時実行数制限（最大5並列）

RUNNING_JOBS=\$(jobs -r \| wc -l)

if \[ \$RUNNING_JOBS -ge 15 \]; then

echo "\[BUILD\] Waiting for scan slots to free up..." \| tee -a
logs/\$LOG_FILE

wait -n \# 任意のジョブ完了まで待機

fi

done \< scan-targets-final.txt

\# 全スキャン完了待機

echo "\[BUILD\] Waiting for all scans to complete..." \| tee -a
logs/\$LOG_FILE

wait

\# 結果の統合と分析

echo "\[BUILD\] Consolidating scan results..." \| tee -a logs/\$LOG_FILE

TOTAL_VULNERABILITIES=0

HIGH_SEVERITY_VULNS=0

MEDIUM_SEVERITY_VULNS=0

LOW_SEVERITY_VULNS=0

\# Nmapの結果解析

for nmap_file in logs/nmap-\*.xml; do

if \[ -f "\$nmap_file" \]; then

\# XMLから脆弱性情報を抽出

NMAP_VULNS=\$(grep -c "VULNERABLE" "\$nmap_file" 2\>/dev/null \|\| echo
"0")

TOTAL_VULNERABILITIES=\$((TOTAL_VULNERABILITIES + NMAP_VULNS))

echo "\[BUILD\] Nmap found \$NMAP_VULNS vulnerabilities in \$nmap_file"
\| tee -a logs/\$LOG_FILE

fi

done

\# Nucleiの結果解析

for nuclei_file in logs/nuclei-\*.json; do

if \[ -f "\$nuclei_file" \] && \[ -s "\$nuclei_file" \]; then

CRITICAL_NUCLEI=\$(jq '\[.\[\] \| select(.info.severity == "critical")\]
\| length' "\$nuclei_file" 2\>/dev/null \|\| echo "0")

HIGH_NUCLEI=\$(jq '\[.\[\] \| select(.info.severity == "high")\] \|
length' "\$nuclei_file" 2\>/dev/null \|\| echo "0")

MEDIUM_NUCLEI=\$(jq '\[.\[\] \| select(.info.severity == "medium")\] \|
length' "\$nuclei_file" 2\>/dev/null \|\| echo "0")

HIGH_SEVERITY_VULNS=\$((HIGH_SEVERITY_VULNS + CRITICAL_NUCLEI +
HIGH_NUCLEI))

MEDIUM_SEVERITY_VULNS=\$((MEDIUM_SEVERITY_VULNS + MEDIUM_NUCLEI))

TOTAL_VULNERABILITIES=\$((TOTAL_VULNERABILITIES + CRITICAL_NUCLEI +
HIGH_NUCLEI + MEDIUM_NUCLEI))

echo "\[BUILD\] Nuclei found Critical:\$CRITICAL_NUCLEI
High:\$HIGH_NUCLEI Medium:\$MEDIUM_NUCLEI in \$nuclei_file" \| tee -a
logs/\$LOG_FILE

fi

done

\# testssl.shの結果解析

for testssl_file in logs/testssl-\*.json; do

if \[ -f "\$testssl_file" \] && \[ -s "\$testssl_file" \]; then

SSL_HIGH=\$(jq '\[.\[\] \| select(.severity == "HIGH")\] \| length'
"\$testssl_file" 2\>/dev/null \|\| echo "0")

SSL_MEDIUM=\$(jq '\[.\[\] \| select(.severity == "MEDIUM")\] \| length'
"\$testssl_file" 2\>/dev/null \|\| echo "0")

SSL_LOW=\$(jq '\[.\[\] \| select(.severity == "LOW")\] \| length'
"\$testssl_file" 2\>/dev/null \|\| echo "0")

HIGH_SEVERITY_VULNS=\$((HIGH_SEVERITY_VULNS + SSL_HIGH))

MEDIUM_SEVERITY_VULNS=\$((MEDIUM_SEVERITY_VULNS + SSL_MEDIUM))

LOW_SEVERITY_VULNS=\$((LOW_SEVERITY_VULNS + SSL_LOW))

TOTAL_VULNERABILITIES=\$((TOTAL_VULNERABILITIES + SSL_HIGH +
SSL_MEDIUM + SSL_LOW))

echo "\[BUILD\] testssl found High:\$SSL_HIGH Medium:\$SSL_MEDIUM
Low:\$SSL_LOW in \$testssl_file" \| tee -a logs/\$LOG_FILE

fi

done

SCAN_END_TIME=\$(date +%s)

SCAN_DURATION=\$((SCAN_END_TIME - SCAN_START_TIME))

echo "\[BUILD\] Vulnerability Scan Summary:" \| tee -a logs/\$LOG_FILE

echo " - Total Vulnerabilities: \$TOTAL_VULNERABILITIES" \| tee -a
logs/\$LOG_FILE

echo " - High Severity: \$HIGH_SEVERITY_VULNS" \| tee -a logs/\$LOG_FILE

echo " - Medium Severity: \$MEDIUM_SEVERITY_VULNS" \| tee -a
logs/\$LOG_FILE

echo " - Low Severity: \$LOW_SEVERITY_VULNS" \| tee -a logs/\$LOG_FILE

echo " - Scan Duration: \${SCAN_DURATION}s" \| tee -a logs/\$LOG_FILE

\# サマリーJSON作成

cat \> logs/\$SUMMARY_FILE \<\< EOF

{

"pipeline_execution_id": "\$PIPELINE_EXECUTION_ID",

"timestamp": "\$(date -u +%Y-%m-%dT%H:%M:%SZ)",

"targets_scanned": \$(wc -l \< scan-targets-final.txt),

"scan_duration_seconds": \$SCAN_DURATION,

"vulnerabilities_found": \$TOTAL_VULNERABILITIES,

"severity_breakdown": {

"high": \$HIGH_SEVERITY_VULNS,

"medium": \$MEDIUM_SEVERITY_VULNS,

"low": \$LOW_SEVERITY_VULNS

},

"tools_used": "\$VULNERABILITY_TOOLS",

"scan_intensity": "\$SCAN_INTENSITY"

}

EOF

\# リスク判定

CRITICAL_THRESHOLD=10

HIGH_THRESHOLD=25

if \[ \$HIGH_SEVERITY_VULNS -gt \$CRITICAL_THRESHOLD \]; then

echo "\[ERROR\] Critical risk: \$HIGH_SEVERITY_VULNS high severity
vulnerabilities exceed threshold (\$CRITICAL_THRESHOLD)" \| tee -a
logs/\$LOG_FILE

export BUILD_ERROR="CRITICAL_VULNERABILITIES_DETECTED"

export BUILD_EXIT_CODE=1

export RISK_LEVEL="CRITICAL"

elif \[ \$TOTAL_VULNERABILITIES -gt \$HIGH_THRESHOLD \]; then

echo "\[WARNING\] High risk: \$TOTAL_VULNERABILITIES total
vulnerabilities exceed threshold (\$HIGH_THRESHOLD)" \| tee -a
logs/\$LOG_FILE

export BUILD_ERROR="HIGH_RISK_VULNERABILITIES_DETECTED"

export BUILD_EXIT_CODE=1

export RISK_LEVEL="HIGH"

else

echo "\[SUCCESS\] Acceptable risk: \$TOTAL_VULNERABILITIES
vulnerabilities within limits" \| tee -a logs/\$LOG_FILE

export RISK_LEVEL="LOW"

fi

fi

post_build:

commands:

\- echo "\[POST_BUILD\] Processing vulnerability scan results..." \| tee
-a logs/\$LOG_FILE

\- \|

\# CloudWatchメトリクス送信

if \[ -f logs/\$SUMMARY_FILE \]; then

TARGETS_SCANNED=\$(jq '.targets_scanned' logs/\$SUMMARY_FILE)

VULNERABILITIES_FOUND=\$(jq '.vulnerabilities_found'
logs/\$SUMMARY_FILE)

HIGH_SEVERITY=\$(jq '.severity_breakdown.high' logs/\$SUMMARY_FILE)

MEDIUM_SEVERITY=\$(jq '.severity_breakdown.medium' logs/\$SUMMARY_FILE)

aws cloudwatch put-metric-data \\

--namespace "TechNova/VulnerabilityScanning" \\

--metric-data \\

MetricName=TargetsScanned,Value=\$TARGETS_SCANNED,Unit=Count \\

MetricName=VulnerabilitiesFound,Value=\$VULNERABILITIES_FOUND,Unit=Count
\\

MetricName=HighSeverityVulns,Value=\$HIGH_SEVERITY,Unit=Count \\

MetricName=MediumSeverityVulns,Value=\$MEDIUM_SEVERITY,Unit=Count \\

--region \$AWS_DEFAULT_REGION \|\| echo "CloudWatch metrics failed"

fi

\# 重要な脆弱性発見時のSNS通知

if \[ "\$RISK_LEVEL" = "CRITICAL" \] \|\| \[ ! -z "\$BUILD_ERROR" \] &&
\[\[ "\$BUILD_ERROR" == \*"CRITICAL"\* \]\]; then

aws sns publish \\

--topic-arn \$CRITICAL_SECURITY_ALERTS_TOPIC \\

--subject "CRITICAL: High-Risk Vulnerabilities Detected" \\

--message "Pipeline: \$PIPELINE_EXECUTION_ID\nRisk Level:
\$RISK_LEVEL\nHigh Severity Vulnerabilities:
\$HIGH_SEVERITY_VULNS\nTotal Vulnerabilities:
\$TOTAL_VULNERABILITIES\nImmediate remediation required." \\

--region \$AWS_DEFAULT_REGION \|\| echo "SNS notification failed"

fi

\# 全結果ファイルをS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "Log upload failed"

if \[ -f logs/\$SUMMARY_FILE \]; then

aws s3 cp logs/\$SUMMARY_FILE
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$SUMMARY_FILE \|\| echo "Summary
upload failed"

fi

\# スキャン結果ファイルのアップロード

for result_file in logs/nmap-\*.xml logs/nmap-\*.txt logs/nuclei-\*.json
logs/testssl-\*.json logs/testssl-\*.log; do

if \[ -f "\$result_file" \]; then

aws s3 cp "\$result_file" s3://\$S3_LOG_BUCKET/\$S3_PREFIX/ \|\| echo
"\$(basename \$result_file) upload failed"

fi

done

\# 統合レポート生成

if \[ -f logs/\$SUMMARY_FILE \]; then

cat \> logs/vulnerability-scan-report.html \<\< EOF

\<!DOCTYPE html\>

\<html\>

\<head\>

\<title\>Vulnerability Scan Report - Pipeline
\$PIPELINE_EXECUTION_ID\</title\>

\<style\>

body { font-family: Arial, sans-serif; margin: 40px; }

.header { background: \#f0f0f0; padding: 20px; border-radius: 5px; }

.section { margin: 20px 0; padding: 15px; border-left: 4px solid
\#007cba; }

.critical { border-left-color: \#d32f2f; background: \#ffebee; }

.high { border-left-color: \#f57c00; background: \#fff3e0; }

.low { border-left-color: \#388e3c; background: \#e8f5e8; }

.metric { display: inline-block; margin: 10px; padding: 10px;
background: \#f5f5f5; border-radius: 3px; }

.vuln-list { max-height: 300px; overflow-y: auto; background: \#fafafa;
padding: 10px; border-radius: 3px; }

\</style\>

\</head\>

\<body\>

\<div class="header"\>

\<h1\>Infrastructure Vulnerability Scan Report\</h1\>

\<p\>Pipeline Execution: \$PIPELINE_EXECUTION_ID\</p\>

\<p\>Scan Date: \$(date)\</p\>

\<p\>Risk Level: \<strong\>\$RISK_LEVEL\</strong\>\</p\>

\</div\>

\<div class="section \${RISK_LEVEL,,}"\>

\<h2\>Scan Summary\</h2\>

\<div class="metric"\>Targets Scanned:
\<strong\>\$TARGETS_SCANNED\</strong\>\</div\>

\<div class="metric"\>Total Vulnerabilities:
\<strong\>\$VULNERABILITIES_FOUND\</strong\>\</div\>

\<div class="metric"\>High Severity:
\<strong\>\$HIGH_SEVERITY_VULNS\</strong\>\</div\>

\<div class="metric"\>Medium Severity:
\<strong\>\$MEDIUM_SEVERITY_VULNS\</strong\>\</div\>

\<div class="metric"\>Low Severity:
\<strong\>\$LOW_SEVERITY_VULNS\</strong\>\</div\>

\<div class="metric"\>Scan Duration:
\<strong\>\${SCAN_DURATION}s\</strong\>\</div\>

\</div\>

\<div class="section"\>

\<h2\>Tools Used\</h2\>

\<p\>\$VULNERABILITY_TOOLS\</p\>

\</div\>

\<div class="section"\>

\<h2\>Detailed Results\</h2\>

\<p\>Complete scan results are available in the pipeline
artifacts:\</p\>

\<ul\>

\<li\>Nmap Results: nmap-\*.xml, nmap-\*.txt\</li\>

\<li\>Nuclei Results: nuclei-\*.json\</li\>

\<li\>testssl Results: testssl-\*.json\</li\>

\<li\>Full Log: \$LOG_FILE\</li\>

\</ul\>

\</div\>

\</body\>

\</html\>

EOF

aws s3 cp logs/vulnerability-scan-report.html
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/vulnerability-scan-report.html \|\|
echo "HTML report upload failed"

fi

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Stopping pipeline due to critical vulnerabilities"
\| tee -a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Continuing pipeline despite vulnerabilities" \| tee
-a logs/\$LOG_FILE

exit 0

else

exit 0

fi

artifacts:

files:

\- logs/\*

\- scan-targets-final.txt

name: vulnerability-scan-results

4.3 動的アプリケーション脆弱性スキャン

resource "aws_codebuild_project" "dynamic_app_scan" {

name = "dynamic-application-security-scan"

description = "Dynamic application security testing using OWASP ZAP and
custom tools"

service_role = aws_iam_role.codebuild_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_LARGE"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials_type = "CODEBUILD"

privileged_mode = true

environment_variable {

name = "ZAP_SCAN_TYPE"

value = "FULL"

}

environment_variable {

name = "OWASP_TOP10_FOCUS"

value = "true"

}

environment_variable {

name = "CUSTOM_PAYLOADS_BUCKET"

value = aws_s3_bucket.security_configs.id

}

}

source {

type = "CODEPIPELINE"

buildspec = "buildspecs/dynamic_app_scan.yml"

}

timeout_in_minutes = 120

tags = {

Stage = "DynamicAnalysis"

Tool = "OWASP-ZAP"

Target = "WebApplications"

}

}

4.4 dynamic_app_scan.yml（OWASP ZAP包括的スキャン）

version: 0.2

env:

variables:

LOG_FILE: "dynamic-app-scan-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

ZAP_REPORT_HTML: "zap-report.html"

ZAP_REPORT_JSON: "zap-report.json"

ZAP_REPORT_XML: "zap-report.xml"

SUMMARY_FILE: "app-scan-summary.json"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up OWASP ZAP and application scanning
environment..." \| tee logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# Dockerインストールと起動

yum update -y

yum install -y docker jq bc wget

service docker start

usermod -a -G docker codebuild

\# OWASP ZAP Dockerイメージの取得

docker pull owasp/zap2docker-stable:latest

\# カスタムペイロードとスキャン設定の取得

aws s3 cp s3://\$CUSTOM_PAYLOADS_BUCKET/app-scan-config/scan-config.json
scan-config.json \|\| echo '{"applications": \[\]}' \> scan-config.json

aws s3 cp s3://\$CUSTOM_PAYLOADS_BUCKET/app-scan-config/custom-payloads/
custom-payloads/ --recursive \|\| mkdir -p custom-payloads

\# ZAPのカスタム設定

mkdir -p zap-config

cat \> zap-config/options.prop \<\< 'EOF'

\# ZAP Configuration

scanner.attackOnStart=true

scanner.delayInMs=200

scanner.threadPerHost=10

scanner.hostPerScan=5

spider.maxDuration=30

spider.postForm=true

spider.processForm=true

passivescan.autoTagScanners=true

EOF

pre_build:

commands:

\- echo "\[PRE_BUILD\] Discovering web application targets..." \| tee -a
logs/\$LOG_FILE

\- \|

\# Webアプリケーションターゲットの自動検出

echo '\[\]' \> app-targets.json

\# ALB/ELBのHTTP/HTTPSエンドポイント

aws elbv2 describe-load-balancers --query
'LoadBalancers\[?State.Code==\`active\`\].\[DNSName,Type\]' --output
text \| while read dns_name type; do

\# HTTPSエンドポイントを優先

https_url="https://\$dns_name"

http_url="http://\$dns_name"

\# 接続テスト

if curl -k -s --connect-timeout 10 "\$https_url" \>/dev/null 2\>&1; then

echo "Found HTTPS endpoint: \$https_url" \| tee -a logs/\$LOG_FILE

jq --arg url "\$https_url" '. += \[{"url": \$url, "type":
"load_balancer", "protocol": "https"}\]' app-targets.json \>
app-targets-temp.json

mv app-targets-temp.json app-targets.json

elif curl -s --connect-timeout 10 "\$http_url" \>/dev/null 2\>&1; then

echo "Found HTTP endpoint: \$http_url" \| tee -a logs/\$LOG_FILE

jq --arg url "\$http_url" '. += \[{"url": \$url, "type":
"load_balancer", "protocol": "http"}\]' app-targets.json \>
app-targets-temp.json

mv app-targets-temp.json app-targets.json

fi

done

\# CloudFrontディストリビューション

aws cloudfront list-distributions --query
'DistributionList.Items\[?Status==\`Deployed\`\].DomainName' --output
text \| tr '\t' '\n' \| while read domain; do

if \[ ! -z "\$domain" \]; then

https_url="https://\$domain"

if curl -k -s --connect-timeout 10 "\$https_url" \>/dev/null 2\>&1; then

echo "Found CloudFront endpoint: \$https_url" \| tee -a logs/\$LOG_FILE

jq --arg url "\$https_url" '. += \[{"url": \$url, "type": "cloudfront",
"protocol": "https"}\]' app-targets.json \> app-targets-temp.json

mv app-targets-temp.json app-targets.json

fi

fi

done

\# API Gateway REST APIs

aws apigateway get-rest-apis --query 'items\[\].\[id,name\]' --output
text \| while read api_id name; do

api_url="https://\${api_id}.execute-api.\${AWS_DEFAULT_REGION}.amazonaws.com/prod"

if curl -k -s --connect-timeout 10 "\$api_url" \>/dev/null 2\>&1; then

echo "Found API Gateway endpoint: \$api_url" \| tee -a logs/\$LOG_FILE

jq --arg url "\$api_url

jq --arg url "apiurl"−−argname"api_url" --arg name "
apiu​rl"−−argname"name" '. += \[{"url": \$url, "type": "api_gateway",
"protocol": "https", "name": \$name}\]' app-targets.json \>
app-targets-temp.json mv app-targets-temp.json app-targets.json fi done

\# カスタム設定からの追加ターゲット

if \[ -f scan-config.json \]; then

jq -r '.applications\[\].url' scan-config.json \| while read custom_url;
do

if \[ ! -z "\$custom_url" \]; then

echo "Adding custom target: \$custom_url" \| tee -a logs/\$LOG_FILE

jq --arg url "\$custom_url" '. += \[{"url": \$url, "type": "custom",
"protocol": "https"}\]' app-targets.json \> app-targets-temp.json

mv app-targets-temp.json app-targets.json

fi

done

fi

TARGET_COUNT=\$(jq length app-targets.json)

echo "\[PRE_BUILD\] Found \$TARGET_COUNT web application targets" \| tee
-a logs/\$LOG_FILE

if \[ \$TARGET_COUNT -eq 0 \]; then

echo "\[PRE_BUILD\] No web application targets found for scanning" \|
tee -a logs/\$LOG_FILE

echo '{"message": "No targets found"}' \> logs/\$SUMMARY_FILE

fi

build: commands: - echo "\[BUILD\] Starting dynamic application security
scanning..." \| tee -a logs/\$LOG_FILE - \| SCAN_START_TIME=\$(date +%s)

\# スキャン対象がない場合の処理 TARGET_COUNT=\$(jq length
app-targets.json) if \[ \$TARGET_COUNT -eq 0 \]; then echo "\[BUILD\] No
targets to scan, creating empty results" \| tee -a logs/\$LOG_FILE cat
\> logs/\$SUMMARY_FILE \<\< EOF

\# 結果集計用変数

TOTAL_VULNERABILITIES=0

HIGH_RISK_VULNS=0

MEDIUM_RISK_VULNS=0

LOW_RISK_VULNS=0

INFO_VULNS=0

declare -A OWASP_TOP10_COUNTS

OWASP_TOP10_COUNTS\[A01\]=0 \# Broken Access Control

OWASP_TOP10_COUNTS\[A02\]=0 \# Cryptographic Failures

OWASP_TOP10_COUNTS\[A03\]=0 \# Injection

OWASP_TOP10_COUNTS\[A04\]=0 \# Insecure Design

OWASP_TOP10_COUNTS\[A05\]=0 \# Security Misconfiguration

OWASP_TOP10_COUNTS\[A06\]=0 \# Vulnerable Components

OWASP_TOP10_COUNTS\[A07\]=0 \# Authentication Failures

OWASP_TOP10_COUNTS\[A08\]=0 \# Software Integrity Failures

OWASP_TOP10_COUNTS\[A09\]=0 \# Logging Monitoring Failures

OWASP_TOP10_COUNTS\[A10\]=0 \# Server-Side Request Forgery

\# 各ターゲットに対してZAPスキャン実行

jq -c '.\[\]' app-targets.json \| while read -r target; do

TARGET_URL=\$(echo "\$target" \| jq -r '.url')

TARGET_TYPE=\$(echo "\$target" \| jq -r '.type')

echo "\[BUILD\] Scanning \$TARGET_URL (\$TARGET_TYPE)" \| tee -a
logs/\$LOG_FILE

\# URLから安全なファイル名を生成

TARGET_SAFE_NAME=\$(echo "\$TARGET_URL" \| sed 's\|https\\://\|\|g' \|
sed 's\|\[^a-zA-Z0-9\]\|\_\|g')

\# ZAP基本スキャン（スパイダー + パッシブスキャン + アクティブスキャン）

if \[ "\$ZAP_SCAN_TYPE" = "FULL" \]; then

echo "\[BUILD\] Running full ZAP scan on \$TARGET_URL" \| tee -a
logs/\$LOG_FILE

\# フルスキャン（時間制限付き）

timeout 45m docker run --rm \\

-v \$(pwd)/logs:/zap/wrk/:rw \\

-v \$(pwd)/zap-config:/zap/config/:ro \\

owasp/zap2docker-stable:latest \\

zap-full-scan.py \\

-t "\$TARGET_URL" \\

-g gen.conf \\

-J "zap-\${TARGET_SAFE_NAME}.json" \\

-H "zap-\${TARGET_SAFE_NAME}.html" \\

-X "zap-\${TARGET_SAFE_NAME}.xml" \\

-r "zap-\${TARGET_SAFE_NAME}-report.html" \\

-x "zap-\${TARGET_SAFE_NAME}-xml-report.xml" \\

-m 30 \\

-T 45 \\

-z "-config scanner.attackOnStart=true -config scanner.delayInMs=200" \\

\>\> logs/\$LOG_FILE 2\>&1

ZAP_EXIT_CODE=\$?

echo "\[BUILD\] ZAP full scan completed with exit code \$ZAP_EXIT_CODE
for \$TARGET_URL" \| tee -a logs/\$LOG_FILE

else

echo "\[BUILD\] Running baseline ZAP scan on \$TARGET_URL" \| tee -a
logs/\$LOG_FILE

\# ベースラインスキャン（軽量・高速）

timeout 20m docker run --rm \\

-v \$(pwd)/logs:/zap/wrk/:rw \\

owasp/zap2docker-stable:latest \\

zap-baseline.py \\

-t "\$TARGET_URL" \\

-J "zap-\${TARGET_SAFE_NAME}.json" \\

-H "zap-\${TARGET_SAFE_NAME}.html" \\

-x "zap-\${TARGET_SAFE_NAME}-xml-report.xml" \\

-T 20 \\

\>\> logs/\$LOG_FILE 2\>&1

ZAP_EXIT_CODE=\$?

echo "\[BUILD\] ZAP baseline scan completed with exit code
\$ZAP_EXIT_CODE for \$TARGET_URL" \| tee -a logs/\$LOG_FILE

fi

\# ZAP結果の解析

if \[ -f "logs/zap-\${TARGET_SAFE_NAME}.json" \]; then

echo "\[BUILD\] Analyzing ZAP results for \$TARGET_URL" \| tee -a
logs/\$LOG_FILE

\# 脆弱性カウント

HIGH_COUNT=\$(jq '\[.site\[\].alerts\[\] \| select(.riskdesc \|
startswith("High"))\] \| length' "logs/zap-\${TARGET_SAFE_NAME}.json"
2\>/dev/null \|\| echo "0")

MEDIUM_COUNT=\$(jq '\[.site\[\].alerts\[\] \| select(.riskdesc \|
startswith("Medium"))\] \| length' "logs/zap-\${TARGET_SAFE_NAME}.json"
2\>/dev/null \|\| echo "0")

LOW_COUNT=\$(jq '\[.site\[\].alerts\[\] \| select(.riskdesc \|
startswith("Low"))\] \| length' "logs/zap-\${TARGET_SAFE_NAME}.json"
2\>/dev/null \|\| echo "0")

INFO_COUNT=\$(jq '\[.site\[\].alerts\[\] \| select(.riskdesc \|
startswith("Informational"))\] \| length'
"logs/zap-\${TARGET_SAFE_NAME}.json" 2\>/dev/null \|\| echo "0")

HIGH_RISK_VULNS=\$((HIGH_RISK_VULNS + HIGH_COUNT))

MEDIUM_RISK_VULNS=\$((MEDIUM_RISK_VULNS + MEDIUM_COUNT))

LOW_RISK_VULNS=\$((LOW_RISK_VULNS + LOW_COUNT))

INFO_VULNS=\$((INFO_VULNS + INFO_COUNT))

TOTAL_VULNERABILITIES=\$((TOTAL_VULNERABILITIES + HIGH_COUNT +
MEDIUM_COUNT + LOW_COUNT + INFO_COUNT))

echo "\[BUILD\] \$TARGET_URL vulnerabilities - High:\$HIGH_COUNT
Medium:\$MEDIUM_COUNT Low:\$LOW_COUNT Info:\$INFO_COUNT" \| tee -a
logs/\$LOG_FILE

\# OWASP Top 10 分類

if \[ "\$OWASP_TOP10_FOCUS" = "true" \]; then

\# 各OWASP Top 10カテゴリの検出

jq -r '.site\[\].alerts\[\].name' "logs/zap-\${TARGET_SAFE_NAME}.json"
2\>/dev/null \| while read alert_name; do

case "\$alert_name" in

\*"Access Control"\*\|\*"Authorization"\*\|\*"Privilege"\*)

OWASP_TOP10_COUNTS\[A01\]=\$((\${OWASP_TOP10_COUNTS\[A01\]} + 1))

;;

\*"Crypto"\*\|\*"Encryption"\*\|\*"Hash"\*\|\*"SSL"\*\|\*"TLS"\*)

OWASP_TOP10_COUNTS\[A02\]=\$((\${OWASP_TOP10_COUNTS\[A02\]} + 1))

;;

\*"Injection"\*\|\*"SQL"\*\|\*"XSS"\*\|\*"Script"\*\|\*"Command"\*)

OWASP_TOP10_COUNTS\[A03\]=\$((\${OWASP_TOP10_COUNTS\[A03\]} + 1))

;;

\*"Configuration"\*\|\*"Default"\*\|\*"Misconfiguration"\*)

OWASP_TOP10_COUNTS\[A05\]=\$((\${OWASP_TOP10_COUNTS\[A05\]} + 1))

;;

\*"Component"\*\|\*"Library"\*\|\*"Dependency"\*\|\*"Version"\*)

OWASP_TOP10_COUNTS\[A06\]=\$((\${OWASP_TOP10_COUNTS\[A06\]} + 1))

;;

\*"Authentication"\*\|\*"Session"\*\|\*"Login"\*)

OWASP_TOP10_COUNTS\[A07\]=\$((\${OWASP_TOP10_COUNTS\[A07\]} + 1))

;;

\*"Integrity"\*\|\*"Tamper"\*\|\*"Signature"\*)

OWASP_TOP10_COUNTS\[A08\]=\$((\${OWASP_TOP10_COUNTS\[A08\]} + 1))

;;

\*"Log"\*\|\*"Monitor"\*\|\*"Audit"\*)

OWASP_TOP10_COUNTS\[A09\]=\$((\${OWASP_TOP10_COUNTS\[A09\]} + 1))

;;

\*"SSRF"\*\|\*"Request Forgery"\*)

OWASP_TOP10_COUNTS\[A10\]=\$((\${OWASP_TOP10_COUNTS\[A10\]} + 1))

;;

esac

done

fi

else

echo "\[BUILD\] ZAP results file not found for \$TARGET_URL" \| tee -a
logs/\$LOG_FILE

fi

done

SCAN_END_TIME=\$(date +%s)

SCAN_DURATION=\$((SCAN_END_TIME - SCAN_START_TIME))

echo "\[BUILD\] Application Security Scan Summary:" \| tee -a
logs/\$LOG_FILE

echo " - Total Vulnerabilities: \$TOTAL_VULNERABILITIES" \| tee -a
logs/\$LOG_FILE

echo " - High Risk: \$HIGH_RISK_VULNS" \| tee -a logs/\$LOG_FILE

echo " - Medium Risk: \$MEDIUM_RISK_VULNS" \| tee -a logs/\$LOG_FILE

echo " - Low Risk: \$LOW_RISK_VULNS" \| tee -a logs/\$LOG_FILE

echo " - Informational: \$INFO_VULNS" \| tee -a logs/\$LOG_FILE

echo " - Scan Duration: \${SCAN_DURATION}s" \| tee -a logs/\$LOG_FILE

\# リスクレベル判定

if \[ \$HIGH_RISK_VULNS -gt 5 \]; then

RISK_LEVEL="CRITICAL"

export BUILD_ERROR="CRITICAL_APP_VULNERABILITIES"

export BUILD_EXIT_CODE=1

elif \[ \$HIGH_RISK_VULNS -gt 0 \] \|\| \[ \$MEDIUM_RISK_VULNS -gt 10
\]; then

RISK_LEVEL="HIGH"

export BUILD_ERROR="HIGH_RISK_APP_VULNERABILITIES"

export BUILD_EXIT_CODE=1

elif \[ \$MEDIUM_RISK_VULNS -gt 0 \] \|\| \[ \$LOW_RISK_VULNS -gt 20 \];
then

RISK_LEVEL="MEDIUM"

else

RISK_LEVEL="LOW"

fi

echo " - Risk Level: \$RISK_LEVEL" \| tee -a logs/\$LOG_FILE

\# サマリーJSON作成

cat \> logs/\$SUMMARY_FILE \<\< EOF

{ "pipeline_execution_id": "\$PIPELINE_EXECUTION_ID", "timestamp":
"\$(date -u +%Y-%m-%dT%H:%M:%SZ)", "targets_scanned": \$TARGET_COUNT,
"scan_duration_seconds": \$SCAN_DURATION, "vulnerabilities_found":
\$TOTAL_VULNERABILITIES, "severity_breakdown": { "high":
\$HIGH_RISK_VULNS, "medium": \$MEDIUM_RISK_VULNS, "low":
\$LOW_RISK_VULNS, "informational": \$INFO_VULNS },
"owasp_top10_findings": { "A01_broken_access_control":
\${OWASP_TOP10_COUNTS\[A01\]}, "A02_cryptographic_failures":
\${OWASP_TOP10_COUNTS\[A02\]}, "A03_injection":
\${OWASP_TOP10_COUNTS\[A03\]}, "A04_insecure_design":
\${OWASP_TOP10_COUNTS\[A04\]}, "A05_security_misconfiguration":
\${OWASP_TOP10_COUNTS\[A05\]}, "A06_vulnerable_components":
\${OWASP_TOP10_COUNTS\[A06\]}, "A07_authentication_failures":
\${OWASP_TOP10_COUNTS\[A07\]}, "A08_software_integrity_failures":
\${OWASP_TOP10_COUNTS\[A08\]}, "A09_logging_monitoring_failures":
\${OWASP_TOP10_COUNTS\[A09\]}, "A10_ssrf": \${OWASP_TOP10_COUNTS\[A10\]}
}, "scan_type": "\$ZAP_SCAN_TYPE", "risk_level": "\$RISK_LEVEL" } EOF fi

post_build: commands: - echo "\[POST_BUILD\] Processing application
security scan results..." \| tee -a logs/\$LOG_FILE - \| \#
CloudWatchメトリクス送信 if \[ -f logs/\$SUMMARY_FILE \]; then
TARGETS_SCANNED=(jq′.targetsscanned′logs/(jq '.targets_scanned' logs/
(jq′.targetss​canned′logs/SUMMARY_FILE)
VULNERABILITIES_FOUND=(jq′.vulnerabilitiesfound′logs/(jq
'.vulnerabilities_found' logs/
(jq′.vulnerabilitiesf​ound′logs/SUMMARY_FILE)
HIGH_SEVERITY=(jq′.severitybreakdown.high′logs/(jq
'.severity_breakdown.high' logs/
(jq′.severityb​reakdown.high′logs/SUMMARY_FILE)
MEDIUM_SEVERITY=(jq′.severitybreakdown.medium′logs/(jq
'.severity_breakdown.medium' logs/
(jq′.severityb​reakdown.medium′logs/SUMMARY_FILE)
OWASP_A03_INJECTION=(jq′.owasptop10findings.A03injection′logs/(jq
'.owasp_top10_findings.A03_injection' logs/
(jq′.owaspt​op10f​indings.A03i​njection′logs/SUMMARY_FILE)

aws cloudwatch put-metric-data \\

--namespace "TechNova/ApplicationSecurity" \\

--metric-data \\

MetricName=AppTargetsScanned,Value=\$TARGETS_SCANNED,Unit=Count \\

MetricName=AppVulnerabilitiesFound,Value=\$VULNERABILITIES_FOUND,Unit=Count
\\

MetricName=AppHighSeverityVulns,Value=\$HIGH_SEVERITY,Unit=Count \\

MetricName=AppMediumSeverityVulns,Value=\$MEDIUM_SEVERITY,Unit=Count \\

MetricName=InjectionVulnerabilities,Value=\$OWASP_A03_INJECTION,Unit=Count
\\

--region \$AWS_DEFAULT_REGION \|\| echo "CloudWatch metrics failed"

fi

\# 重要な脆弱性発見時のSNS通知

if \[ "\$RISK_LEVEL" = "CRITICAL" \] \|\| \[ ! -z "\$BUILD_ERROR" \] &&
\[\[ "\$BUILD_ERROR" == \*"CRITICAL"\* \]\]; then

aws sns publish \\

--topic-arn \$CRITICAL_SECURITY_ALERTS_TOPIC \\

--subject "CRITICAL: High-Risk Application Vulnerabilities Detected" \\

--message "Pipeline: \$PIPELINE_EXECUTION_ID\nRisk Level:
\$RISK_LEVEL\nHigh Risk Vulnerabilities: \$HIGH_RISK_VULNS\nMEDIUM Risk
Vulnerabilities: \$MEDIUM_RISK_VULNS\nTotal Vulnerabilities:
\$TOTAL_VULNERABILITIES\nImmediate remediation required." \\

--region \$AWS_DEFAULT_REGION \|\| echo "SNS notification failed"

fi

\# 結果ファイルをS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "Log upload failed"

if \[ -f logs/\$SUMMARY_FILE \]; then

aws s3 cp logs/\$SUMMARY_FILE
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$SUMMARY_FILE \|\| echo "Summary
upload failed"

fi

\# ZAP結果ファイルのアップロード

for zap_file in logs/zap-\*.json logs/zap-\*.html logs/zap-\*.xml; do

if \[ -f "\$zap_file" \]; then

aws s3 cp "\$zap_file" s3://\$S3_LOG_BUCKET/\$S3_PREFIX/ \|\| echo
"\$(basename \$zap_file) upload failed"

fi

done

\# ターゲット情報もアップロード

if \[ -f app-targets.json \]; then

aws s3 cp app-targets.json
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/app-targets.json \|\| echo "App targets
upload failed"

fi

\# 統合レポート生成

if \[ -f logs/\$SUMMARY_FILE \]; then

cat \> logs/application-security-report.html \<\< EOF

\<!DOCTYPE html\> \<html\> \<head\> \<title\>Application Security Scan
Report - Pipeline \$PIPELINE_EXECUTION_ID\</title\> \<style\> body {
font-family: Arial, sans-serif; margin: 40px; } .header { background:
\#f0f0f0; padding: 20px; border-radius: 5px; } .section { margin: 20px
0; padding: 15px; border-left: 4px solid \#007cba; } .critical {
border-left-color: \#d32f2f; background: \#ffebee; } .high {
border-left-color: \#f57c00; background: \#fff3e0; } .medium {
border-left-color: \#fbc02d; background: \#fffde7; } .low {
border-left-color: \#388e3c; background: \#e8f5e8; } .metric { display:
inline-block; margin: 10px; padding: 10px; background: \#f5f5f5;
border-radius: 3px; } .owasp-grid { display: grid;
grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px;
margin: 20px 0; } .owasp-item { padding: 15px; background: \#f9f9f9;
border-radius: 5px; border-left: 4px solid \#007cba; } .owasp-count {
font-size: 24px; font-weight: bold; color: \#d32f2f; } \</style\>
\</head\> \<body\> \<div class="header"\> \<h1\>Application Security
Scan Report\</h1\> \<p\>Pipeline Execution:
\$PIPELINE_EXECUTION_ID\</p\> \<p\>Scan Date: \$(date)\</p\> \<p\>Risk
Level: \<strong\>\$RISK_LEVEL\</strong\>\</p\> \<p\>Tool: OWASP ZAP
(\$ZAP_SCAN_TYPE scan)\</p\> \</div\>

\<div class="section \${RISK_LEVEL,,}"\>

\<h2\>Scan Summary\</h2\>

\<div class="metric"\>Targets Scanned:
\<strong\>\$TARGETS_SCANNED\</strong\>\</div\>

\<div class="metric"\>Total Vulnerabilities:
\<strong\>\$TOTAL_VULNERABILITIES\</strong\>\</div\>

\<div class="metric"\>High Risk:
\<strong\>\$HIGH_RISK_VULNS\</strong\>\</div\>

\<div class="metric"\>Medium Risk:
\<strong\>\$MEDIUM_RISK_VULNS\</strong\>\</div\>

\<div class="metric"\>Low Risk:
\<strong\>\$LOW_RISK_VULNS\</strong\>\</div\>

\<div class="metric"\>Informational:
\<strong\>\$INFO_VULNS\</strong\>\</div\>

\<div class="metric"\>Scan Duration:
\<strong\>\${SCAN_DURATION}s\</strong\>\</div\>

\</div\>

\<div class="section"\>

\<h2\>OWASP Top 10 Findings\</h2\>

\<div class="owasp-grid"\>

\<div class="owasp-item"\>

\<div class="owasp-count"\>\${OWASP_TOP10_COUNTS\[A01\]}\</div\>

\<div\>A01: Broken Access Control\</div\>

\</div\>

\<div class="owasp-item"\>

\<div class="owasp-count"\>\${OWASP_TOP10_COUNTS\[A02\]}\</div\>

\<div\>A02: Cryptographic Failures\</div\>

\</div\>

\<div class="owasp-item"\>

\<div class="owasp-count"\>\${OWASP_TOP10_COUNTS\[A03\]}\</div\>

\<div\>A03: Injection\</div\>

\</div\>

\<div class="owasp-item"\>

\<div class="owasp-count"\>\${OWASP_TOP10_COUNTS\[A05\]}\</div\>

\<div\>A05: Security Misconfiguration\</div\>

\</div\>

\<div class="owasp-item"\>

\<div class="owasp-count"\>\${OWASP_TOP10_COUNTS\[A06\]}\</div\>

\<div\>A06: Vulnerable Components\</div\>

\</div\>

\<div class="owasp-item"\>

\<div class="owasp-count"\>\${OWASP_TOP10_COUNTS\[A07\]}\</div\>

\<div\>A07: Authentication Failures\</div\>

\</div\>

\</div\>

\</div\>

\<div class="section"\>

\<h2\>Detailed Reports\</h2\>

\<p\>Complete ZAP scan results are available in the pipeline
artifacts:\</p\>

\<ul\>

\<li\>JSON Reports: zap-\*.json\</li\>

\<li\>HTML Reports: zap-\*.html\</li\>

\<li\>XML Reports: zap-\*.xml\</li\>

\<li\>Full Log: \$LOG_FILE\</li\>

\</ul\>

\</div\>

\</body\> \</html\> EOF

aws s3 cp logs/application-security-report.html
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/application-security-report.html \|\|
echo "HTML report upload failed"

fi

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Stopping pipeline due to critical application
vulnerabilities" \| tee -a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Continuing pipeline despite application
vulnerabilities" \| tee -a logs/\$LOG_FILE

exit 0

else

exit 0

fi

artifacts: files: - logs/\* - app-targets.json name:
application-security-scan-results

\### 4.5 ペネトレーションテスト

\`\`\`hcl

resource "aws_codebuild_project" "penetration_test" {

name = "automated-penetration-testing"

description = "Automated penetration testing using multiple frameworks"

service_role = aws_iam_role.codebuild_role.arn

artifacts {

type = "CODEPIPELINE"

}

environment {

compute_type = "BUILD_GENERAL1_LARGE"

image = "aws/codebuild/amazonlinux2-x86_64-standard:latest"

type = "LINUX_CONTAINER"

image_pull_credentials_type = "CODEBUILD"

privileged_mode = true

environment_variable {

name = "PENTEST_SCOPE"

value = "EXTERNAL_ONLY"

}

environment_variable {

name = "PENTEST_INTENSITY"

value = "MODERATE"

}

environment_variable {

name = "MAX_PENTEST_DURATION"

value = "2700" \# 45分

}

}

source {

type = "CODEPIPELINE"

buildspec = "buildspecs/penetration_test.yml"

}

timeout_in_minutes = 60

tags = {

Stage = "DynamicAnalysis"

Tool = "PenetrationTest"

Scope = "Automated"

}

}

**4.6 penetration_test.yml（自動ペネトレーションテスト）**

yaml

version: 0.2

env:

variables:

LOG_FILE: "penetration-test-\$PIPELINE_EXECUTION_ID.log"

S3_PREFIX: "logs/\$PIPELINE_EXECUTION_ID"

PENTEST_RESULTS: "pentest-results.json"

METASPLOIT_RESULTS: "metasploit-results.json"

SUMMARY_FILE: "pentest-summary.json"

phases:

install:

commands:

\- echo "\[INSTALL\] Setting up penetration testing environment..." \|
tee logs/\$LOG_FILE

\- mkdir -p logs

\- \|

\# システム更新とツールインストール

yum update -y

yum install -y docker jq bc git python3-pip

*\# Docker起動*

service docker start

usermod -a -G docker codebuild

*\# Kali Linux Dockerイメージの取得（ペネトレーションテスト用）*

docker pull kalilinux/kali-rolling:latest

*\# セキュリティテスト用Pythonライブラリ*

pip3 install requests beautifulsoup4 paramiko

*\# カスタムペネトレーションテスト設定*

aws s3 cp s3://\$SECURITY_CONFIGS_BUCKET/pentest-config/ pentest-config/
--recursive \|\| mkdir -p pentest-config

*\# 許可されたターゲット設定の確認*

cat \> pentest-config/allowed-targets.json \<\< 'EOF'

{

"allowed_networks": \[

"10.0.0.0/8",

"172.16.0.0/12",

"192.168.0.0/16"

\],

"allowed_domains": \[

"\*.technova-staging.com",

"\*.technova-dev.com"

\],

"prohibited_targets": \[

"production",

"prod",

"live"

\]

}

EOF

pre_build:

commands:

\- echo "\[PRE_BUILD\] Preparing penetration test targets and scope..."
\| tee -a logs/\$LOG_FILE

\- \|

\# ペネトレーションテスト用の安全なターゲット検出

echo '\[\]' \> pentest-targets.json

*\# ステージング環境のリソースのみを対象とする*

if \[ "\$TARGET_ENVIRONMENT" = "staging" \] \|\| \[
"\$TARGET_ENVIRONMENT" = "development" \]; then

echo "\[PRE_BUILD\] Scanning for \$TARGET_ENVIRONMENT environment
targets" \| tee -a logs/\$LOG_FILE

*\# ステージング環境のALB/ELB*

aws elbv2 describe-load-balancers \\

--query
'LoadBalancers\[?State.Code==\`active\`\].\[DNSName,LoadBalancerName\]'
\\

--output text \| while read dns_name lb_name; do

*\# ステージング環境の識別*

if echo "\$lb_name" \| grep -E "(staging\|dev\|test)" \>/dev/null 2\>&1;
then

echo "Found staging load balancer: \$dns_name (\$lb_name)" \| tee -a
logs/\$LOG_FILE

jq --arg url "\$dns_name" --arg name "\$lb_name" '. += \[{"target":
\$url, "type": "load_balancer", "name": \$name, "environment":
"staging"}\]' pentest-targets.json \> pentest-targets-temp.json

mv pentest-targets-temp.json pentest-targets.json

fi

done

*\# ステージング環境のEC2インスタンス（タグベース）*

aws ec2 describe-instances \\

--filters "Name=instance-state-name,Values=running"
"Name=tag:Environment,Values=\$TARGET_ENVIRONMENT" \\

--query
'Reservations\[\].Instances\[?PublicIpAddress\].\[PublicIpAddress,Tags\[?Key==\`Name\`\].Value\|\[0\],InstanceId\]'
\\

--output text \| while read ip name instance_id; do

if \[ ! -z "\$ip" \] && \[ "\$ip" != "None" \]; then

echo "Found staging EC2 instance: \$ip (\$name)" \| tee -a
logs/\$LOG_FILE

jq --arg ip "\$ip" --arg name "\$name" --arg id "\$instance_id" '. +=
\[{"target": \$ip, "type": "ec2_instance", "name": \$name,
"instance_id": \$id, "environment": "staging"}\]' pentest-targets.json

再試行

C

続ける

編集

jq --arg ip "ip"−−argname"ip" --arg name " ip"−−argname"name" --arg id
"\$instance_id" '. += \[{"target": \$ip, "type": "ec2_instance", "name":
\$name, "instance_id": \$id, "environment": "staging"}\]'
pentest-targets.json \> pentest-targets-temp.json mv
pentest-targets-temp.json pentest-targets.json fi done

else

echo "\[PRE_BUILD\] Production environment detected - penetration
testing disabled for safety" \| tee -a logs/\$LOG_FILE

echo '{"error": "Production environment penetration testing
prohibited"}' \> logs/\$SUMMARY_FILE

fi

\# ターゲット数と安全性確認

TARGET_COUNT=\$(jq length pentest-targets.json)

echo "\[PRE_BUILD\] Found \$TARGET_COUNT penetration test targets" \|
tee -a logs/\$LOG_FILE

\# 安全性チェック

if \[ \$TARGET_COUNT -eq 0 \]; then

echo "\[PRE_BUILD\] No safe targets identified for penetration testing"
\| tee -a logs/\$LOG_FILE

else

echo "\[PRE_BUILD\] Verifying target safety..." \| tee -a
logs/\$LOG_FILE

\# 本番環境の誤検出防止

jq -c '.\[\]' pentest-targets.json \| while read -r target; do

TARGET_NAME=\$(echo "\$target" \| jq -r '.name // .target')

if echo "\$TARGET_NAME" \| grep -E "(prod\|production\|live)"
\>/dev/null 2\>&1; then

echo "\[ERROR\] Production target detected in penetration test scope:
\$TARGET_NAME" \| tee -a logs/\$LOG_FILE

export PENTEST_SAFETY_ERROR="PRODUCTION_TARGET_DETECTED"

fi

done

fi

build: commands: - echo "\[BUILD\] Starting automated penetration
testing..." \| tee -a logs/\$LOG_FILE - \| PENTEST_START_TIME=\$(date
+%s) MAX_DURATION=\${MAX_PENTEST_DURATION:-2700}

\# 安全性エラーがある場合は中止

if \[ ! -z "\$PENTEST_SAFETY_ERROR" \]; then

echo "\[BUILD\] Penetration testing aborted due to safety concerns" \|
tee -a logs/\$LOG_FILE

cat \> logs/\$SUMMARY_FILE \<\< EOF

{ "pipeline_execution_id": "\$PIPELINE_EXECUTION_ID", "timestamp":
"\$(date -u +%Y-%m-%dT%H:%M:%SZ)", "status": "ABORTED", "reason":
"\$PENTEST_SAFETY_ERROR", "targets_tested": 0,
"vulnerabilities_exploited": 0, "risk_level": "NONE" } EOF exit 0 fi

TARGET_COUNT=\$(jq length pentest-targets.json)

if \[ \$TARGET_COUNT -eq 0 \]; then

echo "\[BUILD\] No targets available for penetration testing" \| tee -a
logs/\$LOG_FILE

cat \> logs/\$SUMMARY_FILE \<\< EOF

{ "pipeline_execution_id": "\$PIPELINE_EXECUTION_ID", "timestamp":
"\$(date -u +%Y-%m-%dT%H:%M:%SZ)", "status": "COMPLETED",
"targets_tested": 0, "vulnerabilities_exploited": 0,
"attack_vectors_tested": 0, "pentest_duration_seconds": 0, "risk_level":
"NONE" } EOF else echo "\[BUILD\] Starting penetration tests on
TARGETCOUNTtargets"∣tee−alogs/TARGET_COUNT targets" \| tee -a logs/
TARGETC​OUNTtargets"∣tee−alogs/LOG_FILE

\# 結果集計用変数

TOTAL_EXPLOITS_ATTEMPTED=0

SUCCESSFUL_EXPLOITS=0

FAILED_EXPLOITS=0

CRITICAL_FINDINGS=0

HIGH_FINDINGS=0

MEDIUM_FINDINGS=0

\# ペネトレーションテスト結果配列

echo '{"pentest_results": \[\], "summary": {}}' \>
logs/\$PENTEST_RESULTS

\# 各ターゲットに対するペネトレーションテスト

jq -c '.\[\]' pentest-targets.json \| while read -r target; do

CURRENT_TIME=\$(date +%s)

ELAPSED_TIME=\$((CURRENT_TIME - PENTEST_START_TIME))

if \[ \$ELAPSED_TIME -gt \$MAX_DURATION \]; then

echo "\[BUILD\] Maximum penetration test duration reached" \| tee -a
logs/\$LOG_FILE

break

fi

TARGET_IP=\$(echo "\$target" \| jq -r '.target')

TARGET_TYPE=\$(echo "\$target" \| jq -r '.type')

TARGET_NAME=\$(echo "\$target" \| jq -r '.name // .target')

echo "\[BUILD\] Penetration testing \$TARGET_IP (\$TARGET_NAME)" \| tee
-a logs/\$LOG_FILE

\# 基本的なポートスキャンと脆弱性検証

echo "\[BUILD\] Phase 1: Reconnaissance and enumeration" \| tee -a
logs/\$LOG_FILE

\# Nmapによる詳細ポートスキャン

nmap -sS -sV -A --script=vuln,exploit,auth \\

-oX logs/pentest-nmap-\${TARGET_IP//\[^0-9\]/\_}.xml \\

--max-retries 1 --host-timeout 5m \\

\$TARGET_IP \>\> logs/\$LOG_FILE 2\>&1 &

NMAP_PID=\$!

\# 基本的なHTTP/HTTPS脆弱性テスト

echo "\[BUILD\] Phase 2: Web application testing" \| tee -a
logs/\$LOG_FILE

\# HTTPサービス検出とテスト

for port in 80 443 8080 8443; do

if nc -z -w3 \$TARGET_IP \$port 2\>/dev/null; then

protocol="http"

\[ \$port -eq 443 \] \|\| \[ \$port -eq 8443 \] && protocol="https"

target_url="\${protocol}://\${TARGET_IP}:\${port}"

echo "\[BUILD\] Testing web service: \$target_url" \| tee -a
logs/\$LOG_FILE

\# 基本的なWebアプリケーション脆弱性テスト

timeout 300 docker run --rm \\

-v \$(pwd)/logs:/tmp/results \\

kalilinux/kali-rolling:latest \\

bash -c "

apt-get update -qq && apt-get install -y -qq curl nikto dirb
2\>/dev/null

echo 'Testing \$target_url' \>
/tmp/results/pentest-web-\${TARGET_IP//\[^0-9\]/\_}-\${port}.log

\# Basic HTTP enumeration

curl -k -s -I '\$target_url' \>\>
/tmp/results/pentest-web-\${TARGET_IP//\[^0-9\]/\_}-\${port}.log 2\>&1

\# Directory enumeration (limited)

timeout 180 dirb '\$target_url' -w -S -r \>\>
/tmp/results/pentest-web-\${TARGET_IP//\[^0-9\]/\_}-\${port}.log 2\>&1

\# Nikto scan (basic)

timeout 180 nikto -h '\$target_url' -Format txt \>\>
/tmp/results/pentest-web-\${TARGET_IP//\[^0-9\]/\_}-\${port}.log 2\>&1

" \>\> logs/\$LOG_FILE 2\>&1 &

WEB_TEST_PID=\$!

echo "Web test PID: \$WEB_TEST_PID for \$target_url" \| tee -a
logs/\$LOG_FILE

fi

done

\# SSH接続テスト（認証脆弱性）

echo "\[BUILD\] Phase 3: SSH security testing" \| tee -a logs/\$LOG_FILE

if nc -z -w3 \$TARGET_IP 22 2\>/dev/null; then

echo "\[BUILD\] SSH service detected on \$TARGET_IP" \| tee -a
logs/\$LOG_FILE

\# SSH設定の脆弱性チェック

timeout 60 docker run --rm \\

kalilinux/kali-rolling:latest \\

bash -c "

apt-get update -qq && apt-get install -y -qq ssh-audit 2\>/dev/null

ssh-audit \$TARGET_IP 2\>&1

" \> logs/pentest-ssh-\${TARGET_IP//\[^0-9\]/\_}.log 2\>&1 &

SSH_TEST_PID=\$!

fi

wait \$NMAP_PID 2\>/dev/null

wait \$WEB_TEST_PID 2\>/dev/null

wait \$SSH_TEST_PID 2\>/dev/null

\# 結果の分析

echo "\[BUILD\] Analyzing penetration test results for \$TARGET_IP" \|
tee -a logs/\$LOG_FILE

\# Nmapの脆弱性結果解析

if \[ -f "logs/pentest-nmap-\${TARGET_IP//\[^0-9\]/\_}.xml" \]; then

NMAP_VULNS=\$(grep -c "VULNERABLE\\EXPLOIT"
"logs/pentest-nmap-\${TARGET_IP//\[^0-9\]/\_}.xml" 2\>/dev/null \|\|
echo "0")

TOTAL_EXPLOITS_ATTEMPTED=\$((TOTAL_EXPLOITS_ATTEMPTED + NMAP_VULNS))

if \[ \$NMAP_VULNS -gt 0 \]; then

SUCCESSFUL_EXPLOITS=\$((SUCCESSFUL_EXPLOITS + NMAP_VULNS))

HIGH_FINDINGS=\$((HIGH_FINDINGS + NMAP_VULNS))

echo "\[BUILD\] Found \$NMAP_VULNS exploitable vulnerabilities via Nmap"
\| tee -a logs/\$LOG_FILE

fi

fi

\# Webアプリケーション脆弱性結果解析

for web_log in logs/pentest-web-\${TARGET_IP//\[^0-9\]/\_}-\*.log; do

if \[ -f "\$web_log" \]; then

WEB_VULNS=\$(grep -c -E "(OSVDB\|CVE-\|High\|Critical)" "\$web_log"
2\>/dev/null \|\| echo "0")

if \[ \$WEB_VULNS -gt 0 \]; then

MEDIUM_FINDINGS=\$((MEDIUM_FINDINGS + WEB_VULNS))

echo "\[BUILD\] Found \$WEB_VULNS web vulnerabilities on \$TARGET_IP" \|
tee -a logs/\$LOG_FILE

fi

fi

done

\# SSH脆弱性結果解析

if \[ -f "logs/pentest-ssh-\${TARGET_IP//\[^0-9\]/\_}.log" \]; then

SSH_ISSUES=\$(grep -c -E "(FAIL\|WARN)"
"logs/pentest-ssh-\${TARGET_IP//\[^0-9\]/\_}.log" 2\>/dev/null \|\| echo
"0")

if \[ \$SSH_ISSUES -gt 0 \]; then

MEDIUM_FINDINGS=\$((MEDIUM_FINDINGS + SSH_ISSUES))

echo "\[BUILD\] Found \$SSH_ISSUES SSH security issues on \$TARGET_IP"
\| tee -a logs/\$LOG_FILE

fi

fi

done

PENTEST_END_TIME=\$(date +%s)

PENTEST_DURATION=\$((PENTEST_END_TIME - PENTEST_START_TIME))

echo "\[BUILD\] Penetration Test Summary:" \| tee -a logs/\$LOG_FILE

echo " - Targets Tested: \$TARGET_COUNT" \| tee -a logs/\$LOG_FILE

echo " - Exploits Attempted: \$TOTAL_EXPLOITS_ATTEMPTED" \| tee -a
logs/\$LOG_FILE

echo " - Successful Exploits: \$SUCCESSFUL_EXPLOITS" \| tee -a
logs/\$LOG_FILE

echo " - Critical Findings: \$CRITICAL_FINDINGS" \| tee -a
logs/\$LOG_FILE

echo " - High Findings: \$HIGH_FINDINGS" \| tee -a logs/\$LOG_FILE

echo " - Medium Findings: \$MEDIUM_FINDINGS" \| tee -a logs/\$LOG_FILE

echo " - Test Duration: \${PENTEST_DURATION}s" \| tee -a logs/\$LOG_FILE

\# リスク判定

TOTAL_FINDINGS=\$((CRITICAL_FINDINGS + HIGH_FINDINGS + MEDIUM_FINDINGS))

if \[ \$CRITICAL_FINDINGS -gt 0 \] \|\| \[ \$SUCCESSFUL_EXPLOITS -gt 3
\]; then

RISK_LEVEL="CRITICAL"

export BUILD_ERROR="CRITICAL_PENTEST_FINDINGS"

export BUILD_EXIT_CODE=1

elif \[ \$HIGH_FINDINGS -gt 0 \] \|\| \[ \$SUCCESSFUL_EXPLOITS -gt 0 \];
then

RISK_LEVEL="HIGH"

export BUILD_ERROR="HIGH_RISK_PENTEST_FINDINGS"

export BUILD_EXIT_CODE=1

elif \[ \$MEDIUM_FINDINGS -gt 5 \]; then

RISK_LEVEL="MEDIUM"

else

RISK_LEVEL="LOW"

fi

echo " - Risk Level: \$RISK_LEVEL" \| tee -a logs/\$LOG_FILE

\# サマリーJSON作成

cat \> logs/\$SUMMARY_FILE \<\< EOF

{ "pipeline_execution_id": "\$PIPELINE_EXECUTION_ID", "timestamp":
"\$(date -u +%Y-%m-%dT%H:%M:%SZ)", "status": "COMPLETED",
"targets_tested": \$TARGET_COUNT, "pentest_duration_seconds":
\$PENTEST_DURATION, "attack_vectors_tested": \$TOTAL_EXPLOITS_ATTEMPTED,
"vulnerabilities_exploited": \$SUCCESSFUL_EXPLOITS,
"findings_breakdown": { "critical": \$CRITICAL_FINDINGS, "high":
\$HIGH_FINDINGS, "medium": \$MEDIUM_FINDINGS, "total": \$TOTAL_FINDINGS
}, "pentest_scope": "\$PENTEST_SCOPE", "pentest_intensity":
"\$PENTEST_INTENSITY", "risk_level": "\$RISK_LEVEL" } EOF fi

post_build: commands: - echo "\[POST_BUILD\] Processing penetration test
results..." \| tee -a logs/\$LOG_FILE - \| \# CloudWatchメトリクス送信
if \[ -f logs/SUMMARY_FILE \] && \[ " (jq -r '.status'
logs/\$SUMMARY_FILE)" = "COMPLETED" \]; then
TARGETS_TESTED=(jq′.targetstested′logs/(jq '.targets_tested' logs/
(jq′.targetst​ested′logs/SUMMARY_FILE)
VULNS_EXPLOITED=(jq′.vulnerabilitiesexploited′logs/(jq
'.vulnerabilities_exploited' logs/
(jq′.vulnerabilitiese​xploited′logs/SUMMARY_FILE)
CRITICAL_FINDINGS=(jq′.findingsbreakdown.critical′logs/(jq
'.findings_breakdown.critical' logs/
(jq′.findingsb​reakdown.critical′logs/SUMMARY_FILE)
HIGH_FINDINGS=(jq′.findingsbreakdown.high′logs/(jq
'.findings_breakdown.high' logs/
(jq′.findingsb​reakdown.high′logs/SUMMARY_FILE)

aws cloudwatch put-metric-data \\

--namespace "TechNova/PenetrationTesting" \\

--metric-data \\

MetricName=PentestTargetsTested,Value=\$TARGETS_TESTED,Unit=Count \\

MetricName=VulnerabilitiesExploited,Value=\$VULNS_EXPLOITED,Unit=Count
\\

MetricName=CriticalPentestFindings,Value=\$CRITICAL_FINDINGS,Unit=Count
\\

MetricName=HighPentestFindings,Value=\$HIGH_FINDINGS,Unit=Count \\

--region \$AWS_DEFAULT_REGION \|\| echo "CloudWatch metrics failed"

fi

\# 重要な脆弱性発見時のSNS通知

if \[ "\$RISK_LEVEL" = "CRITICAL" \] \|\| \[ ! -z "\$BUILD_ERROR" \] &&
\[\[ "\$BUILD_ERROR" == \*"CRITICAL"\* \]\]; then

aws sns publish \\

--topic-arn \$CRITICAL_SECURITY_ALERTS_TOPIC \\

--subject "CRITICAL: Exploitable Vulnerabilities Found in Penetration
Test" \\

--message "Pipeline: \$PIPELINE_EXECUTION_ID\nRisk Level:
\$RISK_LEVEL\nTargets Tested: \$TARGET_COUNT\nSuccessful Exploits:
\$SUCCESSFUL_EXPLOITS\nCritical Findings: \$CRITICAL_FINDINGS\nHigh
Findings: \$HIGH_FINDINGS\nImmediate security remediation required." \\

--region \$AWS_DEFAULT_REGION \|\| echo "SNS notification failed"

fi

\# 結果ファイルをS3にアップロード

aws s3 cp logs/\$LOG_FILE s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$LOG_FILE
\|\| echo "Log upload failed"

if \[ -f logs/\$SUMMARY_FILE \]; then

aws s3 cp logs/\$SUMMARY_FILE
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/\$SUMMARY_FILE \|\| echo "Summary
upload failed"

fi

\# ペネトレーションテスト結果ファイルのアップロード

for pentest_file in logs/pentest-\*.xml logs/pentest-\*.log; do

if \[ -f "\$pentest_file" \]; then

aws s3 cp "\$pentest_file" s3://\$S3_LOG_BUCKET/\$S3_PREFIX/ \|\| echo
"\$(basename \$pentest_file) upload failed"

fi

done

\# ターゲット情報もアップロード

if \[ -f pentest-targets.json \]; then

aws s3 cp pentest-targets.json
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/pentest-targets.json \|\| echo "Pentest
targets upload failed"

fi

\# 統合レポート生成

if \[ -f logs/\$SUMMARY_FILE \]; then

cat \> logs/penetration-test-report.html \<\< EOF

\<!DOCTYPE html\> \<html\> \<head\> \<title\>Penetration Test Report -
Pipeline \$PIPELINE_EXECUTION_ID\</title\> \<style\> body { font-family:
Arial, sans-serif; margin: 40px; } .header { background: \#f0f0f0;
padding: 20px; border-radius: 5px; } .section { margin: 20px 0; padding:
15px; border-left: 4px solid \#007cba; } .critical { border-left-color:
\#d32f2f; background: \#ffebee; } .high { border-left-color: \#f57c00;
background: \#fff3e0; } .medium { border-left-color: \#fbc02d;
background: \#fffde7; } .low { border-left-color: \#388e3c; background:
\#e8f5e8; } .metric { display: inline-block; margin: 10px; padding:
10px; background: \#f5f5f5; border-radius: 3px; } .finding { margin:
10px 0; padding: 10px; background: \#fafafa; border-radius: 3px;
border-left: 3px solid \#ff9800; } .exploit-success { color: \#d32f2f;
font-weight: bold; } .warning { background: \#fff3e0; padding: 15px;
border-radius: 5px; margin: 20px 0; } \</style\> \</head\> \<body\>
\<div class="header"\> \<h1\>Penetration Test Report\</h1\>
\<p\>Pipeline Execution: \$PIPELINE_EXECUTION_ID\</p\> \<p\>Test Date:
\$(date)\</p\> \<p\>Risk Level: \<strong\>\$RISK_LEVEL\</strong\>\</p\>
\<p\>Scope: \$PENTEST_SCOPE \| Intensity: \$PENTEST_INTENSITY\</p\>
\</div\>

\<div class="warning"\>

\<strong\>Important:\</strong\> This penetration test was conducted in a
controlled environment on staging/development systems only.

Production systems are excluded from automated penetration testing for
security reasons.

\</div\>

\<div class="section \${RISK_LEVEL,,}"\>

\<h2\>Test Summary\</h2\>

\<div class="metric"\>Targets Tested:
\<strong\>\$TARGET_COUNT\</strong\>\</div\>

\<div class="metric"\>Attack Vectors:
\<strong\>\$TOTAL_EXPLOITS_ATTEMPTED\</strong\>\</div\>

\<div class="metric"\>Successful Exploits: \<strong
class="exploit-success"\>\$SUCCESSFUL_EXPLOITS\</strong\>\</div\>

\<div class="metric"\>Critical Findings:
\<strong\>\$CRITICAL_FINDINGS\</strong\>\</div\>

\<div class="metric"\>High Findings:
\<strong\>\$HIGH_FINDINGS\</strong\>\</div\>

\<div class="metric"\>Medium Findings:
\<strong\>\$MEDIUM_FINDINGS\</strong\>\</div\>

\<div class="metric"\>Test Duration:
\<strong\>\${PENTEST_DURATION}s\</strong\>\</div\>

\</div\>

\<div class="section"\>

\<h2\>Testing Methodology\</h2\>

\<ul\>

\<li\>Automated reconnaissance and enumeration\</li\>

\<li\>Port scanning and service detection\</li\>

\<li\>Web application vulnerability testing\</li\>

\<li\>SSH security configuration assessment\</li\>

\<li\>Network service exploitation attempts\</li\>

\</ul\>

\</div\>

\<div class="section"\>

\<h2\>Key Findings\</h2\>

\$(if \[ \$SUCCESSFUL_EXPLOITS -gt 0 \]; then echo "\<div class='finding
exploit-success'\>⚠️ \$SUCCESSFUL_EXPLOITS exploitable vulnerabilities
were successfully validated\</div\>"; fi)

\$(if \[ \$CRITICAL_FINDINGS -gt 0 \]; then echo "\<div
class='finding'\>🔴 \$CRITICAL_FINDINGS critical security issues require
immediate attention\</div\>"; fi)

\$(if \[ \$HIGH_FINDINGS -gt 0 \]; then echo "\<div class='finding'\>🟠
\$HIGH_FINDINGS high-severity findings need prompt remediation\</div\>";
fi)

\$(if \[ \$MEDIUM_FINDINGS -gt 0 \]; then echo "\<div
class='finding'\>🟡 \$MEDIUM_FINDINGS medium-severity issues should be
addressed\</div\>"; fi)

\</div\>

\<div class="section"\>

\<h2\>Detailed Results\</h2\>

\<p\>Complete penetration test results are available in the pipeline
artifacts:\</p\>

\<ul\>

\<li\>Nmap Vulnerability Scans: pentest-nmap-\*.xml\</li\>

\<li\>Web Application Tests: pentest-web-\*.log\</li\>

\<li\>SSH Security Audits: pentest-ssh-\*.log\</li\>

\<li\>Full Test Log: \$LOG_FILE\</li\>

\<li\>Target List: pentest-targets.json\</li\>

\</ul\>

\</div\>

\<div class="section"\>

\<h2\>Recommendations\</h2\>

\$(if \[ \$SUCCESSFUL_EXPLOITS -gt 0 \]; then echo
"\<p\>\<strong\>Priority 1:\</strong\> Address all exploitable
vulnerabilities immediately before production deployment.\</p\>"; fi)

\$(if \[ \$CRITICAL_FINDINGS -gt 0 \]; then echo
"\<p\>\<strong\>Priority 2:\</strong\> Remediate critical security
misconfigurations.\</p\>"; fi)

\$(if \[ \$HIGH_FINDINGS -gt 0 \]; then echo "\<p\>\<strong\>Priority
3:\</strong\> Fix high-severity vulnerabilities to reduce attack
surface.\</p\>"; fi)

\<p\>\<strong\>Ongoing:\</strong\> Implement regular penetration testing
in the CI/CD pipeline.\</p\>

\<p\>\<strong\>Security:\</strong\> Consider professional penetration
testing for production environments.\</p\>

\</div\>

\</body\> \</html\> EOF

aws s3 cp logs/penetration-test-report.html
s3://\$S3_LOG_BUCKET/\$S3_PREFIX/penetration-test-report.html \|\| echo
"HTML report upload failed"

fi

\# エラーハンドリング

if \[ "\$ERROR_HANDLING_MODE" = "STOP" \] && \[ ! -z "\$BUILD_ERROR" \];
then

echo "\[POST_BUILD\] Stopping pipeline due to critical penetration test
findings" \| tee -a logs/\$LOG_FILE

exit \$BUILD_EXIT_CODE

elif \[ "\$ERROR_HANDLING_MODE" = "CONTINUE" \] && \[ ! -z
"\$BUILD_ERROR" \]; then

echo "\[POST_BUILD\] Continuing pipeline despite penetration test
findings" \| tee -a logs/\$LOG_FILE

exit 0

else

exit 0

fi

artifacts: files: - logs/\* - pentest-targets.json name:
penetration-test-results

\## 5. エラーハンドリング Lambda 関数詳細実装

\### 5.1 Lambda関数のコア実装

\`\`\`hcl

resource "aws_lambda_function" "pipeline_error_handler" {

filename = "lambda/pipeline_error_handler.zip"

function_name = "pipeline-error-handler"

role = aws_iam_role.lambda_error_handler_role.arn

handler = "index.handler"

source_code_hash =
data.archive_file.pipeline_error_handler_zip.output_base64sha256

runtime = "python3.9"

timeout = 300

environment {

variables = {

SNS_TOPIC_ARN = aws_sns_topic.pipeline_errors.arn

DYNAMODB_TABLE = aws_dynamodb_table.pipeline_errors.name

S3_BUCKET = aws_s3_bucket.build_logs.id

SLACK_WEBHOOK_SECRET = aws_ssm_parameter.slack_webhook.name

}

}

tags = {

Purpose = "Pipeline Error Handling"

Component = "ErrorHandling"

}

}

data "archive_file" "pipeline_error_handler_zip" {

type = "zip"

output_path = "lambda/pipeline_error_handler.zip"

source {

content = file("\${path.module}/lambda/pipeline_error_handler.py")

filename = "index.py"

}

}

**5.2 pipeline_error_handler.py（包括的エラーハンドリング）**

python

import json

import boto3

import os

import logging

from datetime import datetime, timezone

from typing import Dict, List, Any

import traceback

*\# ログ設定*

logger = logging.getLogger()

logger.setLevel(logging.INFO)

*\# AWSクライアント初期化*

sns = boto3.client('sns')

dynamodb = boto3.resource('dynamodb')

s3 = boto3.client('s3')

ssm = boto3.client('ssm')

codepipeline = boto3.client('codepipeline')

*\# 環境変数*

SNS_TOPIC_ARN = os.environ\['SNS_TOPIC_ARN'\]

DYNAMODB_TABLE = os.environ\['DYNAMODB_TABLE'\]

S3_BUCKET = os.environ\['S3_BUCKET'\]

SLACK_WEBHOOK_SECRET = os.environ\['SLACK_WEBHOOK_SECRET'\]

class PipelineErrorHandler:

def \_\_init\_\_(self):

self.table = dynamodb.Table(DYNAMODB_TABLE)

def handler(self, event, context):

"""

パイプラインエラーハンドリングのメインハンドラー

"""

try:

logger.info(f"Error handler invoked with event: {json.dumps(event)}")

*\# イベント形式の判定（CodePipeline vs Lambda直接呼び出し）*

if 'CodePipeline.job' in event:

return self.\_handle_codepipeline_event(event, context)

else:

return self.\_handle_direct_invocation(event, context)

except Exception as e:

logger.error(f"Error handler failed: {str(e)}")

logger.error(traceback.format_exc())

return {

'statusCode': 500,

'body': json.dumps({'error': 'Error handler failed', 'details': str(e)})

}

def \_handle_codepipeline_event(self, event, context):

"""

CodePipelineからの呼び出し処理

"""

job_id = event\['CodePipeline.job'\]\['id'\]

job_data = event\['CodePipeline.job'\]\['data'\]

try:

*\# ユーザーパラメータから詳細取得*

user_params = json.loads(job_data.get('actionConfiguration',
{}).get('configuration', {}).get('UserParameters', '{}'))

error_details = {

'job_id': job_id,

'pipeline_name': user_params.get('pipeline_name', 'unknown'),

'stage_name': user_params.get('stage_name', 'unknown'),

'error_type': user_params.get('error_type', 'UNKNOWN_ERROR'),

'pipeline_execution_id': job_data.get('inputArtifacts',
\[{}\])\[0\].get('location', {}).get('s3Location', {}).get('objectKey',
'unknown'),

'timestamp': datetime.now(timezone.utc).isoformat()

}

*\# エラー処理実行*

self.\_process_error(error_details)

*\# CodePipelineにジョブ成功を通知*

codepipeline.put_job_success_result(jobId=job_id)

return {'statusCode': 200, 'body': 'Error handled successfully'}

except Exception as e:

logger.error(f"Failed to handle CodePipeline event: {str(e)}")

codepipeline.put_job_failure_result(

jobId=job_id,

failureDetails={'message': f'Error handler failed: {str(e)}', 'type':
'JobFailed'}

)

raise

def \_handle_direct_invocation(self, event, context):

"""

Lambda直接呼び出しの処理

"""

error_details = {

'pipeline_execution_id': event.get('pipeline_execution_id', 'unknown'),

'stage': event.get('stage', 'unknown'),

'error_type': event.get('error_type', 'UNKNOWN_ERROR'),

'exit_code': event.get('exit_code', -1),

'log_file': event.get('log_file', ''),

'error_handling_mode': event.get('error_handling_mode', 'STOP'),

'timestamp': datetime.now(timezone.utc).isoformat()

}

*\# エラー処理実行*

self.\_process_error(error_details)

return {

'statusCode': 200,

'body': json.dumps({'status': 'processed', 'error_details':
error_details})

}

def \_process_error(self, error_details: Dict\[str, Any\]):

"""

エラー処理のメインロジック

"""

logger.info(f"Processing error: {error_details}")

*\# 1. エラー情報をDynamoDBに記録*

self.\_store_error_record(error_details)

*\# 2. エラーの重要度分析*

severity = self.\_analyze_error_severity(error_details)

error_details\['severity'\] = severity

*\# 3. 関連ログの取得と*

log_analysis = self.\_analyze_error_logs(error_details)

error_details\['log_analysis'\] = log_analysis

\# 4. 通知の送信

self.\_send_notifications(error_details)

\# 5. 自動修復の試行（可能な場合）

if self.\_can_auto_remediate(error_details):

remediation_result = self.\_attempt_auto_remediation(error_details)

error_details\['remediation'\] = remediation_result

\# 6. エスカレーション判定

if self.\_should_escalate(error_details):

self.\_escalate_error(error_details)

logger.info(f"Error processing completed for:
{error_details\['pipeline_execution_id'\]}")

def \_store_error_record(self, error_details: Dict\[str, Any\]):

"""

エラー情報をDynamoDBに永続化

"""

try:

item = {

'pipeline_execution_id': error_details\['pipeline_execution_id'\],

'timestamp': error_details\['timestamp'\],

'stage_name': error_details.get('stage', error_details.get('stage_name',
'unknown')),

'error_type': error_details\['error_type'\],

'exit_code': error_details.get('exit_code', -1),

'pipeline_name': error_details.get('pipeline_name',
'security-integrated-deployment-pipeline'),

'error_handling_mode': error_details.get('error_handling_mode', 'STOP'),

'status': 'PROCESSING',

'created_at': datetime.now(timezone.utc).isoformat(),

'ttl': int((datetime.now(timezone.utc).timestamp() + (90 \* 24 \* 60 \*
60))) \# 90日後に期限切れ

}

self.table.put_item(Item=item)

logger.info(f"Error record stored:
{error_details\['pipeline_execution_id'\]}")

except Exception as e:

logger.error(f"Failed to store error record: {str(e)}")

\# エラー記録の失敗は処理を停止させない

def \_analyze_error_severity(self, error_details: Dict\[str, Any\]) -\>
str:

"""

エラーの重要度を分析

"""

error_type = error_details.get('error_type', '').upper()

stage = error_details.get('stage', error_details.get('stage_name',
'')).upper()

exit_code = error_details.get('exit_code', 0)

\# セキュリティ関連エラーの重要度分析

if any(keyword in error_type for keyword in \[

'CRITICAL', 'HIGH_SEVERITY', 'SECURITY_VIOLATION',

'EXTERNAL_ACCESS', 'VULNERABILITY', 'PENTEST'

\]):

return 'CRITICAL'

\# 段階別重要度

if 'CHECKOV' in stage or 'IAM_ANALYSIS' in stage or 'DYNAMIC' in stage:

if 'HIGH' in error_type or exit_code \> 10:

return 'HIGH'

elif 'MEDIUM' in error_type or exit_code \> 0:

return 'MEDIUM'

\# 静的解析エラー

if any(stage_name in stage for stage_name in \['TERRAFORM', 'TFLINT'\]):

if exit_code \> 0:

return 'MEDIUM'

else:

return 'LOW'

\# その他のエラー

if exit_code \> 0:

return 'MEDIUM'

return 'LOW'

def \_analyze_error_logs(self, error_details: Dict\[str, Any\]) -\>
Dict\[str, Any\]:

"""

関連ログファイルを取得して分析

"""

log_analysis = {

'log_retrieved': False,

'error_patterns': \[\],

'recommendations': \[\],

'log_size': 0

}

try:

log_file = error_details.get('log_file', '')

pipeline_execution_id = error_details\['pipeline_execution_id'\]

if log_file and pipeline_execution_id:

\# S3からログファイルを取得

s3_key = f"logs/{pipeline_execution_id}/{log_file}"

try:

response = s3.get_object(Bucket=S3_BUCKET, Key=s3_key)

log_content = response\['Body'\].read().decode('utf-8')

log_analysis\['log_retrieved'\] = True

log_analysis\['log_size'\] = len(log_content)

\# ログ内容の分析

log_analysis\['error_patterns'\] =
self.\_extract_error_patterns(log_content,
error_details\['error_type'\])

log_analysis\['recommendations'\] =
self.\_generate_recommendations(log_content, error_details)

except s3.exceptions.NoSuchKey:

logger.warning(f"Log file not found: {s3_key}")

except Exception as e:

logger.error(f"Failed to retrieve log file: {str(e)}")

except Exception as e:

logger.error(f"Log analysis failed: {str(e)}")

return log_analysis

def \_extract_error_patterns(self, log_content: str, error_type: str)
-\> List\[str\]:

"""

ログからエラーパターンを抽出

"""

patterns = \[\]

try:

lines = log_content.split('\n')

error_lines = \[line for line in lines if any(keyword in line.upper()
for keyword in \['ERROR', 'FAIL', 'CRITICAL', 'VIOLATION'\])\]

\# エラータイプ別のパターン抽出

if 'TERRAFORM' in error_type:

terraform_errors = \[line for line in error_lines if any(tf_keyword in
line for tf_keyword in \['Invalid', 'Missing', 'Duplicate'\])\]

patterns.extend(terraform_errors\[:5\]) \# 最初の5つのエラー

elif 'TFLINT' in error_type:

tflint_errors = \[line for line in error_lines if 'tflint' in
line.lower()\]

patterns.extend(tflint_errors\[:5\])

elif 'CHECKOV' in error_type:

checkov_errors = \[line for line in error_lines if any(ckv in line for
ckv in \['CKV\_', 'Check:', 'FAILED'\])\]

patterns.extend(checkov_errors\[:10\])

elif 'IAM' in error_type:

iam_errors = \[line for line in error_lines if any(iam_keyword in
line.upper() for iam_keyword in \['ACCESS', 'PERMISSION', 'ROLE',
'POLICY'\])\]

patterns.extend(iam_errors\[:5\])

elif 'VULNERABILITY' in error_type:

vuln_errors = \[line for line in error_lines if any(vuln_keyword in
line.upper() for vuln_keyword in \['CVE', 'VULN', 'EXPLOIT', 'HIGH',
'CRITICAL'\])\]

patterns.extend(vuln_errors\[:10\])

else:

\# 一般的なエラーパターン

patterns.extend(error_lines\[:5\])

except Exception as e:

logger.error(f"Error pattern extraction failed: {str(e)}")

return patterns\[:10\] \# 最大10個のパターンを返す

def \_generate_recommendations(self, log_content: str, error_details:
Dict\[str, Any\]) -\> List\[str\]:

"""

エラーに基づく修復推奨事項を生成

"""

recommendations = \[\]

error_type = error_details.get('error_type', '').upper()

try:

\# エラータイプ別の推奨事項

if 'TERRAFORM_VALIDATE_FAILED' in error_type:

if 'Invalid reference' in log_content:

recommendations.append("Fix invalid resource references in Terraform
configuration")

if 'Missing required argument' in log_content:

recommendations.append("Add missing required arguments to resource
blocks")

if 'Duplicate resource' in log_content:

recommendations.append("Remove duplicate resource definitions")

recommendations.append("Run 'terraform validate' locally before
committing")

elif 'TFLINT' in error_type:

recommendations.extend(\[

"Review TFLint findings and fix code quality issues",

"Update Terraform modules to latest versions",

"Follow Terraform best practices and naming conventions",

"Consider using terraform fmt for code formatting"

\])

elif 'CHECKOV' in error_type or 'SECURITY_VIOLATION' in error_type:

recommendations.extend(\[

"Address security misconfigurations identified by Checkov",

"Enable encryption for all data at rest and in transit",

"Review and restrict security group rules",

"Implement least privilege access principles",

"Enable logging and monitoring for all resources"

\])

elif 'IAM' in error_type:

recommendations.extend(\[

"Review and remove unused IAM roles and policies",

"Implement least privilege access principles",

"Remove cross-account trust relationships with external accounts",

"Enable IAM Access Analyzer findings remediation",

"Conduct regular IAM access reviews"

\])

elif 'VULNERABILITY' in error_type or 'PENTEST' in error_type:

recommendations.extend(\[

"Patch identified vulnerabilities immediately",

"Update software packages to latest secure versions",

"Review and harden network security configurations",

"Implement Web Application Firewall (WAF) rules",

"Conduct regular vulnerability assessments"

\])

else:

recommendations.extend(\[

"Review error logs for specific failure details",

"Check resource dependencies and configuration",

"Verify AWS permissions and service limits",

"Test changes in development environment first"

\])

except Exception as e:

logger.error(f"Recommendation generation failed: {str(e)}")

return recommendations\[:8\] \# 最大8個の推奨事項

def \_send_notifications(self, error_details: Dict\[str, Any\]):

"""

エラー通知の送信

"""

try:

severity = error_details.get('severity', 'MEDIUM')

\# SNS通知

self.\_send_sns_notification(error_details)

\# 重要度が高い場合はSlack通知も送信

if severity in \['CRITICAL', 'HIGH'\]:

self.\_send_slack_notification(error_details)

logger.info(f"Notifications sent for error:
{error_details\['pipeline_execution_id'\]}")

except Exception as e:

logger.error(f"Failed to send notifications: {str(e)}")

def \_send_sns_notification(self, error_details: Dict\[str, Any\]):

"""

SNS通知の送信

"""

try:

subject = f"Pipeline Error: {error_details\['error_type'\]} in
{error_details.get('stage', 'unknown')}"

message_parts = \[

f"Pipeline Execution ID: {error_details\['pipeline_execution_id'\]}",

f"Stage: {error_details.get('stage', error_details.get('stage_name',
'unknown'))}",

f"Error Type: {error_details\['error_type'\]}",

f"Severity: {error_details.get('severity', 'MEDIUM')}",

f"Timestamp: {error_details\['timestamp'\]}",

""

\]

\# ログ分析結果の追加

if error_details.get('log_analysis', {}).get('error_patterns'):

message_parts.append("Error Patterns:")

for pattern in
error_details\['log_analysis'\]\['error_patterns'\]\[:3\]:

message_parts.append(f" - {pattern\[:100\]}...")

message_parts.append("")

\# 推奨事項の追加

if error_details.get('log_analysis', {}).get('recommendations'):

message_parts.append("Recommendations:")

for rec in error_details\['log_analysis'\]\['recommendations'\]\[:3\]:

message_parts.append(f" - {rec}")

message_parts.append("")

message_parts.append(f"View full details in CloudWatch Logs and S3
bucket: {S3_BUCKET}")

message = "\n".join(message_parts)

sns.publish(

TopicArn=SNS_TOPIC_ARN,

Subject=subject,

Message=message

)

except Exception as e:

logger.error(f"SNS notification failed: {str(e)}")

def \_send_slack_notification(self, error_details: Dict\[str, Any\]):

"""

Slack通知の送信（重要度の高いエラー用）

"""

try:

\# Slack Webhook URLをSSMから取得

webhook_response = ssm.get_parameter(Name=SLACK_WEBHOOK_SECRET,
WithDecryption=True)

webhook_url = webhook_response\['Parameter'\]\['Value'\]

severity = error_details.get('severity', 'MEDIUM')

color = {'CRITICAL': 'danger', 'HIGH': 'warning', 'MEDIUM': '#ffaa00',
'LOW': 'good'}.get(severity, '#ffaa00')

slack_message = {

"attachments": \[

{

"color": color,

"title": f"🚨 Pipeline Security Error - {severity}",

"title_link":
f"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/{error_details.get('pipeline_name',
'unknown')}/view",

"fields": \[

{

"title": "Pipeline Execution",

"value": error_details\['pipeline_execution_id'\],

"short": True

},

{

"title": "Stage",

"value": error_details.get('stage', error_details.get('stage_name',
'unknown')),

"short": True

},

{

"title": "Error Type",

"value": error_details\['error_type'\],

"short": True

},

{

"title": "Severity",

"value": severity,

"short": True

}

\],

"footer": "TechNova DevSecOps Pipeline",

"ts": int(datetime.now(timezone.utc).timestamp())

}

\]

}

\# 推奨事項がある場合は追加

if error_details.get('log_analysis', {}).get('recommendations'):

recommendations_text = "\n".join(\[f"• {rec}" for rec in
error_details\['log_analysis'\]\['recommendations'\]\[:3\]\])

slack_message\["attachments"\]\[0\]\["fields"\].append({

"title": "Immediate Actions",

"value": recommendations_text,

"short": False

})

\# Slack WebhookにPOST（requests使用を避けてurllib使用）

import urllib.request

import urllib.parse

data = json.dumps(slack_message).encode('utf-8')

req = urllib.request.Request(webhook_url, data=data,
headers={'Content-Type': 'application/json'})

urllib.request.urlopen(req)

logger.info("Slack notification sent successfully")

except Exception as e:

logger.error(f"Slack notification failed: {str(e)}")

def \_can_auto_remediate(self, error_details: Dict\[str, Any\]) -\>
bool:

"""

自動修復可能かどうかを判定

"""

error_type = error_details.get('error_type', '').upper()

\# 自動修復可能なエラータイプ

auto_remediable_errors = \[

'TERRAFORM_VALIDATE_FAILED',

'TFLINT_LOW_SEVERITY_VIOLATIONS',

'UNUSED_PERMISSIONS'

\]

return any(remediable in error_type for remediable in
auto_remediable_errors)

def \_attempt_auto_remediation(self, error_details: Dict\[str, Any\])
-\> Dict\[str, Any\]:

"""

自動修復の試行

"""

remediation_result = {

'attempted': True,

'success': False,

'actions_taken': \[\],

'error': None

}

try:

error_type = error_details.get('error_type', '').upper()

if 'TERRAFORM_VALIDATE' in error_type:

\# Terraformフォーマット修正の試行

remediation_result\['actions_taken'\].append("Attempted terraform fmt on
configuration files")

elif 'UNUSED_PERMISSIONS' in error_type:

\# 未使用権限のドキュメント化

remediation_result\['actions_taken'\].append("Documented unused
permissions for manual review")

\# 実際の修復アクションは安全性のため制限的に実装

remediation_result\['success'\] = True

except Exception as e:

remediation_result\['error'\] = str(e)

logger.error(f"Auto-remediation failed: {str(e)}")

return remediation_result

def \_should_escalate(self, error_details: Dict\[str, Any\]) -\> bool:

"""

エスカレーションが必要かどうかを判定

"""

severity = error_details.get('severity', 'LOW')

error_type = error_details.get('error_type', '').upper()

\# 自動エスカレーション条件

escalation_conditions = \[

severity == 'CRITICAL',

'SECURITY_VIOLATION' in error_type,

'VULNERABILITY' in error_type and 'HIGH' in error_type,

'EXTERNAL_ACCESS' in error_type,

'PENTEST' in error_type and 'CRITICAL' in error_type

\]

return any(escalation_conditions)

def \_escalate_error(self, error_details: Dict\[str, Any\]):

"""

エラーのエスカレーション処理

"""

try:

\# エスカレーション専用のSNSトピックに通知

escalation_subject = f"🚨 ESCALATION REQUIRED:
{error_details\['error_type'\]}"

escalation_message = f"""

CRITICAL SECURITY ISSUE REQUIRES IMMEDIATE ATTENTION

Pipeline Execution: {error_details\['pipeline_execution_id'\]} Error
Type: {error_details\['error_type'\]} Severity:
{error_details.get('severity', 'UNKNOWN')} Stage:
{error_details.get('stage', 'unknown')} Timestamp:
{error_details\['timestamp'\]}

This error has been automatically escalated due to its critical nature.
Immediate investigation and remediation required.

Security Team: Please review and take immediate action. DevOps Team:
Pipeline execution has been halted pending resolution. """

\# 管理者向けSNS通知

sns.publish(

TopicArn=SNS_TOPIC_ARN.replace('pipeline-errors',
'security-escalations'), \# エスカレーション用トピック

Subject=escalation_subject,

Message=escalation_message

)

\# DynamoDBの記録を更新

self.table.update_item(

Key={

'pipeline_execution_id': error_details\['pipeline_execution_id'\],

'timestamp': error_details\['timestamp'\]

},

UpdateExpression='SET \#status = :status, escalated_at = :escalated_at',

ExpressionAttributeNames={'#status': 'status'},

ExpressionAttributeValues={

':status': 'ESCALATED',

':escalated_at': datetime.now(timezone.utc).isoformat()

}

)

logger.warning(f"Error escalated:
{error_details\['pipeline_execution_id'\]}")

except Exception as e:

logger.error(f"Escalation failed: {str(e)}")

**Lambda ハンドラー**

def handler(event, context): error_handler = PipelineErrorHandler()
return error_handler.handler(event, context)

\### 5.3 エラーハンドリング用のLambda IAMロール

\`\`\`hcl

resource "aws_iam_role" "lambda_error_handler_role" {

name = "lambda-pipeline-error-handler-role"

assume_role_policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Action = "sts:AssumeRole"

Effect = "Allow"

Principal = {

Service = "lambda.amazonaws.com"

}

}

\]

})

tags = {

Purpose = "Lambda Error Handler Role"

Component = "ErrorHandling"

}

}

resource "aws_iam_role_policy" "lambda_error_handler_policy" {

name = "lambda-pipeline-error-handler-policy"

role = aws_iam_role.lambda_error_handler_role.id

policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Effect = "Allow"

Action = \[

"logs:CreateLogGroup",

"logs:CreateLogStream",

"logs:PutLogEvents"

\]

Resource = "arn:aws:logs:\*:\*:\*"

},

{

Effect = "Allow"

Action = \[

"sns:Publish"

\]

Resource = \[

aws_sns_topic.pipeline_errors.arn,

aws_sns_topic.security_escalations.arn

\]

},

{

Effect = "Allow"

Action = \[

"dynamodb:PutItem",

"dynamodb:UpdateItem",

"dynamodb:GetItem",

"dynamodb:Query"

\]

Resource = aws_dynamodb_table.pipeline_errors.arn

},

{

Effect = "Allow"

Action = \[

"s3:GetObject",

"s3:PutObject"

\]

Resource = \[

"\${aws_s3_bucket.build_logs.arn}/\*"

\]

},

{

Effect = "Allow"

Action = \[

"ssm:GetParameter"

\]

Resource = \[

aws_ssm_parameter.slack_webhook.arn

\]

},

{

Effect = "Allow"

Action = \[

"codepipeline:PutJobSuccessResult",

"codepipeline:PutJobFailureResult"

\]

Resource = "\*"

}

\]

})

}

5.4 エラー記録用DynamoDBテーブル

resource "aws_dynamodb_table" "pipeline_errors" {

name = "pipeline-errors"

billing_mode = "PAY_PER_REQUEST"

hash_key = "pipeline_execution_id"

range_key = "timestamp"

attribute {

name = "pipeline_execution_id"

type = "S"

}

attribute {

name = "timestamp"

type = "S"

}

attribute {

name = "error_type"

type = "S"

}

attribute {

name = "severity"

type = "S"

}

global_secondary_index {

name = "ErrorTypeIndex"

hash_key = "error_type"

range_key = "timestamp"

}

global_secondary_index {

name = "SeverityIndex"

hash_key = "severity"

range_key = "timestamp"

}

ttl {

attribute_name = "ttl"

enabled = true

}

tags = {

Purpose = "Pipeline Error Tracking"

Component = "ErrorHandling"

}

}

**6. 監視とレポーティングシステム**

**6.1 CloudWatchダッシュボード**

resource "aws_cloudwatch_dashboard" "security_pipeline_dashboard" {

dashboard_name = "TechNova-SecurityPipeline-Dashboard"

dashboard_body = jsonencode({

widgets = \[

{

type = "metric"

x = 0

y = 0

width = 12

height = 6

properties = {

metrics = \[

\["TechNova/SecurityAnalysis", "TFLintErrors"\],

\[".", "TFLintWarnings"\],

\[".", "CheckovTotalChecks"\],

\[".", "CheckovFailedChecks"\],

\[".", "CheckovComplianceScore"\]

\]

view = "timeSeries"

stacked = false

region = "us-east-1"

title = "Static Analysis Metrics"

period = 300

}

},

{

type = "metric"

x = 12

y = 0

width = 12

height = 6

properties = {

metrics = \[

\["TechNova/IAMSecurity", "ExternalAccessFindings"\],

\[".", "UnusedPermissions"\],

\[".", "CrossAccountTrusts"\],

\[".", "TotalSecurityFindings"\]

\]

view = "timeSeries"

stacked = false

region = "us-east-1"

title = "IAM Security Metrics"

period = 300

}

},

{

type = "metric"

x = 0

y = 6

width = 8

height = 6

properties = {

metrics = \[

\["TechNova/VulnerabilityScanning", "VulnerabilitiesFound"\],

\[".", "HighSeverityVulns"\],

\[".", "MediumSeverityVulns"\]

\]

view = "timeSeries"

stacked = true

region = "us-east-1"

title = "Vulnerability Scan Results"

period = 300

}

},

{

type = "metric"

x = 8

y = 6

width = 8

height = 6

properties = {

metrics = \[

\["TechNova/ApplicationSecurity", "AppVulnerabilitiesFound"\],

\[".", "AppHighSeverityVulns"\],

\[".", "InjectionVulnerabilities"\]

\]

view = "timeSeries"

stacked = false

region = "us-east-1"

title = "Application Security Metrics"

period = 300

}

},

{

type = "metric"

x = 16

y = 6

width = 8

height = 6

properties = {

metrics = \[

\["TechNova/PenetrationTesting", "VulnerabilitiesExploited"\],

\[".", "CriticalPentestFindings"\],

\[".", "HighPentestFindings"\]

\]

view = "timeSeries"

stacked = false

region = "us-east-1"

title = "Penetration Test Results"

period = 300

}

},

{

type = "log"

x = 0

y = 12

width = 24

height = 6

properties = {

query = "SOURCE '/aws/codebuild/terraform-validate-with-error-handling'
\| fields @timestamp, @message \| filter @message like /ERROR/ \| sort
@timestamp desc \| limit 50"

region = "us-east-1"

title = "Recent Pipeline Errors"

view = "table"

}

}

\]

})

}

6.2 セキュリティメトリクス集約Lambda

resource "aws_lambda_function" "security_metrics_aggregator" {

filename = "lambda/security_metrics_aggregator.zip"

function_name = "security-metrics-aggregator"

role = aws_iam_role.lambda_metrics_role.arn

handler = "index.handler"

source_code_hash =
data.archive_file.security_metrics_aggregator_zip.output_base64sha256

runtime = "python3.9"

timeout = 900

environment {

variables = {

DYNAMODB_TABLE = aws_dynamodb_table.pipeline_errors.name

S3_BUCKET = aws_s3_bucket.build_logs.id

SNS_TOPIC_ARN = aws_sns_topic.security_reports.arn

}

}

tags = {

Purpose = "Security Metrics Aggregation"

Component = "Monitoring"

}

}

\# EventBridge rule for daily execution

resource "aws_cloudwatch_event_rule" "daily_security_report" {

name = "daily-security-metrics-aggregation"

description = "Trigger security metrics aggregation daily"

schedule_expression = "cron(0 8 \* \* ? \*)" \# 毎日午前8時UTC

tags = {

Purpose = "Daily Security Reporting"

}

}

resource "aws_cloudwatch_event_target" "security_metrics_target" {

rule = aws_cloudwatch_event_rule.daily_security_report.name

target_id = "SecurityMetricsAggregatorTarget"

arn = aws_lambda_function.security_metrics_aggregator.arn

}

resource "aws_lambda_permission" "allow_eventbridge" {

statement_id = "AllowExecutionFromEventBridge"

action = "lambda:InvokeFunction"

function_name =
aws_lambda_function.security_metrics_aggregator.function_name

principal = "events.amazonaws.com"

source_arn = aws_cloudwatch_event_rule.daily_security_report.arn

}

6.3 security_metrics_aggregator.py（日次セキュリティレポート）

import json

import boto3

import os

from datetime import datetime, timezone, timedelta

from typing import Dict, List, Any

import logging

logger = logging.getLogger()

logger.setLevel(logging.INFO)

\# AWSクライアント

cloudwatch = boto3.client('cloudwatch')

dynamodb = boto3.resource('dynamodb')

s3 = boto3.client('s3')

sns = boto3.client('sns')

\# 環境変数

DYNAMODB_TABLE = os.environ\['DYNAMODB_TABLE'\]

S3_BUCKET = os.environ\['S3_BUCKET'\] SNS_TOPIC_ARN =
os.environ\['SNS_TOPIC_ARN'\]

class SecurityMetricsAggregator: def **init**(self): self.table =
dynamodb.Table(DYNAMODB_TABLE)

def handler(self, event, context):

"""

日次セキュリティメトリクス集約のメインハンドラー

"""

try:

logger.info("Starting daily security metrics aggregation")

\# 過去24時間のメトリクス取得

end_time = datetime.now(timezone.utc)

start_time = end_time - timedelta(days=1)

\# 各セキュリティ領域のメトリクス集約

static_analysis_metrics = self.\_get_static_analysis_metrics(start_time,
end_time)

iam_security_metrics = self.\_get_iam_security_metrics(start_time,
end_time)

vulnerability_metrics = self.\_get_vulnerability_metrics(start_time,
end_time)

app_security_metrics = self.\_get_app_security_metrics(start_time,
end_time)

pentest_metrics = self.\_get_pentest_metrics(start_time, end_time)

error_metrics = self.\_get_error_metrics(start_time, end_time)

\# 統合レポート作成

daily_report = {

'report_date': end_time.strftime('%Y-%m-%d'),

'report_period': {

'start': start_time.isoformat(),

'end': end_time.isoformat()

},

'static_analysis': static_analysis_metrics,

'iam_security': iam_security_metrics,

'vulnerability_scanning': vulnerability_metrics,

'application_security': app_security_metrics,

'penetration_testing': pentest_metrics,

'error_analysis': error_metrics,

'overall_security_score': self.\_calculate_security_score(

static_analysis_metrics, iam_security_metrics,

vulnerability_metrics, app_security_metrics, pentest_metrics

)

}

\# レポート保存とトレンド分析

self.\_save_report(daily_report)

trend_analysis = self.\_analyze_trends(daily_report)

daily_report\['trend_analysis'\] = trend_analysis

\# 通知とアラート

self.\_send_daily_report(daily_report)

\# 重要な問題がある場合はアラート

if self.\_should_alert(daily_report):

self.\_send_security_alert(daily_report)

logger.info("Daily security metrics aggregation completed successfully")

return {

'statusCode': 200,

'body': json.dumps({

'status': 'success',

'report_date': daily_report\['report_date'\],

'overall_security_score': daily_report\['overall_security_score'\]

})

}

except Exception as e:

logger.error(f"Security metrics aggregation failed: {str(e)}")

return {

'statusCode': 500,

'body': json.dumps({'error': str(e)})

}

def \_get_static_analysis_metrics(self, start_time: datetime, end_time:
datetime) -\> Dict\[str, Any\]:

"""

静的解析メトリクスの取得

"""

metrics = {}

try:

\# TFLintメトリクス

tflint_errors = self.\_get_cloudwatch_metric_sum(

'TechNova/SecurityAnalysis', 'TFLintErrors', start_time, end_time

)

tflint_warnings = self.\_get_cloudwatch_metric_sum(

'TechNova/SecurityAnalysis', 'TFLintWarnings', start_time, end_time

)

\# Checkovメトリクス

checkov_total = self.\_get_cloudwatch_metric_sum(

'TechNova/SecurityAnalysis', 'CheckovTotalChecks', start_time, end_time

)

checkov_failed = self.\_get_cloudwatch_metric_sum(

'TechNova/SecurityAnalysis', 'CheckovFailedChecks', start_time, end_time

)

checkov_compliance = self.\_get_cloudwatch_metric_average(

'TechNova/SecurityAnalysis', 'CheckovComplianceScore', start_time,
end_time

)

metrics = {

'tflint': {

'errors': tflint_errors,

'warnings': tflint_warnings,

'total_issues': tflint_errors + tflint_warnings

},

'checkov': {

'total_checks': checkov_total,

'failed_checks': checkov_failed,

'compliance_score': checkov_compliance,

'pass_rate': ((checkov_total - checkov_failed) / checkov_total \* 100)
if checkov_total \> 0 else 100

},

'overall_static_score':
self.\_calculate_static_analysis_score(tflint_errors, tflint_warnings,
checkov_compliance)

}

except Exception as e:

logger.error(f"Failed to get static analysis metrics: {str(e)}")

metrics = {'error': str(e)}

return metrics

def \_get_iam_security_metrics(self, start_time: datetime, end_time:
datetime) -\> Dict\[str, Any\]:

"""

IAMセキュリティメトリクスの取得

"""

metrics = {}

try:

external_access = self.\_get_cloudwatch_metric_sum(

'TechNova/IAMSecurity', 'ExternalAccessFindings', start_time, end_time

)

unused_permissions = self.\_get_cloudwatch_metric_sum(

'TechNova/IAMSecurity', 'UnusedPermissions', start_time, end_time

)

cross_account_trusts = self.\_get_cloudwatch_metric_sum(

'TechNova/IAMSecurity', 'CrossAccountTrusts', start_time, end_time

)

total_findings = self.\_get_cloudwatch_metric_sum(

'TechNova/IAMSecurity', 'TotalSecurityFindings', start_time, end_time

)

metrics = {

'external_access_findings': external_access,

'unused_permissions': unused_permissions,

'cross_account_trusts': cross_account_trusts,

'total_findings': total_findings,

'iam_security_score':
self.\_calculate_iam_security_score(external_access, unused_permissions,
cross_account_trusts),

'risk_level': self.\_determine_iam_risk_level(external_access,
unused_permissions, total_findings)

}

except Exception as e:

logger.error(f"Failed to get IAM security metrics: {str(e)}")

metrics = {'error': str(e)}

return metrics

def \_get_vulnerability_metrics(self, start_time: datetime, end_time:
datetime) -\> Dict\[str, Any\]:

"""

脆弱性スキャンメトリクスの取得

"""

metrics = {}

try:

targets_scanned = self.\_get_cloudwatch_metric_sum(

'TechNova/VulnerabilityScanning', 'TargetsScanned', start_time, end_time

)

vulnerabilities_found = self.\_get_cloudwatch_metric_sum(

'TechNova/VulnerabilityScanning', 'VulnerabilitiesFound', start_time,
end_time

)

high_severity = self.\_get_cloudwatch_metric_sum(

'TechNova/VulnerabilityScanning', 'HighSeverityVulns', start_time,
end_time

)

medium_severity = self.\_get_cloudwatch_metric_sum(

'TechNova/VulnerabilityScanning', 'MediumSeverityVulns', start_time,
end_time

)

metrics = {

'targets_scanned': targets_scanned,

'vulnerabilities_found': vulnerabilities_found,

'high_severity_vulns': high_severity,

'medium_severity_vulns': medium_severity,

'vulnerability_density': vulnerabilities_found / targets_scanned if
targets_scanned \> 0 else 0,

'critical_vulnerability_rate': high_severity / vulnerabilities_found \*
100 if vulnerabilities_found \> 0 else 0,

'vulnerability_risk_score':
self.\_calculate_vulnerability_risk_score(high_severity,
medium_severity, vulnerabilities_found)

}

except Exception as e:

logger.error(f"Failed to get vulnerability metrics: {str(e)}")

metrics = {'error': str(e)}

return metrics

def \_get_app_security_metrics(self, start_time: datetime, end_time:
datetime) -\> Dict\[str, Any\]:

"""

アプリケーションセキュリティメトリクスの取得

"""

metrics = {}

try:

app_targets = self.\_get_cloudwatch_metric_sum(

'TechNova/ApplicationSecurity', 'AppTargetsScanned', start_time,
end_time

)

app_vulns = self.\_get_cloudwatch_metric_sum(

'TechNova/ApplicationSecurity', 'AppVulnerabilitiesFound', start_time,
end_time

)

app_high_severity = self.\_get_cloudwatch_metric_sum(

'TechNova/ApplicationSecurity', 'AppHighSeverityVulns', start_time,
end_time

)

injection_vulns = self.\_get_cloudwatch_metric_sum(

'TechNova/ApplicationSecurity', 'InjectionVulnerabilities', start_time,
end_time

)

metrics = {

'applications_scanned': app_targets,

'vulnerabilities_found': app_vulns,

'high_severity_vulns': app_high_severity,

'injection_vulnerabilities': injection_vulns,

'owasp_top10_coverage': self.\_calculate_owasp_coverage(app_vulns,
injection_vulns),

'application_security_score':
self.\_calculate_app_security_score(app_high_severity, app_vulns,
app_targets)

}

except Exception as e:

logger.error(f"Failed to get application security metrics: {str(e)}")

metrics = {'error': str(e)}

return metrics

def \_get_pentest_metrics(self, start_time: datetime, end_time:
datetime) -\> Dict\[str, Any\]:

"""

ペネトレーションテストメトリクスの取得

"""

metrics = {}

try:

pentest_targets = self.\_get_cloudwatch_metric_sum(

'TechNova/PenetrationTesting', 'PentestTargetsTested', start_time,
end_time

)

vulns_exploited = self.\_get_cloudwatch_metric_sum(

'TechNova/PenetrationTesting', 'VulnerabilitiesExploited', start_time,
end_time

)

critical_findings = self.\_get_cloudwatch_metric_sum(

'TechNova/PenetrationTesting', 'CriticalPentestFindings', start_time,
end_time

)

high_findings = self.\_get_cloudwatch_metric_sum(

'TechNova/PenetrationTesting', 'HighPentestFindings', start_time,
end_time

)

metrics = {

'targets_tested': pentest_targets,

'vulnerabilities_exploited': vulns_exploited,

'critical_findings': critical_findings,

'high_findings': high_findings,

'exploitation_rate': vulns_exploited / pentest_targets \* 100 if
pentest_targets \> 0 else 0,

'pentest_security_score':
self.\_calculate_pentest_security_score(vulns_exploited,
critical_findings, high_findings)

}

except Exception as e:

logger.error(f"Failed to get penetration test metrics: {str(e)}")

metrics = {'error': str(e)}

return metrics

def \_get_error_metrics(self, start_time: datetime, end_time: datetime)
-\> Dict\[str, Any\]:

"""

エラーメトリクスの取得

"""

metrics = {}

try:

\# DynamoDBからエラー統計を取得

response = self.table.scan(

FilterExpression='#ts BETWEEN :start AND :end',

ExpressionAttributeNames={'#ts': 'timestamp'},

ExpressionAttributeValues={

':start': start_time.isoformat(),

':end': end_time.isoformat()

}

)

errors = response\['Items'\]

total_errors = len(errors)

\# 重要度別集計

severity_counts = {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0}

error_type_counts = {}

stage_error_counts = {}

for error in errors:

severity = error.get('severity', 'UNKNOWN')

error_type = error.get('error_type', 'UNKNOWN')

stage = error.get('stage_name', 'UNKNOWN')

severity_counts\[severity\] = severity_counts.get(severity, 0) + 1

error_type_counts\[error_type\] = error_type_counts.get(error_type, 0) +
1

stage_error_counts\[stage\] = stage_error_counts.get(stage, 0) + 1

metrics = {

'total_errors': total_errors,

'severity_breakdown': severity_counts,

'top_error_types': dict(sorted(error_type_counts.items(), key=lambda x:
x\[1\], reverse=True)\[:5\]),

'stage_error_distribution': stage_error_counts,

'error_rate_trend': self.\_calculate_error_rate_trend(start_time,
end_time),

'pipeline_reliability_score':
self.\_calculate_pipeline_reliability_score(severity_counts,
total_errors)

}

except Exception as e:

logger.error(f"Failed to get error metrics: {str(e)}")

metrics = {'error': str(e)}

return metrics

def \_get_cloudwatch_metric_sum(self, namespace: str, metric_name: str,
start_time: datetime, end_time: datetime) -\> float:

"""

CloudWatchメトリクスの合計値を取得

"""

try:

response = cloudwatch.get_metric_statistics(

Namespace=namespace,

MetricName=metric_name,

StartTime=start_time,

EndTime=end_time,

Period=3600, \# 1時間ごと

Statistics=\['Sum'\]

)

return sum(point\['Sum'\] for point in response\['Datapoints'\])

except Exception:

return 0.0

def \_get_cloudwatch_metric_average(self, namespace: str, metric_name:
str, start_time: datetime, end_time: datetime) -\> float:

"""

CloudWatchメトリクスの平均値を取得

"""

try:

response = cloudwatch.get_metric_statistics(

Namespace=namespace,

MetricName=metric_name,

StartTime=start_time,

EndTime=end_time,

Period=3600,

Statistics=\['Average'\]

)

if response\['Datapoints'\]:

return sum(point\['Average'\] for point in response\['Datapoints'\]) /
len(response\['Datapoints'\])

return 0.0

except Exception:

return 0.0

def \_calculate_security_score(self, static_analysis: Dict,
iam_security: Dict,

vulnerability: Dict, app_security: Dict, pentest: Dict) -\> Dict\[str,
Any\]:

"""

全体的なセキュリティスコアの計算

"""

try:

\# 各領域のスコア（0-100）

static_score = static_analysis.get('overall_static_score', 50)

iam_score = iam_security.get('iam_security_score', 50)

vuln_score = 100 - vulnerability.get('vulnerability_risk_score', 50)

app_score = app_security.get('application_security_score', 50)

pentest_score = pentest.get('pentest_security_score', 50)

\# 重み付け平均（静的解析とIAMを重視）

weights = {

'static_analysis': 0.25,

'iam_security': 0.25,

'vulnerability_scanning': 0.20,

'application_security': 0.15,

'penetration_testing': 0.15

}

overall_score = (

static_score \* weights\['static_analysis'\] +

iam_score \* weights\['iam_security'\] +

vuln_score \* weights\['vulnerability_scanning'\] +

app_score \* weights\['application_security'\] +

pentest_score \* weights\['penetration_testing'\]

)

\# リスクレベルの決定

if overall_score \>= 90:

risk_level = 'LOW'

elif overall_score \>= 70:

risk_level = 'MEDIUM'

elif overall_score \>= 50:

risk_level = 'HIGH'

else:

risk_level = 'CRITICAL'

return {

'overall_score': round(overall_score, 2),

'risk_level': risk_level,

'component_scores': {

'static_analysis': static_score,

'iam_security': iam_score,

'vulnerability_scanning': vuln_score,

'application_security': app_score,

'penetration_testing': pentest_score

},

'score_breakdown': weights

}

except Exception as e:

logger.error(f"Failed to calculate security score: {str(e)}")

return {'overall_score': 0, 'risk_level': 'UNKNOWN', 'error': str(e)}

def \_calculate_static_analysis_score(self, tflint_errors: int,
tflint_warnings: int, checkov_compliance: float) -\> float:

"""

静的解析スコアの計算

"""

\# TFLintスコア（エラーとワーニングの影響）

tflint_penalty = min(tflint_errors \* 10 + tflint_warnings \* 5, 50)

tflint_score = max(100 - tflint_penalty, 0)

\# Checkovスコア（コンプライアンススコア直接使用）

checkov_score = checkov_compliance if checkov_compliance \> 0 else 50

\# 重み付け平均

return (tflint_score \* 0.4 + checkov_score \* 0.6)

def \_calculate_iam_security_score(self, external_access: int,
unused_permissions: int, cross_account_trusts: int) -\> float:

"""

IAMセキュリティスコアの計算

"""

\# 各要素のペナルティ計算

external_penalty = min(external_access \* 15, 40) \# 外部アクセスは重要

unused_penalty = min(unused_permissions \* 2, 30) \# 未使用権限

trust_penalty = min(cross_account_trusts \* 5, 30) \#
クロスアカウント信頼

total_penalty = external_penalty + unused_penalty + trust_penalty

return max(100 - total_penalty, 0)

def \_calculate_vulnerability_risk_score(self, high_severity: int,
medium_severity: int, total_vulns: int) -\> float:

"""

脆弱性リスクスコアの計算（高いほど危険）

"""

if total_vulns == 0:

return 0

\# 重要度別重み付け

risk_score = (high_severity \* 10 + medium_severity \* 5) /
max(total_vulns, 1)

return min(risk_score, 100)

def \_calculate_app_security_score(self, high_severity: int,
total_vulns: int, targets: int) -\> float:

"""

アプリケーションセキュリティスコアの計算

"""

if targets == 0:

return 100 \# スキャン対象なしは満点

\# 脆弱性密度とスコア

vuln_density = total_vulns / targets

high_severity_rate = high_severity / max(total_vulns, 1) \* 100

penalty = min(vuln_density \* 20 + high_severity_rate, 100)

return max(100 - penalty, 0)

def \_calculate_pentest_security_score(self, exploited: int, critical:
int, high: int) -\> float:

"""

ペネトレーションテストセキュリティスコアの計算

"""

\# 実際に悪用された脆弱性は重大なペナルティ

exploit_penalty = exploited \* 25

critical_penalty = critical \* 15

high_penalty = high \* 10

total_penalty = exploit_penalty + critical_penalty + high_penalty

return max(100 - total_penalty, 0)

def \_calculate_owasp_coverage(self, total_vulns: int, injection_vulns:
int) -\> float:

"""

OWASP Top 10カバレッジの計算

"""

if total_vulns == 0:

return 100

\# 注入攻撃の割合から推定（簡略化）

injection_rate = injection_vulns / total_vulns \* 100

return min(injection_rate \* 2, 100) \# 概算カバレッジ

def \_calculate_error_rate_trend(self, start_time: datetime, end_time:
datetime) -\> str:

"""

エラー率のトレンド分析

"""

try:

\# 前日と比較

prev_start = start_time - timedelta(days=1)

prev_end = start_time

\# 前日のエラー数を取得

prev_response = self.table.scan(

FilterExpression='#ts BETWEEN :start AND :end',

ExpressionAttributeNames={'#ts': 'timestamp'},

ExpressionAttributeValues={

':start': prev_start.isoformat(),

':end': prev_end.isoformat()

}

)

prev_error_count = len(prev_response\['Items'\])

\# 今日のエラー数を取得

current_response = self.table.scan(

FilterExpression='#ts BETWEEN :start AND :end',

ExpressionAttributeNames={'#ts': 'timestamp'},

ExpressionAttributeValues={

':start': start_time.isoformat(),

':end': end_time.isoformat()

}

)

current_error_count = len(current_response\['Items'\])

if prev_error_count == 0 and current_error_count == 0:

return 'STABLE'

elif prev_error_count == 0:

return 'INCREASING'

elif current_error_count == 0:

return 'DECREASING'

else:

change_rate = (current_error_count - prev_error_count) /
prev_error_count \* 100

if change_rate \> 20:

return 'INCREASING'

elif change_rate \< -20:

return 'DECREASING'

else:

return 'STABLE'

except Exception:

return 'UNKNOWN'

def \_calculate_pipeline_reliability_score(self, severity_counts: Dict,
total_errors: int) -\> float:

"""

パイプライン信頼性スコアの計算

"""

if total_errors == 0:

return 100.0

\# 重要度別ペナルティ

penalty = (

severity_counts.get('CRITICAL', 0) \* 20 +

severity_counts.get('HIGH', 0) \* 15 +

severity_counts.get('MEDIUM', 0) \* 10 +

severity_counts.get('LOW', 0) \* 5

)

return max(100 - penalty, 0)

def \_analyze_trends(self, daily_report: Dict) -\> Dict\[str, Any\]:

"""

トレンド分析

"""

trends = {}

try:

\# 過去7日間のレポートを取得してトレンド分析

\# 簡略化版 - 実際の実装では過去データとの比較を行う

overall_score =
daily_report\['overall_security_score'\]\['overall_score'\]

if overall_score \>= 80:

trends\['security_trend'\] = 'IMPROVING'

elif overall_score \>= 60:

trends\['security_trend'\] = 'STABLE'

else:

trends\['security_trend'\] = 'DETERIORATING'

trends\['key_improvements'\] = \[\]

trends\['areas_of_concern'\] = \[\]

\# 改善点の特定

if daily_report\['static_analysis'\].get('checkov',
{}).get('compliance_score', 0) \> 80:

trends\['key_improvements'\].append('Strong compliance posture
maintained')

if daily_report\['iam_security'\].get('external_access_findings', 0) ==
0:

trends\['key_improvements'\].append('No external access violations
detected')

\# 懸念点の特定

if daily_report\['vulnerability_scanning'\].get('high_severity_vulns',
0) \> 0:

trends\['areas_of_concern'\].append('High severity vulnerabilities
detected')

if
daily_report\['penetration_testing'\].get('vulnerabilities_exploited',
0) \> 0:

trends\['areas_of_concern'\].append('Exploitable vulnerabilities found')

except Exception as e:

logger.error(f"Trend analysis failed: {str(e)}")

trends = {'error': str(e)}

return trends

def \_save_report(self, daily_report: Dict):

"""

日次レポートをS3に保存

"""

try:

report_date = daily_report\['report_date'\]

s3_key = f"daily-security-reports/{report_date}/security-report.json"

s3.put_object(

Bucket=S3_BUCKET,

Key=s3_key,

Body=json.dumps(daily_report, indent=2, default=str),

ContentType='application/json'

)

logger.info(f"Daily report saved to S3: {s3_key}")

except Exception as e:

logger.error(f"Failed to save daily report: {str(e)}")

def \_send_daily_report(self, daily_report: Dict):

"""

日次レポートの送信

"""

try:

overall_score =
daily_report\['overall_security_score'\]\['overall_score'\]

risk_level = daily_report\['overall_security_score'\]\['risk_level'\]

subject = f"Daily Security Report - {daily_report\['report_date'\]}
(Score: {overall_score:.1f})"

\# レポート本文作成

message_parts = \[

f"TechNova DevSecOps Daily Security Report",

f"Report Date: {daily_report\['report_date'\]}",

f"Overall Security Score: {overall_score:.1f}/100",

f"Risk Level: {risk_level}",

"",

"=== Security Metrics Summary ===",

""

\]

\# 静的解析サマリー

static = daily_report.get('static_analysis', {})

if 'error' not in static:

message_parts.extend(\[

"Static Analysis:",

f" • TFLint Issues: {static.get('tflint', {}).get('total_issues', 0)}",

f" • Checkov Compliance: {static.get('checkov',
{}).get('compliance_score', 0):.1f}%",

""

\])

\# IAMセキュリティサマリー

iam = daily_report.get('iam_security', {})

if 'error' not in iam:

message_parts.extend(\[

"IAM Security:",

f" • External Access Findings: {iam.get('external_access_findings',
0)}",

f" • Unused Permissions: {iam.get('unused_permissions', 0)}",

f" • Risk Level: {iam.get('risk_level', 'UNKNOWN')}",

""

\])

\# 脆弱性スキャンサマリー

vuln = daily_report.get('vulnerability_scanning', {})

if 'error' not in vuln:

message_parts.extend(\[

"Vulnerability Scanning:",

f" • Total Vulnerabilities: {vuln.get('vulnerabilities_found', 0)}",

f" • High Severity: {vuln.get('high_severity_vulns', 0)}",

f" • Targets Scanned: {vuln.get('targets_scanned', 0)}",

""

\])

\# トレンド分析

trends = daily_report.get('trend_analysis', {})

if 'error' not in trends:

message_parts.extend(\[

"Trend Analysis:",

f" • Security Trend: {trends.get('security_trend', 'UNKNOWN')}",

""

\])

if trends.get('key_improvements'):

message_parts.append("Key Improvements:")

for improvement in trends\['key_improvements'\]\[:3\]:

message_parts.append(f" ✓ {improvement}")

message_parts.append("")

if trends.get('areas_of_concern'):

message_parts.append("Areas of Concern:")

for concern in trends\['areas_of_concern'\]\[:3\]:

message_parts.append(f" ⚠ {concern}")

message_parts.append("")

message_parts.extend(\[

"Detailed metrics available in CloudWatch Dashboard:",

"https://console.aws.amazon.com/cloudwatch/home#dash

message_parts.extend(\[ "Detailed metrics available in CloudWatch
Dashboard:",
"<https://console.aws.amazon.com/cloudwatch/home#dashboards>:name=TechNova-SecurityPipeline-Dashboard",
"", f"Full report stored in S3:
s3://{S3_BUCKET}/daily-security-reports/{daily_report\['report_date'\]}/security-report.json"
\])

message = "\n".join(message_parts)

sns.publish(

TopicArn=SNS_TOPIC_ARN,

Subject=subject,

Message=message

)

logger.info("Daily security report sent successfully")

except Exception as e:

logger.error(f"Failed to send daily report: {str(e)}")

def \_should_alert(self, daily_report: Dict) -\> bool:

"""

アラートが必要かどうかの判定

"""

try:

overall_score =
daily_report\['overall_security_score'\]\['overall_score'\]

risk_level = daily_report\['overall_security_score'\]\['risk_level'\]

\# アラート条件

alert_conditions = \[

overall_score \< 50, \# 全体スコアが50未満

risk_level in \['CRITICAL', 'HIGH'\], \# リスクレベルが高い

daily_report.get('vulnerability_scanning',
{}).get('high_severity_vulns', 0) \> 5, \# 高深刻度脆弱性が5個以上

daily_report.get('penetration_testing',
{}).get('vulnerabilities_exploited', 0) \> 0, \# 悪用可能な脆弱性

daily_report.get('iam_security', {}).get('external_access_findings', 0)
\> 0, \# 外部アクセス検出

daily_report.get('error_analysis', {}).get('severity_breakdown',
{}).get('CRITICAL', 0) \> 0 \# クリティカルエラー

\]

return any(alert_conditions)

except Exception:

return False

def \_send_security_alert(self, daily_report: Dict):

"""

セキュリティアラートの送信

"""

try:

overall_score =
daily_report\['overall_security_score'\]\['overall_score'\]

risk_level = daily_report\['overall_security_score'\]\['risk_level'\]

subject = f"🚨 SECURITY ALERT: Daily Security Score Below Threshold
({overall_score:.1f}/100)"

alert_reasons = \[\]

\# アラート理由の特定

if overall_score \< 50:

alert_reasons.append(f"Overall security score critically low:
{overall_score:.1f}/100")

if daily_report.get('vulnerability_scanning',
{}).get('high_severity_vulns', 0) \> 5:

high_vulns =
daily_report\['vulnerability_scanning'\]\['high_severity_vulns'\]

alert_reasons.append(f"High severity vulnerabilities detected:
{high_vulns}")

if daily_report.get('penetration_testing',
{}).get('vulnerabilities_exploited', 0) \> 0:

exploited =
daily_report\['penetration_testing'\]\['vulnerabilities_exploited'\]

alert_reasons.append(f"Exploitable vulnerabilities found: {exploited}")

if daily_report.get('iam_security', {}).get('external_access_findings',
0) \> 0:

external = daily_report\['iam_security'\]\['external_access_findings'\]

alert_reasons.append(f"External access violations detected: {external}")

message_parts = \[

"CRITICAL SECURITY ALERT",

f"Date: {daily_report\['report_date'\]}",

f"Overall Security Score: {overall_score:.1f}/100",

f"Risk Level: {risk_level}",

"",

"ALERT TRIGGERS:",

\]

for reason in alert_reasons:

message_parts.append(f" 🔴 {reason}")

message_parts.extend(\[

"",

"IMMEDIATE ACTION REQUIRED:",

"1. Review CloudWatch security dashboard immediately",

"2. Investigate high-priority security findings",

"3. Execute security remediation procedures",

"4. Notify security team and stakeholders",

"",

"Security Dashboard:",

"https://console.aws.amazon.com/cloudwatch/home#dashboards:name=TechNova-SecurityPipeline-Dashboard",

"",

f"Detailed report:
s3://{S3_BUCKET}/daily-security-reports/{daily_report\['report_date'\]}/security-report.json"

\])

message = "\n".join(message_parts)

\# 重要度の高いアラートは別のSNSトピックに送信

critical_topic_arn = SNS_TOPIC_ARN.replace('security-reports',
'security-alerts')

sns.publish(

TopicArn=critical_topic_arn,

Subject=subject,

Message=message

)

logger.warning(f"Security alert sent for report:
{daily_report\['report_date'\]}")

except Exception as e:

logger.error(f"Failed to send security alert: {str(e)}")

def handler(event, context): aggregator = SecurityMetricsAggregator()
return aggregator.handler(event, context)

\## 7. 実装まとめ

\### 7.1 システム全体の依存関係と追加リソース

\`\`\`hcl

\# S3バケット群

resource "aws_s3_bucket" "pipeline_artifacts" {

bucket = "technova-pipeline-artifacts-\${random_id.bucket_suffix.hex}"

tags = {

Purpose = "CodePipeline Artifacts"

Environment = "Multi-Account"

}

}

resource "aws_s3_bucket" "build_logs" {

bucket = "technova-build-logs-\${random_id.bucket_suffix.hex}"

tags = {

Purpose = "Build Logs Storage"

Environment = "Multi-Account"

}

}

resource "aws_s3_bucket" "security_configs" {

bucket = "technova-security-configs-\${random_id.bucket_suffix.hex}"

tags = {

Purpose = "Security Configuration Storage"

Environment = "Multi-Account"

}

}

resource "random_id" "bucket_suffix" {

byte_length = 4

}

\# S3バケット暗号化

resource "aws_s3_bucket_server_side_encryption_configuration"
"pipeline_artifacts_encryption" {

bucket = aws_s3_bucket.pipeline_artifacts.id

rule {

apply_server_side_encryption_by_default {

kms_master_key_id = aws_kms_key.pipeline_key.arn

sse_algorithm = "aws:kms"

}

}

}

resource "aws_s3_bucket_server_side_encryption_configuration"
"build_logs_encryption" {

bucket = aws_s3_bucket.build_logs.id

rule {

apply_server_side_encryption_by_default {

kms_master_key_id = aws_kms_key.pipeline_key.arn

sse_algorithm = "aws:kms"

}

}

}

resource "aws_s3_bucket_server_side_encryption_configuration"
"security_configs_encryption" {

bucket = aws_s3_bucket.security_configs.id

rule {

apply_server_side_encryption_by_default {

kms_master_key_id = aws_kms_key.pipeline_key.arn

sse_algorithm = "aws:kms"

}

}

}

\# KMS暗号化キー

resource "aws_kms_key" "pipeline_key" {

description = "TechNova Pipeline Encryption Key"

deletion_window_in_days = 7

tags = {

Purpose = "Pipeline Encryption"

Environment = "Multi-Account"

}

}

resource "aws_kms_alias" "pipeline_key_alias" {

name = "alias/technova-pipeline-key"

target_key_id = aws_kms_key.pipeline_key.key_id

}

\# SNS通知トピック群

resource "aws_sns_topic" "pipeline_errors" {

name = "technova-pipeline-errors"

tags = {

Purpose = "Pipeline Error Notifications"

}

}

resource "aws_sns_topic" "security_reports" {

name = "technova-security-reports"

tags = {

Purpose = "Daily Security Reports"

}

}

resource "aws_sns_topic" "security_alerts" {

name = "technova-security-alerts"

tags = {

Purpose = "Critical Security Alerts"

}

}

resource "aws_sns_topic" "critical_security_alerts" {

name = "technova-critical-security-alerts"

tags = {

Purpose = "Critical Security Alerts"

}

}

resource "aws_sns_topic" "pipeline_approvals" {

name = "technova-pipeline-approvals"

tags = {

Purpose = "Pipeline Manual Approvals"

}

}

resource "aws_sns_topic" "iam_analysis_approvals" {

name = "technova-iam-analysis-approvals"

tags = {

Purpose = "IAM Analysis Approvals"

}

}

resource "aws_sns_topic" "production_approvals" {

name = "technova-production-approvals"

tags = {

Purpose = "Production Deployment Approvals"

}

}

\# SNSトピックポリシー

resource "aws_sns_topic_policy" "pipeline_errors_policy" {

arn = aws_sns_topic.pipeline_errors.arn

policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Effect = "Allow"

Principal = {

Service = \[

"codebuild.amazonaws.com",

"codepipeline.amazonaws.com",

"lambda.amazonaws.com"

\]

}

Action = "sns:Publish"

Resource = aws_sns_topic.pipeline_errors.arn

}

\]

})

}

\# CodeCommitリポジトリ

resource "aws_codecommit_repository" "iac_repo" {

repository_name = "technova-iac-repository"

repository_description = "TechNova Infrastructure as Code Repository"

tags = {

Purpose = "IaC Source Code"

Environment = "Multi-Account"

}

}

\# SSMパラメータ（Slack Webhook等）

resource "aws_ssm_parameter" "slack_webhook" {

name = "/technova/pipeline/slack-webhook"

type = "SecureString"

value = var.slack_webhook_url

tags = {

Purpose = "Slack Integration"

Component = "Notifications"

}

}

\# 変数定義

variable "slack_webhook_url" {

description = "Slack Webhook URL for notifications"

type = string

default = "https://hooks.slack.com/services/CHANGEME"

sensitive = true

}

variable "error_handling_mode" {

description = "Error handling mode: STOP, CONTINUE, or MANUAL_APPROVAL"

type = string

default = "MANUAL_APPROVAL"

validation {

condition = contains(\["STOP", "CONTINUE", "MANUAL_APPROVAL"\],
var.error_handling_mode)

error_message = "Error handling mode must be STOP, CONTINUE, or
MANUAL_APPROVAL."

}

}

\# Lambda関数のZIPファイル作成用データソース

data "archive_file" "security_metrics_aggregator_zip" {

type = "zip"

output_path = "lambda/security_metrics_aggregator.zip"

source {

content = file("\${path.module}/lambda/security_metrics_aggregator.py")

filename = "index.py"

}

}

\# Lambda IAMロール（メトリクス集約用）

resource "aws_iam_role" "lambda_metrics_role" {

name = "lambda-security-metrics-role"

assume_role_policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Action = "sts:AssumeRole"

Effect = "Allow"

Principal = {

Service = "lambda.amazonaws.com"

}

}

\]

})

}

resource "aws_iam_role_policy" "lambda_metrics_policy" {

name = "lambda-security-metrics-policy"

role = aws_iam_role.lambda_metrics_role.id

policy = jsonencode({

Version = "2012-10-17"

Statement = \[

{

Effect = "Allow"

Action = \[

"logs:CreateLogGroup",

"logs:CreateLogStream",

"logs:PutLogEvents"

\]

Resource = "arn:aws:logs:\*:\*:\*"

},

{

Effect = "Allow"

Action = \[

"cloudwatch:GetMetricStatistics",

"cloudwatch:ListMetrics"

\]

Resource = "\*"

},

{

Effect = "Allow"

Action = \[

"dynamodb:Scan",

"dynamodb:Query"

\]

Resource = aws_dynamodb_table.pipeline_errors.arn

},

{

Effect = "Allow"

Action = \[

"s3:PutObject",

"s3:GetObject"

\]

Resource = "\${aws_s3_bucket.build_logs.arn}/\*"

},

{

Effect = "Allow"

Action = \[

"sns:Publish"

\]

Resource = \[

aws_sns_topic.security_reports.arn,

aws_sns_topic.security_alerts.arn

\]

}

\]

})

}

7.2 アウトプット定義

\# 重要なリソースの出力

output "pipeline_name" {

description = "Name of the security integrated pipeline"

value = aws_codepipeline.security_integrated_pipeline.name

}

output "pipeline_arn" {

description = "ARN of the security integrated pipeline"

value = aws_codepipeline.security_integrated_pipeline.arn

}

output "artifacts_bucket" {

description = "S3 bucket for pipeline artifacts"

value = aws_s3_bucket.pipeline_artifacts.bucket

}

output "build_logs_bucket" {

description = "S3 bucket for build logs and security reports"

value = aws_s3_bucket.build_logs.bucket

}

output "security_dashboard_url" {

description = "CloudWatch Security Dashboard URL"

value =
"https://console.aws.amazon.com/cloudwatch/home#dashboards:name=\${aws_cloudwatch_dashboard.security_pipeline_dashboard.dashboard_name}"

}

output "pipeline_url" {

description = "CodePipeline Console URL"

value =
"https://console.aws.amazon.com/codesuite/codepipeline/pipelines/\${aws_codepipeline.security_integrated_pipeline.name}/view"

}

output "error_handler_function_name" {

description = "Error handler Lambda function name"

value = aws_lambda_function.pipeline_error_handler.function_name

}

output "metrics_aggregator_function_name" {

description = "Security metrics aggregator Lambda function name"

value = aws_lambda_function.security_metrics_aggregator.function_name

}

output "sns_topics" {

description = "SNS notification topics"

value = {

pipeline_errors = aws_sns_topic.pipeline_errors.arn

security_reports = aws_sns_topic.security_reports.arn

security_alerts = aws_sns_topic.security_alerts.arn

critical_security_alerts = aws_sns_topic.critical_security_alerts.arn

}

}

output "access_analyzer_arn" {

description = "Organization Access Analyzer ARN"

value = aws_accessanalyzer_analyzer.organization_analyzer.arn

}

7.3 デプロイメント手順書

\# TechNova DevSecOps Pipeline デプロイメント手順

\## 前提条件

1\. \*\*AWS Organizations設定\*\*

\- 120アカウント構成のAWS Organizations

\- 管理アカウントでの実行権限

2\. \*\*必要な権限\*\*

\- AdministratorAccess（初期構築時）

\- 以下のサービスへのフルアクセス権限：

\- CodePipeline, CodeBuild, CodeCommit

\- IAM, Organizations, Access Analyzer

\- S3, DynamoDB, Lambda, SNS, CloudWatch

3\. \*\*ツールバージョン\*\*

\- Terraform \>= 1.5.0

\- AWS CLI \>= 2.0

\- Python \>= 3.9

\## デプロイメント手順

\### ステップ1: 環境変数設定

\`\`\`bash

export AWS_REGION=us-east-1

export TECHNOVA_ENVIRONMENT=staging

export SLACK_WEBHOOK_URL="your-slack-webhook-url"

ステップ2: Terraformワークスペース準備

\# Terraformワークスペース初期化

terraform init

\# 環境別ワークスペース作成

terraform workspace new \$TECHNOVA_ENVIRONMENT

terraform workspace select \$TECHNOVA_ENVIRONMENT

ステップ3: 設定ファイル作成

\# terraform.tfvars作成

cat \> terraform.tfvars \<\< EOF

slack_webhook_url = "\$SLACK_WEBHOOK_URL"

error_handling_mode = "MANUAL_APPROVAL"

EOF

ステップ4: デプロイメント実行

\# 計画確認

terraform plan -var-file=terraform.tfvars

\# デプロイ実行

terraform apply -var-file=terraform.tfvars

ステップ5: 初期設定

\# CodeCommitリポジトリへの初期コミット

aws codecommit create-repository --repository-name
technova-iac-repository

git clone
https://git-codecommit.us-east-1.amazonaws.com/v1/repos/technova-iac-repository

cd technova-iac-repository

\# buildspecファイル配置

mkdir -p buildspecs

cp ../buildspecs/\* buildspecs/

\# 初期コミット

git add .

git commit -m "Initial pipeline configuration"

git push origin main

ステップ6: SNS通知設定

\# メール通知購読

aws sns subscribe \\

--topic-arn \$(terraform output -raw sns_topics \| jq -r
.security_alerts) \\

--protocol email \\

--notification-endpoint your-security-team@technova.com

aws sns subscribe \\

--topic-arn \$(terraform output -raw sns_topics \| jq -r
.pipeline_errors) \\

--protocol email \\

--notification-endpoint <your-devops-team@technova.com>

**運用開始後の確認項目**

**1. パイプライン動作確認**

- CodePipelineの初回実行成功

- 各ステージの正常動作確認

- エラーハンドリングの動作確認

**2. セキュリティ分析確認**

- 静的解析（Terraform Validate, TFLint, Checkov）結果確認

- IAM Access Analyzer分析結果確認

- 動的脆弱性スキャン結果確認

- ペネトレーションテスト結果確認

**3. 監視とアラート確認**

- CloudWatchダッシュボード表示確認

- SNS通知受信確認

- Slack通知受信確認（設定した場合）

- 日次セキュリティレポート受信確認

**4. エラーハンドリング確認**

- エラー発生時のLambda関数動作確認

- DynamoDBへのエラー記録確認

- エスカレーション機能確認

**トラブルシューティング**

**よくある問題と解決方法**

1.  **CodeBuildでのタイムアウト**

bash

*\# timeout_in_minutesを延長*

terraform apply -var="build_timeout=120"

2.  **IAM権限エラー**

bash

*\# 必要な権限をチェック*

aws iam simulate-principal-policy \\

--policy-source-arn arn:aws:iam::ACCOUNT:role/codebuild-role \\

--action-names iam:ListRoles

3.  **S3バケット作成エラー**

bash

*\# バケット名の重複を解決*

terraform destroy -target=aws_s3_bucket.pipeline_artifacts

terraform apply

**継続的な保守作業**

**週次作業**

- セキュリティダッシュボードの確認

- 高優先度のセキュリティ問題対応

- パイプライン実行成功率の確認

**月次作業**

- セキュリティポリシーの見直し

- 未使用IAM権限のクリーンアップ

- 脆弱性トレンドの分析

**四半期作業**

- セキュリティツールのバージョンアップ

- パイプライン設定の見直し

- コンプライアンス要件の確認

**緊急時対応手順**

**セキュリティインシデント発生時**

1.  パイプライン緊急停止

bash

aws codepipeline stop-pipeline-execution \\

--pipeline-name security-integrated-deployment-pipeline \\

--pipeline-execution-id \$EXECUTION_ID

2.  影響範囲の調査

3.  修復対応の実施

4.  パイプライン再開

**システム障害時**

1.  CloudWatchアラームの確認

2.  Lambda関数ログの確認

3.  必要に応じてマニュアル実行

この包括的なセキュリティ分析パイプラインにより、TechNova社の120アカウント環境における
Infrastructure as Code
のセキュリティ品質を確保し、DevSecOpsプラクティスを実現することができます。

これで、TechNova社の要件に基づく包括的なIaC &
CI/CD統合セキュリティ分析パイプラインの実装が完了しました。このシステムは、静的解析、IAM分析、動的脆弱性検証を統合した多層防御型のDevSecOpsパイプラインとして機能し、120アカウント環境全体のセキュリティガバナンスを実現します。

- 

